Citance Number: 1 | Reference Article:  C94-1032.txt | Citing Article:  C96-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>N-gram models arc usually used for scoring (Gu et al, 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manualy segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Corpus.</S><S sid = NA ssid = NA>Figure 5 shows tile percentage of sentences (not words) correctly segmented and tagged.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C94-1032.txt | Citing Article:  P11-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm, but we don't know how to handle unknown words in this algorithm.</S><S sid = NA ssid = NA>This slot corresponds to the combined state in the second order IIMM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the segmentation accuracy of the proposed sys- tem is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphologi- cal analysis accuracy, it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model.</S><S sid = NA ssid = NA>For open texts, the sentence accuracy of the raw part of speech trigram without word model is 62.7% for the top candidate and 70.4% for the top-5, while that of smoothed trigram with word model is 66.9% for the top and 80.3% for the top-5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used the Viterbi-like dynamic programming procedure described in [Nagata, 1994] to get the most likely word segmentation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the backward N-best search, how(wet, we want N most likely word segmentation and part of speech sequence.</S><S sid = NA ssid = NA>First, a linear time dynamic programming is used for record- ing the scores of all partial paths in a table 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C94-1032.txt | Citing Article:  W97-0120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the segmentation accuracy of the proposed sys- tem is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphologi- cal analysis accuracy, it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model.</S><S sid = NA ssid = NA>For open texts, tile morphological nalyzer achieved 95.1% recall and 94.6% precision for the top candidate, and 97.8% recall and 73.2% precision for the 5 best candidates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C94-1032.txt | Citing Article:  W96-0113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm, but we don't know how to handle unknown words in this algorithm.</S><S sid = NA ssid = NA>This slot corresponds to the combined state in the second order IIMM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C94-1032.txt | Citing Article:  P06-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Nagata (1994) proposed a stochastic word segmenter based on a word-gram model to solve the word segmentation problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The word model must account for morphology and word for- marion to estimate the part of speech and tile probabil- ity of a word hypothesis.</S><S sid = NA ssid = NA>Once word hypotheses for unknown words are gener- ated, the proposed N-best algorithm will find tile most likely word segmentation a d part of speech assignment taking into account he entire sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C94-1032.txt | Citing Article:  P06-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Corpus.</S><S sid = NA ssid = NA>Tile corpus was divided into 90% R)r training and 10% for test- ing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C94-1032.txt | Citing Article:  C00-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Fully stochastic language models (e.g. Nagata 1994), on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The reason is described later.</S><S sid = NA ssid = NA>We show in this paper that we can buihl a stochastic Japanese morphological nalyzer that offers approxi- mately 95% accuracy on a statistical language model- ing technique and an efficient two-pass N-best search strategy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C94-1032.txt | Citing Article:  C00-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best accuracy reported for statistical methods to date is around 95% (e.g. Nagata 1994).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show in this paper that we can buihl a stochastic Japanese morphological nalyzer that offers approxi- mately 95% accuracy on a statistical language model- ing technique and an efficient two-pass N-best search strategy.</S><S sid = NA ssid = NA>In recent years, we have seen a fair number of l)al)ers re- porting accuracies ofmore than 95% for English part of speech tagging with statistical language modeling tech- niques \[2-4, 10, 11\].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C94-1032.txt | Citing Article:  W01-0512.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the N best sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parse.prev?ous i the pointer to the (best) previous parse structure as in conventional Viterbi decoding, which is not necessary if we use the backward N best search.</S><S sid = NA ssid = NA>The proposed algorithm amalgamates and extends three well-known algorithms in different fields: the Minimum Connective-Cost Method \[7\] for Japanese morphologi- cal analysis, Extended Viterbi Algorithm for charac- ter recognition \[6\], and "l~'ee-Trellis N-Best Search for speech recognition \[15\].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C94-1032.txt | Citing Article:  P01-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximum probability solution in Japanese morphological analysis (Nagata, 1994).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm, but we don't know how to handle unknown words in this algorithm.</S><S sid = NA ssid = NA>A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C94-1032.txt | Citing Article:  A00-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages (Nagata, 1994).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then tested the proposed system, which uses smoothed part of speech trigram with word model, on the open test sentences.</S><S sid = NA ssid = NA>We also connt the number of crossings, which is tile mmtber of c,'mes where a bracketed sequence from the standard data overlaps a bracketed sequence from tile system output, but neither sequence is completely coutained in the other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C94-1032.txt | Citing Article:  P09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we apply a dynamic programming search (Nagata, 1994) to k best MIRA.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The search algorithm consists of a forward dynamic programming search and a backward A* search.</S><S sid = NA ssid = NA>It consists of the forward dynamic pro- gramming search and the backward A* search.</S> | Discourse Facet:  NA | Annotator: Automatic


