<PAPER>
	<S sid="0">Dependency-based Syntactic&amp;#x2013;Semantic Analysis with PropBank and NomBank</S><ABSTRACT>
	<SECTION title="Introduction: Syntactic?Semantic. " number="1">
			<S sid="1" ssid="1">Analysis Intuitively, semantic interpretation should help syntactic disambiguation, and joint syntactic?semantic analysis has a long tradition in linguis tic theory.</S>
			<S sid="2" ssid="2">This motivates a statistical modeling of the problem of finding a syntactic tree y?</S>
			<S sid="3" ssid="3">syn and a semantic graph y?</S>
			<S sid="4" ssid="4">semfor a sentence x as maximiz ing a function F that scores the joint syntactic?</S>
			<S sid="5" ssid="5">semantic structure: ?y? syn , y?</S>
			<S sid="6" ssid="6">sem ? = arg max y syn ,y sem F (x, y syn , y sem ) The dependencies in the feature representation used to compute F determine the tractability of thesearch procedure needed to perform the maximiza tion.</S>
			<S sid="7" ssid="7">To be able to use complex syntactic features c ? 2008.</S>
			<S sid="8" ssid="8">Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid="9" ssid="9">Some rights reserved.</S>
			<S sid="10" ssid="10">such as paths when predicting semantic structures, exact search is clearly intractable.</S>
			<S sid="11" ssid="11">This is true even with simpler feature representations ? the problemis a special case of multi-headed dependency anal ysis, which is NP-hard even if the number of heads is bounded (Chickering et al, 1994).This means that we must resort to a simplifica tion such as an incremental method or a rerankingapproach.</S>
			<S sid="12" ssid="12">We chose the latter option and thus cre ated syntactic and semantic submodels.</S>
			<S sid="13" ssid="13">The joint syntactic?semantic prediction is selected from a small list of candidates generated by the respective subsystems.</S>
	</SECTION>
	<SECTION title="Syntactic Submodel. " number="2">
			<S sid="14" ssid="1">We model the process of syntactic parsing of a sentence x as finding the parse tree y?</S>
			<S sid="15" ssid="2">syn = argmax yF (x, y) that maximizes a scoring func tion F . The learning problem consists of fitting this function so that the cost of the predictions is as low as possible according to a cost function ?.</S>
			<S sid="16" ssid="3">In this work, we consider linear scoring functions of the following form: F (x, y) = w ??(x, y) where ?(x, y) is a numeric feature representation of the pair (x, y) andw a vector of feature weights.</S>
			<S sid="17" ssid="4">We defined the syntactic cost ? as the sum of linkcosts, where the link cost was 0 for a correct de pendency link with a correct label, 0.5 for a correct link with an incorrect label, and 1 for an incorrect link.</S>
			<S sid="18" ssid="5">A widely used framework for fitting the weight vector is the max-margin model (Taskar et al,2003), which is a generalization of the wellknown support vector machines to general costbased prediction problems.</S>
			<S sid="19" ssid="6">Since the large num ber of training examples and features in our casemake an exact solution of the max-margin optimization problem impractical, we used the on line passive?aggressive algorithm (Crammer et al, 1832006), which approximates the optimization pro cess in two ways:?</S>
			<S sid="20" ssid="7">The weight vector w is updated incremen tally, one example at a time.?</S>
			<S sid="21" ssid="8">For each example, only the most violated con straint is considered.The algorithm is a margin-based variant of the perceptron (preliminary experiments show that it outperforms the ordinary perceptron on this task).</S>
			<S sid="22" ssid="9">Al gorithm 1 shows pseudocode for the algorithm.</S>
			<S sid="23" ssid="10">Algorithm 1 The Online PA Algorithm input Training set T = {(x t , y t )} T t=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (x t , y t ) in T let y?</S>
			<S sid="24" ssid="11">t = argmax y F (x t , y) + ?(y t , y) let ? t = min ? C, F (x t ,y? t )?F (x t ,y t )+?(y t ,y? t ) ??(x,y t )??(x,y? t )?</S>
			<S sid="25" ssid="12">2 ? w ? w + ? t (?(x, y t )??(x, y?</S>
			<S sid="26" ssid="13">t )) returnwaverage We used a C value of 0.01, and the number of iterations was 6.</S>
			<S sid="27" ssid="14">2.1 Features and Search.</S>
			<S sid="28" ssid="15">The feature function ? is a second-order edge factored representation (McDonald and Pereira,2006; Carreras, 2007).</S>
			<S sid="29" ssid="16">The second-order repre sentation allows us to express features not only ofhead?dependent links, but also of siblings and chil dren of the dependent.</S>
			<S sid="30" ssid="17">This feature set forces usto adopt the expensive search procedure by Car reras (2007), which extends Eisner?s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies.</S>
			<S sid="31" ssid="18">Since the cost function ? is based on the cost of single links, this procedure can also be used to find the maximizer of F (x i , y ij )+?(y i , y ij), which is needed at train ing time.</S>
			<S sid="32" ssid="19">The search was constrained to disallow multiple root links.</S>
			<S sid="33" ssid="20">2.2 Handling Nonprojective Links.</S>
			<S sid="34" ssid="21">Although only 0.4% of the links in the training set are nonprojective, 7.6% of the sentences contain at least one nonprojective link.</S>
			<S sid="35" ssid="22">Many of these linksrepresent long-range dependencies ? such as whmovement ? that are valuable for semantic pro cessing.</S>
			<S sid="36" ssid="23">Nonprojectivity cannot be handled by span-based dynamic programming algorithms.</S>
			<S sid="37" ssid="24">For parsers that consider features of single links only,the Chu-Liu/Edmonds algorithm can be used instead.</S>
			<S sid="38" ssid="25">However, this algorithm cannot be gen eralized to the second-order setting ? McDonaldand Pereira (2006) proved that this problem is NP hard, and described an approximate greedy search algorithm.</S>
			<S sid="39" ssid="26">To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time.</S>
			<S sid="40" ssid="27">The useof trace labels in the pseudo-projective transfor mation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once.</S>
			<S sid="41" ssid="28">Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels.</S>
	</SECTION>
	<SECTION title="Semantic Submodel. " number="3">
			<S sid="42" ssid="1">Our semantic model consists of three parts: ? A SRL classifier pipeline that generates a list of candidate predicate?argument structures.</S>
			<S sid="43" ssid="2">A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments.</S>
			<S sid="44" ssid="3">A global classifier that rescores the predicate?</S>
			<S sid="45" ssid="4">argument structures in the filtered candidate list.Rather than training the models on goldstandard syntactic input, we created an automati cally parsed training set by 5-fold cross-validation.</S>
			<S sid="46" ssid="5">Training on automatic syntax makes the semanticclassifiers more resilient to parsing errors, in par ticular adjunct labeling errors.</S>
			<S sid="47" ssid="6">3.1 SRL Pipeline.</S>
			<S sid="48" ssid="7">The SRL pipeline consists of classifiers for predicate identification, predicate disambiguation, sup port identification (for noun predicates), argument identification, and argument classification.</S>
			<S sid="49" ssid="8">We trained one set of classifiers for verb predicatesand another for noun predicates.</S>
			<S sid="50" ssid="9">For the pred icate disambiguation classifiers, we trained one subclassifier for each lemma.</S>
			<S sid="51" ssid="10">All classifiers in thepipeline were L2-regularized linear logistic regres sion classifiers, implemented using the efficientLIBLINEAR package (Lin et al, 2008).</S>
			<S sid="52" ssid="11">For multi class problems, we used the one-vs-all binarization 184 method, which makes it easy to prevent outputs not allowed by the PropBank or NomBank frame.</S>
			<S sid="53" ssid="12">Since our classifiers were logistic, their outputvalues could be meaningfully interpreted as prob abilities.</S>
			<S sid="54" ssid="13">This allowed us to combine the scores from subclassifiers into a score for the completepredicate?argument structure.</S>
			<S sid="55" ssid="14">To generate the candidate lists used by the global SRL models, we ap plied beam search based on these scores using a beam width of 4.</S>
			<S sid="56" ssid="15">The features used by the classifiers are listed in Tables 1 and 2.</S>
			<S sid="57" ssid="16">In the tables, the features used by the classifiers for noun and verb predicates are indicated by N and V, respectively.</S>
			<S sid="58" ssid="17">We selected the feature sets by greedy forward subset selection.</S>
			<S sid="59" ssid="18">Feature PredId PredDis PREDWORD N,V N,V PREDLEMMA N,V N,V PREDPARENTWORD/POS N,V N,V CHILDDEPSET N,V N,V CHILDWORDSET N,V N,V CHILDWORDDEPSET N,V N,V CHILDPOSSET N,V N,V CHILDPOSDEPSET N,V N,V DEPSUBCAT N,V N,V PREDRELTOPARENT N,V N,VTable 1: Classifier features in predicate identifica tion and disambiguation.</S>
			<S sid="60" ssid="19">Feature Supp ArgId ArgCl PREDPARENTWORD/POS N N,V CHILDDEPSET N N,V N,V PREDLEMMASENSE N N,V N,V VOICE V V POSITION N N,V N,V ARGWORD/POS N N,V N,V LEFTWORD/POS N N,V RIGHTWORD/POS N N,V N,V LEFTSIBLINGWORD/POS N,V RIGHTSIBLINGWORD/POS N N PREDPOS N N,V V RELPATH N N,V N,V POSPATH N RELPATHTOSUPPORT N N VERBCHAINHASSUBJ V V CONTROLLERHASOBJ V N PREDRELTOPARENT N N,V N,V FUNCTION N,VTable 2: Classifier features in argument identifica tion and classification and support detection.</S>
			<S sid="61" ssid="20">Features Used in Predicate Identification and Disambiguation PREDWORD, PREDLEMMA.</S>
			<S sid="62" ssid="21">The lexical form and lemma of the predicate.</S>
			<S sid="63" ssid="22">PREDPARENTWORD and PREDPARENTPOS.</S>
			<S sid="64" ssid="23">Form and part-of-speech tag of the parent node of the predicate.CHILDDEPSET, CHILDWORDSET, CHILDWORDDEPSET, CHILDPOSSET, CHILD POSDEPSET.</S>
			<S sid="65" ssid="24">These features represent the set of dependents of the predicate using combinations of dependency labels, words, and parts of speech.DEPSUBCAT.</S>
			<S sid="66" ssid="25">Subcategorization frame: the con catenation of the dependency labels of the predicate dependents.PREDRELTOPARENT.</S>
			<S sid="67" ssid="26">Dependency relation be tween the predicate and its parent.</S>
			<S sid="68" ssid="27">Features Used in Argument Identification and Classification PREDLEMMASENSE.</S>
			<S sid="69" ssid="28">The lemma and sense number of the predicate, e.g. give.01.VOICE.</S>
			<S sid="70" ssid="29">For verbs, this feature is Active or Pas sive.</S>
			<S sid="71" ssid="30">For nouns, it is not defined.</S>
			<S sid="72" ssid="31">POSITION.</S>
			<S sid="73" ssid="32">Position of the argument with respect to the predicate: Before, After, or On.</S>
			<S sid="74" ssid="33">ARGWORD and ARGPOS.</S>
			<S sid="75" ssid="34">Lexical form and part-of-speech tag of the argument node.LEFTWORD, LEFTPOS, RIGHTWORD, RIGHTPOS.</S>
			<S sid="76" ssid="35">Form/part-of-speech tag of the left most/rightmost dependent of the argument.</S>
			<S sid="77" ssid="36">LEFTSIBLINGWORD, LEFTSIBLINGPOS,RIGHTSIBLINGWORD, RIGHTSIBLING POS.</S>
			<S sid="78" ssid="37">Form/part-of-speech tag of the left/right sibling of the argument.</S>
			<S sid="79" ssid="38">PREDPOS.</S>
			<S sid="80" ssid="39">Part-of-speech tag of the predicate.</S>
			<S sid="81" ssid="40">RELPATH.</S>
			<S sid="82" ssid="41">A representation of the complex grammatical relation between the predicate and the argument.</S>
			<S sid="83" ssid="42">It consists of the sequenceof dependency relation labels and link directions in the path between predicate and argu ment, e.g. IM?OPRD?OBJ?.POSPATH.</S>
			<S sid="84" ssid="43">An alternative view of the grammat ical relation, which consists of the POS tagspassed when moving from predicate to argu ment, e.g. VB?TO?VBP?PRP.</S>
			<S sid="85" ssid="44">RELPATHTOSUPPORT.</S>
			<S sid="86" ssid="45">The RELPATH from the argument to a support chain.</S>
			<S sid="87" ssid="46">VERBCHAINHASSUBJ.</S>
			<S sid="88" ssid="47">Binary feature that is setto true if the predicate verb chain has a sub ject.</S>
			<S sid="89" ssid="48">The purpose of this feature is to resolve verb coordination ambiguity as in Figure 1.</S>
			<S sid="90" ssid="49">CONTROLLERHASOBJ.</S>
			<S sid="91" ssid="50">Binary feature that is true if the link between the predicate verb chain and its parent is OPRD, and the parent has an object.</S>
			<S sid="92" ssid="51">This feature is meant to resolve control ambiguity as in Figure 2.</S>
			<S sid="93" ssid="52">185FUNCTION.</S>
			<S sid="94" ssid="53">The grammatical function of the ar gument node.</S>
			<S sid="95" ssid="54">For direct dependents of the predicate, this is identical to the RELPATH.</S>
			<S sid="96" ssid="55">I SBJ eat drinkyouand COORD SBJ CONJROOT SBJ COORD ROOT drinkandeatI CONJ Figure 1: Coordination ambiguity: The subject I is in an ambiguous position with respect to drink.</S>
			<S sid="97" ssid="56">I to IMSBJ want sleephim OBJ OPRD ROOT IM sleepI SBJ want ROOT to OPRD Figure 2: Subject/object control ambiguity: I is in an ambiguous position with respect to sleep.</S>
			<S sid="98" ssid="57">3.2 Linguistically Motivated Global.</S>
			<S sid="99" ssid="58">Constraints The following three global constraints were used to filter the candidates generated by the pipeline.CORE ARGUMENT CONSISTENCY.</S>
			<S sid="100" ssid="59">Core argu ment labels must not appear more than once.DISCONTINUITY CONSISTENCY.</S>
			<S sid="101" ssid="60">If there is a la bel C-X, it must be preceded by a label X. REFERENCE CONSISTENCY.</S>
			<S sid="102" ssid="61">If there is a label R-X and the label is inside a relative clause, it must be preceded by a label X. 3.3 Global SRL Model.</S>
			<S sid="103" ssid="62">Toutanova et al (2005) have showed that a global model that scores the complete predicate?argument structure can lead to substantial perfor mance gains.</S>
			<S sid="104" ssid="63">We therefore created a global SRLclassifier using the following global features in ad dition to the features from the pipeline: CORE ARGUMENT LABEL SEQUENCE.</S>
			<S sid="105" ssid="64">The complete sequence of core argument labels.</S>
			<S sid="106" ssid="65">The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1.</S>
			<S sid="107" ssid="66">MISSING CORE ARGUMENT LABELS.</S>
			<S sid="108" ssid="67">The setof core argument labels declared in the Prop Bank/NomBank frame that are not present in the predicate?argument structure.</S>
			<S sid="109" ssid="68">Similarly to the syntactic submodel, we trained the global SRL model using the online passive?</S>
			<S sid="110" ssid="69">aggressive algorithm.</S>
			<S sid="111" ssid="70">The cost function ? was defined as the number of incorrect links in thepredicate?argument structure.</S>
			<S sid="112" ssid="71">The number of it erations was 20 and the regularization parameter C was 0.01.</S>
			<S sid="113" ssid="72">Interestingly, we noted that the global SRL model outperformed the pipeline even when no global features were added.</S>
			<S sid="114" ssid="73">This shows that theglobal learning model can correct label bias prob lems introduced by the pipeline architecture.</S>
	</SECTION>
	<SECTION title="Syntactic?Semantic Integration. " number="4">
			<S sid="115" ssid="1">Our baseline joint feature representation containedonly three features: the log probability of the syn tactic tree and the log probability of the semantic structure according to the pipeline and the global model, respectively.</S>
			<S sid="116" ssid="2">This model was trained on the complete training set using cross-validation.</S>
			<S sid="117" ssid="3">The probabilities were obtained using the multinomial logistic function (?softmax?).</S>
			<S sid="118" ssid="4">We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline.</S>
			<S sid="119" ssid="5">Time prevented us from exploring this direction conclusively.</S>
	</SECTION>
	<SECTION title="Results. " number="5">
			<S sid="120" ssid="1">The submitted results on the development and test corpora are presented in the upper part of Table 3.</S>
			<S sid="121" ssid="2">After the submission deadline, we corrected a bugin the predicate identification method.</S>
			<S sid="122" ssid="3">This re sulted in improved results shown in the lower part.</S>
			<S sid="123" ssid="4">Corpus Syn acc Sem F1 Macro F1 Development 88.47 80.80 84.66 Test WSJ 90.13 81.75 85.95 Test Brown 82.81 69.06 75.95 Test WSJ + Brown 89.32 80.37 84.86 Development 88.47 81.86 85.17 Test WSJ 90.13 83.75 86.61 Test Brown 82.84 69.85 76.34 Test WSJ + Brown 89.32 81.65 85.49 Table 3: Results.</S>
			<S sid="124" ssid="5">5.1 Syntactic Results.</S>
			<S sid="125" ssid="6">Table 4 shows the effect of adding second-order features to the parser in terms of accuracy as well as training and parsing time on a Mac Pro, 3.2 GHz.</S>
			<S sid="126" ssid="7">The training times were measured on thecomplete training set and the parsing time and accuracies on the development set.</S>
			<S sid="127" ssid="8">Similarly to Car reras (2007), we see that these features have a very large impact on parsing accuracy, but also that the parser pays dearly in terms of efficiency as the search complexity increases fromO(n3) toO(n4).</S>
			<S sid="128" ssid="9">186 Since the low efficiency of the second-order parserrestricts its use to batch applications, we see an interesting research direction to find suitable com promises between the two approaches, for instance by sacrificing the exact search procedure.</S>
			<S sid="129" ssid="10">System Training Parse Labeled Unlabeled 1st order 65 min 28 sec 85.78 89.51 2nd order 60 hours 34 min 88.33 91.43 Table 4: Impact of second-order features.Table 5 shows the dependency types most af fected by the addition of second-order features to the parser when ordered by the increase in F1.</S>
			<S sid="130" ssid="11">As can be seen, they are all verb adjunct categories,which demonstrates the effect of grandchild fea tures on PP attachment and labeling.</S>
			<S sid="131" ssid="12">Label ?R ?P ?F 1 TMP 14.7 12.9 13.9 DTV 0 19.9 10.5 LOC 7.8 12.3 9.9 PRP 12.4 6.7 9.6 DIR 5.9 7.2 6.5 Table 5: Labels affected by second-order features.</S>
			<S sid="132" ssid="13">5.2 Semantic Results.</S>
			<S sid="133" ssid="14">To assess the effect of the components in the se mantic submodel, we tested their performance on the top-scoring parses from the syntactic model.</S>
			<S sid="134" ssid="15">Table 6 shows the results.</S>
			<S sid="135" ssid="16">The baseline systemconsists of the SRL pipeline only (P).</S>
			<S sid="136" ssid="17">Adding linguistic constraints (C) results in a more precisionoriented system with slightly lower recall, but sig nificantly higher F1.</S>
			<S sid="137" ssid="18">Even higher performance is obtained when adding the global SRL model (G).</S>
			<S sid="138" ssid="19">System P R F1 P 80.74 77.98 79.33 P+C 82.42 77.66 79.97 P+C+G 83.64 78.14 80.40 Table 6: SRL results on the top-scoring parse trees.</S>
			<S sid="139" ssid="20">5.3 Syntactic?Semantic Integration.</S>
			<S sid="140" ssid="21">The final experiment concerned the integration of syntactic and semantic analysis.</S>
			<S sid="141" ssid="22">In this setting, the system chooses the output that maximizes the joint syntactic?semantic score, based on the top N syntactic trees.</S>
			<S sid="142" ssid="23">Table 7 shows the results on the development set.</S>
			<S sid="143" ssid="24">We see that syntactic?semantic integration improves both syntactic accuracy and semantic F1.</S>
			<S sid="144" ssid="25">This holds for the constraint-based SRL system as well as for the full system.</S>
			<S sid="145" ssid="26">Sem model N Syn acc Sem F1 Macro F1 P+C 1 88.33 79.97 84.17 P+C 16 88.42 80.42 84.44 P+C+G 1 88.33 80.40 84.39 P+C+G 16 88.47 80.80 84.66 Table 7: Syntactic?semantic integration.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="6">
			<S sid="146" ssid="1">We have described a system1 for syntactic and se mantic dependency analysis based on PropBank and NomBank, and detailed the implementation of its subsystems.</S>
			<S sid="147" ssid="2">Crucial to our success was the high performance of the syntactic parser, whichachieved a high accuracy.</S>
			<S sid="148" ssid="3">In addition, we recon firmed the benefits of global inference in semantic analysis: both constraint-based and learning-based methods resulted in improvements over a baseline.</S>
			<S sid="149" ssid="4">Finally, we showed that integration of syntacticand semantic analysis is beneficial for both subtasks.</S>
			<S sid="150" ssid="5">We hope that this shared task will spur further research that leads to new feature representa tions and search procedures to handle the problem of joint syntactic and semantic analysis.</S>
	</SECTION>
</ABSTRACT>
</PAPER>
