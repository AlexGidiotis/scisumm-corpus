Citance Number: 1 | Reference Article:  P05-1034.txt | Citing Article:  W09-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.</S><S sid = NA ssid = NA>If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1034.txt | Citing Article:  D09-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.</S><S sid = NA ssid = NA>From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1034.txt | Citing Article:  W06-1628.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string.</S><S sid = NA ssid = NA>A detailed description of these heuristics can be found in Quirk et al. (04).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1034.txt | Citing Article:  P12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.</S><S sid = NA ssid = NA>A detailed description of these heuristics can be found in Quirk et al. (04).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1034.txt | Citing Article:  P07-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.</S><S sid = NA ssid = NA>If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1034.txt | Citing Article:  W11-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A detailed description of these heuristics can be found in Quirk et al. (04).</S><S sid = NA ssid = NA>We will initially approach the decoding problem as a bottom up, exhaustive search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1034.txt | Citing Article:  P08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is a syntactically-informed MT system, designed following (Quirk et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Treelet Translation: Syntactically Informed Phrasal SMT</S><S sid = NA ssid = NA>A detailed description of these heuristics can be found in Quirk et al. (04).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1034.txt | Citing Article:  P08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.</S><S sid = NA ssid = NA>The word alignments are used to project the source dependency parses onto the target sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1034.txt | Citing Article:  D11-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper.</S><S sid = NA ssid = NA>State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1034.txt | Citing Article:  D11-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.</S><S sid = NA ssid = NA>This approach offers the following advantages over string-based SMT systems: Instead of limiting learned phrases to contiguous word sequences, we allow translation by all possible phrases that form connected subgraphs (treelets) in the source and target dependency trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1034.txt | Citing Article:  W08-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.</S><S sid = NA ssid = NA>If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1034.txt | Citing Article:  D11-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The word alignments are used to project the source dependency parses onto the target sentences.</S><S sid = NA ssid = NA>We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1034.txt | Citing Article:  D11-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One simple means of incorporating syntax into SMT is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach.</S><S sid = NA ssid = NA>A detailed description of these heuristics can be found in Quirk et al. (04).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1034.txt | Citing Article:  D11-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Under the assumption that constituents generally move as a whole, we predict the probability of each given ordering of modifiers independently.</S><S sid = NA ssid = NA>Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1034.txt | Citing Article:  D11-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.</S><S sid = NA ssid = NA>Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1034.txt | Citing Article:  I08-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.</S><S sid = NA ssid = NA>If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string: de démarrage appears in the wrong place.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1034.txt | Citing Article:  I08-2093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The target language model was trained using only the French side of the corpus; additional data may improve its performance.</S><S sid = NA ssid = NA>Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1034.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The word alignments are used to project the source dependency parses onto the target sentences.</S><S sid = NA ssid = NA>We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1034.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.</S><S sid = NA ssid = NA>The word alignments are used to project the source dependency parses onto the target sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1034.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also keep treelet counts for maximum likelihood estimation.</S><S sid = NA ssid = NA>Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03).</S> | Discourse Facet:  NA | Annotator: Automatic


