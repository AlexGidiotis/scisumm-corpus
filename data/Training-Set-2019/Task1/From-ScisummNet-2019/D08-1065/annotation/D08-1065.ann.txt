Citance Number: 1 | Reference Article:  D08-1065.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation</S><S sid = NA ssid = NA>We have presented a procedure for performing Minimum Bayes-Risk Decoding on translation lattices.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1065.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation</S><S sid = NA ssid = NA>We have presented a procedure for performing Minimum Bayes-Risk Decoding on translation lattices.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1065.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first compare lattice MBR to N-best MBR decoding and MAP decoding (Table 2).</S><S sid = NA ssid = NA>We now present MBR decoding on translation lattices.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1065.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures.</S><S sid = NA ssid = NA>While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1065.txt | Citing Article:  P14-2023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear, global loss functions such as BLEU.</S><S sid = NA ssid = NA>There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1065.txt | Citing Article:  D11-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The lattice density is the average number of arcs per word and can be varied using Forward-Backward pruning (Sixtus and Ortmanns, 1999).</S><S sid = NA ssid = NA>The translation lattices are pruned using Forward-Backward pruning (Sixtus and Ortmanns, 1999) so that the average numbers of arcs per word (lattice density) is 30.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1065.txt | Citing Article:  C10-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.</S><S sid = NA ssid = NA>Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1065.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we show how the performance of lattice MBR changes as a function of the lattice density.</S><S sid = NA ssid = NA>On aren and enzh, there are some gains beyond a lattice density of 30.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1065.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have shown the effect of the MBR scaling factor on the performance of lattice MBR.</S><S sid = NA ssid = NA>We present lattice MBR experiments in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1065.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.</S><S sid = NA ssid = NA>Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1065.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>up to 1081 as per Tromble et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At a lattice density of 30, the lattices in aren contain on an average about 1081 hypotheses!</S><S sid = NA ssid = NA>However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1065.txt | Citing Article:  W10-1756.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is chosen using a grid search on the dev2 set (Table 1).</S><S sid = NA ssid = NA>For each language pair, we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lattice and Forest based search and training procedures are not yet common in statistical machine translation.</S><S sid = NA ssid = NA>In contrast to a phrase-based SMT system, a syntax based SMT system (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A different MBR inspired decoding approach is pursued in Zhang and Gildea (2008) for machine translation using Synchronous Context Free Grammars.</S><S sid = NA ssid = NA>While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Such an approach is also followed in Dreyer et al. (2007).</S><S sid = NA ssid = NA>We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.</S><S sid = NA ssid = NA>The results show that Lattice MBR performance generally improves when the size of the lattice is increased.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.</S><S sid = NA ssid = NA>We now present experiments to evaluate MBR decoding on lattices under the linear corpus BLEU gain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al (2008) replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hypothesis length.</S><S sid = NA ssid = NA>This score is therefore a linear function in counts of words Δc0 and n-gram matches Δcn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare this to N-best MBR with: a) sentence-level BLEU, and b) sentence-level log BLEU.</S><S sid = NA ssid = NA>This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1065.txt | Citing Article:  P09-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We now present MBR decoding on translation lattices.</S><S sid = NA ssid = NA>Several feature functions are then computed over the phrasepairs.</S> | Discourse Facet:  NA | Annotator: Automatic


