Citance Number: 1 | Reference Article:  C04-1080.txt | Citing Article:  D12-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While replicating earlier experiments, Banko and Moore (2004) discovered that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We discovered this was due to several factors.</S><S sid = NA ssid = NA>When using tag to the left and tag to the right as features in addition to the current tag, accuracy improved to 96.55%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1080.txt | Citing Article:  C08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, the expectation maximization algorithm for bi tag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions (Banko and Moore, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Second we describe a method for sequential unsupervised training of tag sequence and lexical probabilities in an HMM, which we observe leads to improved accuracy over simultaneous training with certain types of models.</S><S sid = NA ssid = NA>Second, we revised the training paradigm for HMMs, in which lexical and transition probabilities are typically estimated simultaneously.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1080.txt | Citing Article:  C08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As Banko and Moore (2004) discovered when 5 Note that the POS tag information is not used in these experiments, except for by the C& amp; C tagger.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We discovered this was due to several factors.</S><S sid = NA ssid = NA>It is important to note that this accuracy can be obtained without the intensive training required by Toutanova et. al?s log-linear models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1080.txt | Citing Article:  C08-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To consider the effect of the CCG-based initialization for lexicons with differing ambiguity, I use tag cutoffs that remove any lexical entry containing a category that appears with a particular word less than X% of the time (Banko and Moore, 2004), as well as using no cutoffs at all.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, tags that appeared the tag of a particular word less than X% of the time were omitted from the set of possible tags for that word.</S><S sid = NA ssid = NA>4.2 The Effect of Lexicon Construction on.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1080.txt | Citing Article:  D07-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While this makes unsupervised part-of-speech tagging a relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners.</S><S sid = NA ssid = NA>Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1080.txt | Citing Article:  W09-0715.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Banko and Moore (2004) compared unsupervised HMM and transformation-based taggers trained on the same portions of the Penn Treebank, and showed that the quality of the lexicon used for training had a high impact on the tagging results. Duh and Kirchhoff (2005) presented a minimally supervised approach to tagging for dialectal Arabic (Colloquial Egyptian), based on a morphological analyzer for Modern Standard Arabic and unlabeled texts in a number of dialects.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a comprehensive evaluation of several methods for unsupervised part-of-speech tagging, comparing several variations of hidden Markov model taggers and unsupervised transformation-based learning using the same corpus and same lexicons.</S><S sid = NA ssid = NA>In section 6, we revisit our new approach to HMM tagging, this time, in the supervised framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1080.txt | Citing Article:  D08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Part-Of-Speech Tagging In Context</S><S sid = NA ssid = NA>In this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1080.txt | Citing Article:  P09-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Author Language Average accuracy Toutanova et al (2003) English 97.24% Banko and Moore (2004) English 96.55% Dandapat and Sarkar (2006) Bengali 84.37% Rao et al (2007) Hindi 76.34% Bengali 72.17% Telegu 53.17% Rao and Yarowsky (2007) Hindi 70.67% Bengali 65.47% Telegu 65.85% Sastry et al (2007) Hindi 69.98% Bengali 67.52% Telegu 68.32% Ekbal et al (2007) Hindi 71.65% Bengali 80.63% Telegu 53.15% Ours Assamese 85.64% and searched in the affix-probability table.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, we explore an HMM approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (Lafferty et. al. 2001, Toutanova et. al. 2003).</S><S sid = NA ssid = NA>In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1080.txt | Citing Article:  P07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P (wi|ti? 1, ti ,ti+1) lexical probabilities and found, on the Penn Treebank, that incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>based taggers involves estimating lexical probabilities, P(wi|ti), and tag sequence probabilities, P(ti | ti-1 ... ti-n).</S><S sid = NA ssid = NA>As shown in Table 3, incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%, relatively reducing error rate by 17.4%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1080.txt | Citing Article:  P07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One difficulty with their approach ,noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We did not begin with any estimates of the likelihoods of tags for words, but only the knowledge of what tags are possible for each word in the lexicon, i.e., something we could obtain from a manually-constructed dictionary.</S><S sid = NA ssid = NA>That is, tags that appeared the tag of a particular word less than X% of the time were omitted from the set of possible tags for that word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1080.txt | Citing Article:  P07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We discovered that the Supervised Tagger Test Accuracy Baseline 92.19 Standard HMM 95.87 Contextualized HMM 96.59 Dependency Using LR tag features 96.55 Dependency Best Feature Set 97.24 Table 3: Comparison of Supervised Taggers quality of the lexicon made available to unsupervised learner made the greatest difference to tagging accuracy.</S><S sid = NA ssid = NA>As shown in Table 3, incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%, relatively reducing error rate by 17.4%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1080.txt | Citing Article:  P07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reason why Banko and Moore (2004) get less than HunPos is not because their system is inherently worse, but rather because it lacks the engineering hacks built into TnT and HunPos.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also chose to examine this tagger?s results when using only <ti, t i-1, t i+1> as feature templates, which represents the same amount of context built into our contextualized tagger.</S><S sid = NA ssid = NA>In contrast to the HMM taggers previously described, which make use of contextual information coming from the left side only, UTBL considers both left and right contexts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1080.txt | Citing Article:  P09-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) employ a contrastive estimation tech 1As (Banko and Moore, 2004) point out, unsupervised tagging accuracy varies wildly depending on the dictionary employed.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This tagger was assessed using a tag set other than that which is employed by the Penn Treebank.</S><S sid = NA ssid = NA>This helped stabilize training when estimation of lexical probabilities can be noisy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1080.txt | Citing Article:  C10-2159.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We define the unsupervised part-of-speech (POS) tagging problem as predicting the correct part-of speech tag of a word in a given context using an unlabeled corpus and a dictionary with possible word? tag pairs0 The performance of an unsupervised POS tagging system depends highly on the quality of the word7ujh tag dictionary (Bankoand Moore, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Part-Of-Speech Tagging In Context</S><S sid = NA ssid = NA>That is, tags that appeared the tag of a particular word less than X% of the time were omitted from the set of possible tags for that word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1080.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since we are not starting out with any known estimates for probabilities of tags given a word, the learner considers this tag to be just as likely as the word?s other, more probable, possibilities.</S><S sid = NA ssid = NA>This finding highlights the importance of the need for clean dictionaries whether they are constructed by hand or automatically when we seek to be fully unsupervised.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1080.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5.3 Results.</S><S sid = NA ssid = NA>In addition to comparing against a baseline tagger, which always chooses a word?s most frequent tag, we implemented and trained a version of a standard HMM trigram tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1080.txt | Citing Article:  P10-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we show how these strategies may becombined straightforwardly to produce improvements on the task of learning super taggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While this makes unsupervised part-of-speech tagging a relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners.</S><S sid = NA ssid = NA>In this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1080.txt | Citing Article:  P10-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the standard splits of the data used in semi-supervised tagging experiments (e.g.Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test.CCG-TUT.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Specifically, we allocated sections 00 18 for training, 19-21 for development, and 22-24 for testing.</S><S sid = NA ssid = NA>We discovered that the Supervised Tagger Test Accuracy Baseline 92.19 Standard HMM 95.87 Contextualized HMM 96.59 Dependency Using LR tag features 96.55 Dependency Best Feature Set 97.24 Table 3: Comparison of Supervised Taggers quality of the lexicon made available to unsupervised learner made the greatest difference to tagging accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C04-1080.txt | Citing Article:  W06-1647.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Banko and Moore (2004) showed that unsupervised tagger ac curacies on English degrade from 96% to 77% if the lexicon is not constrained such that only high frequency tags exist in the POS-set for each word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unfiltered Lexicon Optimized Lexicon Merialdo HMM 71.9 93.9 Contextualized HMM 76.9 94.0 Kupiec HMM 77.1 95.9 UTBL 77.2 95.9 Contextualized HMM with Classes 77.2 95.9 Table 1: Tag Accuracy of Unsupervised POS Taggers 5.1 Using Unambiguous Tag Sequences To.</S><S sid = NA ssid = NA>The effect of thresholding tags based on relative frequency in the training set is shown for our set of part-of-speech taggers in the curve in Figure 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C04-1080.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Some of these are annotation errors in the tree bank (Banko and Moore, 2004, Figure 2): such (mis)taggings can severely degrade the accuracy of part-of-speech disambiguators, without additional supervision (Banko and Moore, 2004,? 5, Table 1).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As shown in Table 1, the elimination of noisy possible part-of-speech assignments raised accuracy back into the realm of previously published results.</S><S sid = NA ssid = NA>Figure 2: Manually-Tagged Examples being mistagged during Treebank construction, as shown in the example in Figure 2a.</S> | Discourse Facet:  NA | Annotator: Automatic


