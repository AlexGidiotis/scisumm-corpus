Citance Number: 1 | Reference Article:  C92-3126.txt | Citing Article:  E93-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For a treatment of DOP in more formal terms we refer to (Bod, 1992a).</S> | Reference Offset:  ['36','51'] | Reference Text:  <S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S><S sid = 51 ssid = >An additional remark should be devoted here to formal granlmars and disambiguation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C92-3126.txt | Citing Article:  E93-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Bod, 1992b) super strong equivalence relations between other stochastic grammars are studied.</S> | Reference Offset:  ['53','65'] | Reference Text:  <S sid = 53 ssid = >No one has ever succeeded in doing so except in relatively small grammars.</S><S sid = 65 ssid = >1992 grammaticality and the structure of new utterances univocally.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C92-3126.txt | Citing Article:  E93-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is easy to show that an input string can be parsed with conventional parsing techniques, by applying subtrees instead of rules to the input string (Bod, 1992a).</S> | Reference Offset:  ['38','40'] | Reference Text:  <S sid = 38 ssid = >The only exmt condition is that of every such rule its corresponding construction should be remembered in order to generate a parse-tree for the input string (by composing the constructions that correspond to the rules ilmt are applied).</S><S sid = 40 ssid = >Often we are not interested in all parses of an alnbiguous input string, neither in their exact probabilities, but only in which parse is the preferred parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C92-3126.txt | Citing Article:  P12-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In their place we added a new feature, the probability of a rule's source side tree given its root label, which is essentially the same model used in Data-Oriented Parsing (Bod, 1992).</S> | Reference Offset:  ['0','39'] | Reference Text:  <S sid = 0 ssid = >A Computational Model Of Language Performance: Data Oriented Parsing</S><S sid = 39 ssid = >For a construction t, the corresponding production rule is given by root(t) ~ leaves(O In order to calculate the pteterredparse of an input string by maximizing the conditional probability, all parses with all possible tuples of constructions must be generated, which becomes highly inefficient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C92-3126.txt | Citing Article:  C96-2215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Due to this extension, the one to one mapping between a derivation and a parse tree, which holds in CFGs, does not hold any more; many derivations might generate the same parse-tree ,rl &apos; his seemingly spurious ambiguity turns out crucial for statistical disambiguation as defined in (Bod, 1992) and in (Schabes and Waters, 1993), where the derivations are considered different stochastic processes and their probabilities all contribute to the probability of the generated parse.</S> | Reference Offset:  ['45','46'] | Reference Text:  <S sid = 45 ssid = >In DOP, the probability of a parse depends on all tuples of coustructious that generate that parse.</S><S sid = 46 ssid = >~lhe more different ways in which a parse can be generated, the lligher its probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C92-3126.txt | Citing Article:  P01-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >context-free rulesCharniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4.</S> | Reference Offset:  ['36','38'] | Reference Text:  <S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S><S sid = 38 ssid = >The only exmt condition is that of every such rule its corresponding construction should be remembered in order to generate a parse-tree for the input string (by composing the constructions that correspond to the rules ilmt are applied).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C92-3126.txt | Citing Article:  N06-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It occurs because many systems, such as the ones proposed by (Bod, 1992), (Galley, et .al., 2004), and (Langkilde and Knight, 1998) represent their result space in terms of weighted partial results of various sizes that may be assembled in multiple ways.</S> | Reference Offset:  ['6','46'] | Reference Text:  <S sid = 6 ssid = >Lexical labels represent words.</S><S sid = 46 ssid = >~lhe more different ways in which a parse can be generated, the lligher its probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C92-3126.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The Data-Oriented Parsing (DOP) method suggested in Scha (1990) and developed in Bod (19921995) is a probabilistic parsing strategy which does not single out a narrowly predefined set of structures as the statistically significant ones.</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >A Computational Model Of Language Performance: Data Oriented Parsing</S><S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C92-3126.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In Bod (1992, 1993a), a first instantiation of this model is given, called DOP1, which uses (1) labelled trees for the utterance analyses, (2) subtrees for the fragments, (3) node substitution for combining subtrees, and (4), the sum of the probabilities of all distinct ways of generating an analysis as a def &apos ;mition of the probability of that analysis.</S> | Reference Offset:  ['33','46'] | Reference Text:  <S sid = 33 ssid = >+/- P (E I~E2~ ... c~lS k) We will use Bayes' decomposition formula to derive the conditional probability of "1) given s. Let 7/~ and Tj be parses of s; the conditional probability of T i given s, is illen given by: P(Ti)P(sFI" i) P(r)P(srl~) V(7)ts) . . .</S><S sid = 46 ssid = >~lhe more different ways in which a parse can be generated, the lligher its probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C92-3126.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bod (1992, 1993a) shows that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1.</S> | Reference Offset:  ['36','45'] | Reference Text:  <S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S><S sid = 45 ssid = >In DOP, the probability of a parse depends on all tuples of coustructious that generate that parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C92-3126.txt | Citing Article:  C00-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Data-Oriented Parsing Bothprobabil is tic and non-probabilistic DOP are based on the DOP model in Bod (1992) which extracts a Stochastic Tree-Substitution Grammar.</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >A Computational Model Of Language Performance: Data Oriented Parsing</S><S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C92-3126.txt | Citing Article:  P98-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Bod (1992) demonstrated that DOP can be implemented using conventional context-free parsing techniques.</S> | Reference Offset:  ['36','42'] | Reference Text:  <S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S><S sid = 42 ssid = >"llais call be achieved by using Monte Carlo techniques (see e.g. \[Hammersley 1964\]): we estimate the preferred parse by taking random samples frotn the space of possibilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C92-3126.txt | Citing Article:  P98-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Next, parsing proceeds with the subtrees that are triggered by the dialogue context C (provided that all subtrees are converted into equivalent rewrite rules -see Bod 1992, Sima &apos; an 1995).</S> | Reference Offset:  ['0','36'] | Reference Text:  <S sid = 0 ssid = >A Computational Model Of Language Performance: Data Oriented Parsing</S><S sid = 36 ssid = >In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S> | Discourse Facet:  NA | Annotator: Automatic


