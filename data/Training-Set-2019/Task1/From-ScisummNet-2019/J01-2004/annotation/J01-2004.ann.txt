Citance Number: 1 | Reference Article:  J01-2004.txt | Citing Article:  W05-0104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S sid = NA ssid = NA>The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J01-2004.txt | Citing Article:  P08-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid = NA ssid = NA>The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J01-2004.txt | Citing Article:  W04-0307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown</S><S sid = NA ssid = NA>We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This last model shows the performance from the acoustic model alone, without the influence of the language model.</S><S sid = NA ssid = NA>The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This last model shows the performance from the acoustic model alone, without the influence of the language model.</S><S sid = NA ssid = NA>It is not part of the probability model itself.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S><S sid = NA ssid = NA>That is, search efficiency for these parsers is improved by both statistical search heuristics and DP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is an incremental parser with a pruning strategy and no backtracking.</S><S sid = NA ssid = NA>We implement this as a beam search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown</S><S sid = NA ssid = NA>We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J01-2004.txt | Citing Article:  P04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S><S sid = NA ssid = NA>The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J01-2004.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S><S sid = NA ssid = NA>Furthermore, this is quite a large beam (see discussion below), so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J01-2004.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>At the end one has a beam-width's number of best parses (Roark, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>And so on until the end of the string.</S><S sid = NA ssid = NA>In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J01-2004.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Like the nonlexicalized parser in Roark and Johnson (1999), we found that the search efficiency, in terms of number of rule expansions considered or number of analyses advanced, also improved as we increased the amount of conditioning.</S><S sid = NA ssid = NA>In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J01-2004.txt | Citing Article:  P04-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The n-best lists were provided by Brian Roark (Roark, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S sid = NA ssid = NA>The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J01-2004.txt | Citing Article:  P05-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown</S><S sid = NA ssid = NA>We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J01-2004.txt | Citing Article:  W10-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Probabilistic Top-Down Parsing And Language Modeling</S><S sid = NA ssid = NA>A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J01-2004.txt | Citing Article:  W10-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown</S><S sid = NA ssid = NA>We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J01-2004.txt | Citing Article:  D09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Future research will show if this early promise can be fully realized.</S><S sid = NA ssid = NA>The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur, as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J01-2004.txt | Citing Article:  D09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Perplexity is the exponential of the cross entropy, which we will define next.</S><S sid = NA ssid = NA>A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J01-2004.txt | Citing Article:  D09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We will focus our very brief review, however, on those that use grammars or parsing for their language models.</S><S sid = NA ssid = NA>There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J01-2004.txt | Citing Article:  D09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;', the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT'', that is, the total probability mass represented by the priority queues is monotonically decreasing.</S><S sid = NA ssid = NA>Let Dw, be the set of all partial derivations for a prefix string 4.</S> | Discourse Facet:  NA | Annotator: Automatic


