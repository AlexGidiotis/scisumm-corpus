Citance Number: 1 | Reference Article:  J93-2006.txt | Citing Article:  A00-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If we want to determine the most likely syntactic part of speech or tag for each word in a sentence, we can formulate a probabilistic tagging model.</S><S sid = NA ssid = NA>&quot;Augmenting a hidden Markov model for phrase-dependent tagging.&quot; In Speech and Language Workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J93-2006.txt | Citing Article:  A00-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, capitalization information, when available, can help to indicate whether a word is a proper noun.</S><S sid = NA ssid = NA>To estimate p(w, I t,) for an unknown word, we first determined the features we thought would distinguish parts of speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J93-2006.txt | Citing Article:  P98-2251.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (Ayuso, Shaked, and Weischedel 1987; Bates 1989; Grishman, Hirschman, and Nhan 1986; Weischedel et al. 1989).</S><S sid = NA ssid = NA>Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTm,1 and Delphi (Stallard 1989), have used these techniques quite successfully.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J93-2006.txt | Citing Article:  C96-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can incorporate these features of the word into the probability that this particular word will occur given a particular tag using the following: We estimate the probability of each ending for each tag directly from supervised training data.</S><S sid = NA ssid = NA>The exact probability of a particular tag given a particular word is computed directly by the product of the &quot;forward&quot; and &quot;backward&quot; probabilities to that tag, divided by the probability of the word sequence given this model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J93-2006.txt | Citing Article:  P01-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are four independent categories of features: inflectional endings, derivational endings, hyphenation, and capitalization; these are not necessarily independent, though we are treating them as such for our tests.</S><S sid = NA ssid = NA>The disadvantage of this is that uses of a word that did not occur in the training set will be unknown to the system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J93-2006.txt | Citing Article:  A97-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Some well-known previous efforts (Church 1988; de Marcken 1990) have dealt with unknown words using various heuristics.</S><S sid = NA ssid = NA>Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J93-2006.txt | Citing Article:  C00-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are four independent categories of features: inflectional endings, derivational endings, hyphenation, and capitalization; these are not necessarily independent, though we are treating them as such for our tests.</S><S sid = NA ssid = NA>The disadvantage of this is that uses of a word that did not occur in the training set will be unknown to the system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J93-2006.txt | Citing Article:  W07-0813.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The new aspects of our work are (1) incorporating the treatment of unknown words uniformly within the probability model, (2) approximating the component probabilities for unknowns directly from the training data, and (3) measuring the contribution of the tri-tag model, of the ending, and of capitalization.</S><S sid = NA ssid = NA>In sum, adding a probability model of typical endings of words to the trkag model has yielded an accuracy of 82% for unknown words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J93-2006.txt | Citing Article:  A00-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Hindle and Rooth's test, they evaluated their probability model in the limited case of verb—noun phrase—prepositional phrase.</S><S sid = NA ssid = NA>Simple rules can predict which word designates the semantic clause of a noun phrase very reliably.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J93-2006.txt | Citing Article:  P08-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In order to reduce the ambiguity further, we tested various ways to limit how many tags were returned based on their probabilities.</S><S sid = NA ssid = NA>Some well-known previous efforts (Church 1988; de Marcken 1990) have dealt with unknown words using various heuristics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J93-2006.txt | Citing Article:  W05-0404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given these probabilities, one can then find the most likely tag sequence for a given word sequence.</S><S sid = NA ssid = NA>, tr,l, given a particular word sequence, where p(T) is the a priori probability of tag sequence T, p(W I T) is the conditional probability of word sequence W occurring given that a sequence of tags T occurred, and p(W) is the unconditioned probability of word sequence W. Then, in principle, we can consider all possible tag sequences, evaluate p(T I W) of each, and choose the tag sequence T that is most likely, i.e., the sequence that maximizes p(T I W).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J93-2006.txt | Citing Article:  W04-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to the ending, Weischedel et al (1993) exploit capitalisation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity (Ayuso, Shaked, and Weischedel 1987; Bates 1989; Grishman, Hirschman, and Nhan 1986; Weischedel et al. 1989).</S><S sid = NA ssid = NA>Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTm,1 and Delphi (Stallard 1989), have used these techniques quite successfully.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J93-2006.txt | Citing Article:  E95-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here the algorithm tries to combine the constituent to the right of the conjunction with that on the left of the conjunction.</S><S sid = NA ssid = NA>Furthermore, capitalization information, when available, can help to indicate whether a word is a proper noun.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J93-2006.txt | Citing Article:  W99-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here the algorithm tries to combine the constituent to the right of the conjunction with that on the left of the conjunction.</S><S sid = NA ssid = NA>Furthermore, capitalization information, when available, can help to indicate whether a word is a proper noun.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J93-2006.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Of the word tags, 22 are tags for open class words and 14 for closed class words.</S><S sid = NA ssid = NA>Note the first word, &quot;Bailey,&quot; is unknown to the system, therefore, all of the open class tags are possible.</S> | Discourse Facet:  NA | Annotator: Automatic


