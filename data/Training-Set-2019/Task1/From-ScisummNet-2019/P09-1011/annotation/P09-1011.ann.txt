Citance Number: 1 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, in the NFL domain, 70% of scoring records are mentioned whereas only 1% of punting records are mentioned.</S><S sid = NA ssid = NA>We have presented a generative model of correspondences between a world state and an unsegmented stream of text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model 2’ achieved an F1 of 39.9, an improvement over Model 1, which attained 32.8.</S><S sid = NA ssid = NA>The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used the dataset created by Liang et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second domain is weather forecasts, for which we created a new dataset.</S><S sid = NA ssid = NA>We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001–2004 Robocup finals.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each record type t E T has a separate field choice model, which specifies a distribution over a sequence of fields.</S><S sid = NA ssid = NA>The field type determines the way we expect the field value to be rendered in words: integer fields can be numerically perturbed, string fields can be spliced, and categorical fields are represented by open-ended word distributions, which are to be learned.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, a word is chosen from the parameters learned in the model of Liang et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006).</S><S sid = NA ssid = NA>Table 2 shows the top words for each of these field values learned by our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the model of Liang et al (2009) to impute the decisions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word.</S><S sid = NA ssid = NA>We use the term scenario to refer to such a (w, s) pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unless otherwise specified, performance is reported on all scenarios, which were also used for training.</S><S sid = NA ssid = NA>Furthermore, in some of our examples, much of the world state is not referenced at all in the text, and, conversely, the text references things which are not represented in our world state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1011.txt | Citing Article:  D10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a generative model of correspondences between a world state and an unsegmented stream of text.</S><S sid = NA ssid = NA>The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Learning Semantic Correspondences with Less Supervision</S><S sid = NA ssid = NA>A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this less restricted data setting, we must resolve multiple ambiguities: (1) the segmentation of the text into utterances; (2) the identification of relevant facts, i.e., the choice of records and aspects of those records; and (3) the alignment of utterances to facts (facts are the meaning representations of the utterances).</S><S sid = NA ssid = NA>The parameters of this hierarchical hidden semi-Markov model can be estimated efficiently using EM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also compare our model to the results of Chen and Mooney (2008) in Table 4.</S><S sid = NA ssid = NA>Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our model combines segmentation with alignment.</S><S sid = NA ssid = NA>DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.</S><S sid = NA ssid = NA>Fortunately, most of the fields are integer fields or string fields (generally names or brief descriptions), which provide important anchor points for learning the correspondences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a generative model of correspondences between a world state and an unsegmented stream of text.</S><S sid = NA ssid = NA>We believe that further progress is possible with a richer model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each record type t E T has a separate field choice model, which specifies a distribution over a sequence of fields.</S><S sid = NA ssid = NA>Conditioned on the fields f, the words w are generated independently:4 where r(k) and f(k) are the record and field responsible for generating word wk, as determined by the segmentation c. The word choice model pw(w I t, v) specifies a distribution over words given the field type t and field value v. This distribution is a mixture of a global backoff distribution over words and a field-specific distribution which depends on the field type t. Although we designed our word choice model to be relatively general, it is undoubtedly influenced by the three domains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 6 shows our results.</S><S sid = NA ssid = NA>The parameters of this hierarchical hidden semi-Markov model can be estimated efficiently using EM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.</S><S sid = NA ssid = NA>Fortunately, most of the fields are integer fields or string fields (generally names or brief descriptions), which provide important anchor points for learning the correspondences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, the complexity of the language used in the recap is far greater than what we can represent using our simple model.</S><S sid = NA ssid = NA>We did not experiment with Model 3 since the discourse structure on records in this domain is not at all governed by a simple Markov model on record types—indeed, most regions do not refer to any records at all.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Learning Semantic Correspondences with Less Supervision</S><S sid = NA ssid = NA>Model 2’ achieved an F1 of 39.9, an improvement over Model 1, which attained 32.8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1011.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Learning Semantic Correspondences with Less Supervision</S><S sid = NA ssid = NA>We arrive at the final component of our model, which governs how the information about a particular field of a record is rendered into words.</S> | Discourse Facet:  NA | Annotator: Automatic


