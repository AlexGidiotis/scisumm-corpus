Citance Number: 1 | Reference Article:  C08-1022.txt | Citing Article:  W09-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.</S><S sid = NA ssid = NA>We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C08-1022.txt | Citing Article:  W12-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We c ? 2008.</S><S sid = NA ssid = NA>In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C08-1022.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.</S><S sid = NA ssid = NA>Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C08-1022.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best results of 92.15% are reported by De Felice and Pulman (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The best reported results to date on determiner selection are those in Turner and Charniak (2007).</S><S sid = NA ssid = NA>We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C08-1022.txt | Citing Article:  W12-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English</S><S sid = NA ssid = NA>We c ? 2008.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C08-1022.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.</S><S sid = NA ssid = NA>We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C08-1022.txt | Citing Article:  P10-2065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We c ? 2008.</S><S sid = NA ssid = NA>Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C08-1022.txt | Citing Article:  W11-1412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)).</S><S sid = NA ssid = NA>Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C08-1022.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We c ? 2008.</S><S sid = NA ssid = NA>We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C08-1022.txt | Citing Article:  D10-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We c ? 2008.</S><S sid = NA ssid = NA>Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C08-1022.txt | Citing Article:  W12-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.</S><S sid = NA ssid = NA>We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.</S> | Discourse Facet:  NA | Annotator: Automatic


