Citance Number: 1 | Reference Article:  E03-1009.txt | Citing Article:  C04-1191.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E03-1009.txt | Citing Article:  P09-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E03-1009.txt | Citing Article:  D09-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Word baseline each word is in its own class.</S><S sid = NA ssid = NA>This algorithm iteratively improves the likelihood of a given clustering by moving each word from its current cluster to the cluster that will give the maximum increase in likelihood, or leaving it in its original cluster if no improvement can be found.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E03-1009.txt | Citing Article:  C08-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If we have a random set of tags the mutual information will be zero and the conditional entropy will be the same as the entropy of the tag set.</S><S sid = NA ssid = NA>First, early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E03-1009.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This just has the effect of discriminating between classes that will have lots of types (i.e. open class clusters) and clusters that tend to have few types (corresponding to closed class words).</S><S sid = NA ssid = NA>We selected the 10 clusters with the largest number of zero frequency word types in.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E03-1009.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 5 shows how the conditional entropy varies with respect to the frequency for these models.</S><S sid = NA ssid = NA>Combining Distributional And Morphological Information For Part Of Speech Induction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E03-1009.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These are summarised in Table 2.</S><S sid = NA ssid = NA>Combining Distributional And Morphological Information For Part Of Speech Induction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E03-1009.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model.</S><S sid = NA ssid = NA>We have also evaluated this method by comparing the perplexity of a class-based language model derived from these classes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E03-1009.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 4 shows the results of the perplexity evaluation on the WSJ data.</S><S sid = NA ssid = NA>We used the full vocabulary of the training and test sets together which was 45679, of which 14576 had frequency zero in the training data and thus had to be categorised based solely on their morphology and frequency.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E03-1009.txt | Citing Article:  D09-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E03-1009.txt | Citing Article:  D09-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compared different models with varying numbers of clusters: 32 64 and 128.</S><S sid = NA ssid = NA>A second form of evaluation is to use some data that has been manually or semi-automatically annotated with part of speech (POS) tags, and to use some information theoretic measure to look at the correlation between the 'correct' data and the induced POS tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E03-1009.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E03-1009.txt | Citing Article:  P10-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We performed experiments on parts of the Wall Street Journal corpus, using the corpus tags.</S><S sid = NA ssid = NA>In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E03-1009.txt | Citing Article:  P10-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The task studied in this paper is the unsupervised learning of parts-of-speech, that is to say lexical categories corresponding to traditional notions of, for example, nouns and verbs.</S><S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E03-1009.txt | Citing Article:  E09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E03-1009.txt | Citing Article:  W11-1806.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Word baseline each word is in its own class.</S><S sid = NA ssid = NA>We performed experiments on parts of the Wall Street Journal corpus, using the corpus tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E03-1009.txt | Citing Article:  D07-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).</S><S sid = NA ssid = NA>As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E03-1009.txt | Citing Article:  D07-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As Clark (2003) points out, many-to-1 accuracy has several defects.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are particularly interested in rare words: as (Rosenfeld, 2000, pp.1313-1314) points out, it is most important to cluster the infrequent words, as we will have reliable information about the frequent words; and yet it is these words that are most difficult to cluster.</S><S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E03-1009.txt | Citing Article:  W10-2911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).</S><S sid = NA ssid = NA>We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E03-1009.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).</S><S sid = NA ssid = NA>This relates to two other approaches that we are aware of (Fine et al., 1998) and (Weber et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


