Citance Number: 1 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The current state of the art discovers many semantic classes but fails to label their concepts.</S><S sid = NA ssid = NA>In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ours is a top down approach.</S><S sid = NA ssid = NA>Finally, we use simple syntactic patterns to discover class names from each class' signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts.</S><S sid = NA ssid = NA>Automatically Labeling Semantic Classes</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The top-4 highest scoring relationships are: To name a class, we simply search for these syntactic relationships in the signature of a concept.</S><S sid = NA ssid = NA>Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1041.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ours is a top down approach.</S><S sid = NA ssid = NA>One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1041.txt | Citing Article:  P10-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.</S><S sid = NA ssid = NA>In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1041.txt | Citing Article:  P11-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we use simple syntactic patterns to discover class names from each class' signature.</S><S sid = NA ssid = NA>Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1041.txt | Citing Article:  W08-1807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S><S sid = NA ssid = NA>Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1041.txt | Citing Article:  C10-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This research was partly supported by NSF grant #EIA-0205111.</S><S sid = NA ssid = NA>That is, they use patterns to independently discover semantic relationships of words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1041.txt | Citing Article:  E09-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.</S><S sid = NA ssid = NA>We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1041.txt | Citing Article:  P08-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Because of the low coverage of proper nouns in WordNet, only 33 of the 125 concepts we evaluated had WordNet generated labels.</S><S sid = NA ssid = NA>For the 33 concepts that WordNet named, it achieved a score of 75.3% and a lenient score of 82.7%, which is high considering the simple algorithm we used to extract labels using WordNet.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1041.txt | Citing Article:  P06-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.</S><S sid = NA ssid = NA>There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1041.txt | Citing Article:  P08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The assumption is that the best representative for a concept is a large set of very similar instances.</S><S sid = NA ssid = NA>For each concept that contains at least five instances in the WordNet hierarchy, we named the concept with the most frequent common ancestor of each pair of instances.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1041.txt | Citing Article:  P08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class.</S><S sid = NA ssid = NA>Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1041.txt | Citing Article:  C10-2110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given a question such as &quot;What color ...&quot;, the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately.</S><S sid = NA ssid = NA>Finally, we use simple syntactic patterns to discover class names from each class' signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1041.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ours is a top down approach.</S><S sid = NA ssid = NA>Finally, we use simple syntactic patterns to discover class names from each class' signature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1041.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We therefore multiply mief with the following discounting factor: n m where n is the number of words and N = cef × By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.</S><S sid = NA ssid = NA>In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1041.txt | Citing Article:  P05-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.</S><S sid = NA ssid = NA>Automatically Labeling Semantic Classes</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1041.txt | Citing Article:  W09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also, the words dog, computer and company all have a sense that is a hyponym of person.</S><S sid = NA ssid = NA>For example, WordNet includes a rare sense of computer that means `the person who computes'.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1041.txt | Citing Article:  C10-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: Following (Pantel and Lin 2002), we construct a committee for each semantic class.</S><S sid = NA ssid = NA>The two columns of numbers indicate the frequency and mutual information score for each feature respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


