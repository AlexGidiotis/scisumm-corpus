Citance Number: 1 | Reference Article:  P08-1068.txt | Citing Article:  P14-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures.</S><S sid = NA ssid = NA>Our feature mappings are quite high-dimensional, so we eliminated all features which occur only once in the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1068.txt | Citing Article:  P14-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).</S><S sid = NA ssid = NA>Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1068.txt | Citing Article:  W09-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005).</S><S sid = NA ssid = NA>As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1068.txt | Citing Article:  P11-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Based on Table 3, we can extrapolate that cluster-based features reduce the need for supervised data by roughly a factor of 2.</S><S sid = NA ssid = NA>By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1068.txt | Citing Article:  W10-1409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that our feature sets were originally tuned for English parsing, and except for the use of Czech clusters, we made no attempt to retune our features for Czech.</S><S sid = NA ssid = NA>Dependency parsing depends critically on predicting head-modifier relationships, which can be difficult due to the statistical sparsity of these word-to-word interactions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1068.txt | Citing Article:  W10-1409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).</S><S sid = NA ssid = NA>Our research, however, applies this technique to dependency parsing rather than named-entity recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1068.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).</S><S sid = NA ssid = NA>Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1068.txt | Citing Article:  N12-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).</S><S sid = NA ssid = NA>Similar observations regarding the effect of model order have also been made by Carreras (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Simple Semi-supervised Dependency Parsing</S><S sid = NA ssid = NA>For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Miller et al. (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.</S><S sid = NA ssid = NA>Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first- and second-order cluster-based feature sets are supersets of the baseline feature sets: they include all of the baseline feature templates, and add an additional layer of features that incorporate word clusters.</S><S sid = NA ssid = NA>In our experiments, we employed two different feature sets: a baseline feature set which draws upon “normal” information sources such as word forms and parts of speech, and a cluster-based feature set that also uses information derived from the Brown cluster hierarchy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).</S><S sid = NA ssid = NA>We briefly describe the Brown algorithm below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters.</S><S sid = NA ssid = NA>In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1068.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).</S><S sid = NA ssid = NA>Terry Koo was funded by NSF grant DMS-0434222 and a grant from NTT, Agmt.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Simple Semi-supervised Dependency Parsing</S><S sid = NA ssid = NA>In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.</S><S sid = NA ssid = NA>We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Simple Semi-supervised Dependency Parsing</S><S sid = NA ssid = NA>We present a simple and effective semisupervised method for training dependency parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Simple Semi-supervised Dependency Parsing</S><S sid = NA ssid = NA>In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser.</S><S sid = NA ssid = NA>First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1068.txt | Citing Article:  D09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section.</S><S sid = NA ssid = NA>The development and test sets were not projectivized, so our secondorder parser is guaranteed to make errors in test sentences containing non-projective dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


