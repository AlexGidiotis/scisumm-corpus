Citance Number: 1 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Headden III et al (2009) showed that performance could be improved by including high frequency words as well as tags in their model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S><S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The final base distribution over CFG-DMV rules (Psh) is inspired by the skip-head smoothing model of Headden III et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We call this EVG smoothed-skip-head.</S><S sid = NA ssid = NA>As we see below, backing off by ignoring the part-ofspeech of the head H worked better than ignoring the argument position v. For L-EVG we smooth the argument part-ofspeech distribution (conditioned on the head word) with the unlexicalized EVG smoothed-skip-head model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S><S sid = NA ssid = NA>For the Lexicalized-EVG, we modify this procedure slightly, by first running MB smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S><S sid = NA ssid = NA>Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1012.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This graph indicates that the improvements in the posterior probability of the model are correlated with the evaluation, though the correlation is not as high as we might require in order to use LLH as a model selection criteria similar to Headden III et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 1) where each edge indicates a head-argument relation.</S><S sid = NA ssid = NA>To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate ¯θ′.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1012.txt | Citing Article:  P13-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We call this the Extended Valence Grammar (EVG).</S><S sid = NA ssid = NA>In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1012.txt | Citing Article:  D12-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Klein and Manning (2004) use Expectation Maximization to estimate the model parameters.</S><S sid = NA ssid = NA>Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the The big dog barks Dependency Model with Valence (DMV) by Klein and Manning (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The most successful recent work on dependency induction has focused on the Dependency Model with Valence (DMV) by Klein and Manning (2004).</S><S sid = NA ssid = NA>Klein and Manning (2004) use Expectation Maximization to estimate the model parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An additional point of comparison is the lexicalized unsupervised parser of Headden III et al (2009), which yields the current state-of-the-art unsupervised accuracy on English at 68.8%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that smoothing can be employed in an unsupervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-ofthe-art results on this task.</S><S sid = NA ssid = NA>DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S><S sid = NA ssid = NA>Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As in the previous section, we find that the presence of linguistic rules greatly reduces this sensitivity: for HDP-DEP, the standard deviation over five randomly initialized runs with the English-specific rules is 1.5%, compared to 4.5% for the parser developed by Headden III et al. (2009) and 8.0% for DMV (Klein and Manning, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Q(¯θ) is then initialized using the standard VB M-step.</S><S sid = NA ssid = NA>They initialize using the harmonic initializer of Klein and Manning (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1012.txt | Citing Article:  D10-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rule sets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al (2009)) to benefit from their complementary strengths.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.</S><S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1012.txt | Citing Article:  W12-1911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S><S sid = NA ssid = NA>Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1012.txt | Citing Article:  W12-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In Headden III et al (2009), by using the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The dependency models in this paper will be formulated as a particular kind of Probabilistic Context Free Grammar (PCFG), described below.</S><S sid = NA ssid = NA>Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For better comparison with previous work we implemented three model extensions, borrowed from Headden III et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S><S sid = NA ssid = NA>As in previous work, we restrict ourselves to projective dependency trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We fix λ = 1/3, which is a crude approximation to the value learned by Headden III et al. (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S><S sid = NA ssid = NA>Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Headden III et al (2009) also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cohen et al. (2008) investigate using Bayesian Priors with DMV.</S><S sid = NA ssid = NA>We compare to work by Cohen et al. (2008) and Cohen and Smith (2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1012.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S><S sid = NA ssid = NA>Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1012.txt | Citing Article:  N12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we would like to learn the parts-of-speech in our dependency model from text and not rely on the gold-standard tags.</S><S sid = NA ssid = NA>Since rules in the same equivalence class have the same probability, In order to perform smoothing, we will find useful a class of PCFGs in which the probabilities of certain rules are required to be the same.</S> | Discourse Facet:  NA | Annotator: Automatic


