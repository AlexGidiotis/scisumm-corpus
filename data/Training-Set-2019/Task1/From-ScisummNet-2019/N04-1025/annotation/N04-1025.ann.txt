Citance Number: 1 | Reference Article:  N04-1025.txt | Citing Article:  W12-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S><S sid = NA ssid = NA>To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1025.txt | Citing Article:  D08-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their comments and Luo Si for helpful discussions.</S><S sid = NA ssid = NA>This is a well-known issue in language model applications, and it is standard to compensate for this sparseness by smoothing the frequencies in the trained models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1025.txt | Citing Article:  W06-1809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our statistical model is based on a variation of the multinomial naïve Bayes classifier, which we call the ‘Smoothed Unigram’ model.</S><S sid = NA ssid = NA>We have shown that reading difficulty can be estimated with a simple language modeling approach using a modified naïve Bayes classifier.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1025.txt | Citing Article:  W06-1809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While word difficulty is well-known to be an excellent predictor of reading difficulty (Chall & Edgar, 1995), it was not at all clear how effective our language model approach would be for predicting Web page reading difficulty.</S><S sid = NA ssid = NA>Our evaluation suggests that reasonably effective models can be trained with small amounts of easilyacquired data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1025.txt | Citing Article:  C10-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S><S sid = NA ssid = NA>As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1025.txt | Citing Article:  N07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S><S sid = NA ssid = NA>Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1025.txt | Citing Article:  H05-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, Reading A-Z documents were written to pre-established criteria which includes objective factors such as type/ token ratio (Reading A-Z.com, 2003), so it is not surprising that the correlation is high.</S><S sid = NA ssid = NA>To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1025.txt | Citing Article:  H05-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their comments and Luo Si for helpful discussions.</S><S sid = NA ssid = NA>This is a well-known issue in language model applications, and it is standard to compensate for this sparseness by smoothing the frequencies in the trained models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1025.txt | Citing Article:  W11-2308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous work (Stenner, 1996, also citing Squires et al., 1983 and Crawford et al., 1975) suggests that a comprehension rate of 75% for a text is a desirable target.</S><S sid = NA ssid = NA>As an example of retraining, we showed that the classifier obtained good correlation with difficulty for at least two languages, English and French, with the only algorithm difference being a change in the morphology handling during feature processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1025.txt | Citing Article:  N12-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also showed that the Smoothed Unigram method is robust for short passages and Web documents.</S><S sid = NA ssid = NA>As we show in our evaluation, this generally results in better accuracy for Web documents and very short passages (less than 10 words).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1025.txt | Citing Article:  W08-1605.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We derived the learning curve of our classifier as a function of the mean model training set size in tokens.</S><S sid = NA ssid = NA>Our statistical model is based on a variation of the multinomial naïve Bayes classifier, which we call the ‘Smoothed Unigram’ model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1025.txt | Citing Article:  W12-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).</S><S sid = NA ssid = NA>We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1025.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also included a fourth predictor: the FleschKincaid score (Kincaid et al. 1975), which is a linear combination of the text’s average sentence length (in tokens), and the average number of syllables per token.</S><S sid = NA ssid = NA>Widely-used traditional readability formulas such as Flesch-Kincaid usually perform poorly in this scenario.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1025.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While traditional formulas are based on linear regression with two or three variables, statistical language models can capture more detailed patterns of individual word usage.</S><S sid = NA ssid = NA>The word ‘the’ is very common and varies less in frequency across grade levels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1025.txt | Citing Article:  E09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Section 3 summarizes related work on readability, focusing on existing vocabulary-based measures that can be thought of as simplified language model techniques.</S><S sid = NA ssid = NA>The results from the most closely related previous work (Si and Callan, 2001) are not directly comparable to ours; among other factors, their task used a dataset trained on science curriculum descriptions, not text written at different levels of difficulty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1025.txt | Citing Article:  E09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results from the most closely related previous work (Si and Callan, 2001) are not directly comparable to ours; among other factors, their task used a dataset trained on science curriculum descriptions, not text written at different levels of difficulty.</S><S sid = NA ssid = NA>A comprehensive summary of early readability work may be found in Chall (1958) and Klare (1963).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1025.txt | Citing Article:  D12-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we looked at how well the model could be extended to a language other than English – in this study, we give results for French.</S><S sid = NA ssid = NA>To test the flexibility of our language model approach, we did a preliminary study for French reading difficulty prediction.</S> | Discourse Facet:  NA | Annotator: Automatic


