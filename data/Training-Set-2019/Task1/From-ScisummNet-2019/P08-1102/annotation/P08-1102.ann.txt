Citance Number: 1 | Reference Article:  P08-1102.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Jiang et al (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid = NA ssid = NA>2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1102.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the begin, middle and end of a word respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid = NA ssid = NA>The feature templates we adopted are selected from those of Ng and Low (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1102.txt | Citing Article:  C08-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Templates called lexical-target in the column below are introduced by Jiang et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As predications generated from such templates depend on the current character, we name these templates lexical-target.</S><S sid = NA ssid = NA>Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1102.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid = NA ssid = NA>Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1102.txt | Citing Article:  D12-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid = NA ssid = NA>We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1102.txt | Citing Article:  C10-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the feature templates the same as Jiang et al, (2008) to extract features form E model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All feature templates and their instances are shown in Table 1.</S><S sid = NA ssid = NA>In following subsections, we describe the feature templates and the perceptron training algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1102.txt | Citing Article:  P11-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1102.txt | Citing Article:  P12-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first is the "character-based" approach, where basic processing units are characters which compose words (Jiang et al., 2008a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Besides the usual character-based features, additional features dependent on POSâ€™s or words can also be employed to improve the performance.</S><S sid = NA ssid = NA>Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1102.txt | Citing Article:  C10-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1102.txt | Citing Article:  C10-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid = NA ssid = NA>In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1102.txt | Citing Article:  C10-2096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S><S sid = NA ssid = NA>When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.</S><S sid = NA ssid = NA>As predications generated from such templates depend on the current character, we name these templates lexical-target.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0]).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S><S sid = NA ssid = NA>The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid = NA ssid = NA>Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Inspired by (Jiang et al, 2008), we set the real-value of C0 to be 2.0, the value of C-1C0 and C0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x âˆˆ X to outputs y âˆˆ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid = NA ssid = NA>Besides the usual character-based features, additional features dependent on POSâ€™s or words can also be employed to improve the performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid = NA ssid = NA>Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1102.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1102.txt | Citing Article:  D10-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1102.txt | Citing Article:  D12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.</S><S sid = NA ssid = NA>With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.</S> | Discourse Facet:  NA | Annotator: Automatic


