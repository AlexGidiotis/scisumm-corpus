Citance Number: 1 | Reference Article:  P07-1032.txt | Citing Article:  W07-1202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types.</S><S sid = NA ssid = NA>In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1032.txt | Citing Article:  P14-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005).</S><S sid = NA ssid = NA>The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1032.txt | Citing Article:  C08-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000).</S><S sid = NA ssid = NA>Carroll et al. (1998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1032.txt | Citing Article:  S10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory rules which combine the cate- and Curran, 2004b), with impressive results of over gories (Steedman, 2000).</S><S sid = NA ssid = NA>Formalism-Independent Parser Evaluation with CCG and DepBank</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1032.txt | Citing Article:  S10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult.</S><S sid = NA ssid = NA>This F-score is an upper bound on the performance of the CCG parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1032.txt | Citing Article:  S10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.</S><S sid = NA ssid = NA>Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1032.txt | Citing Article:  S10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3 The CCG Parser declarative sentence is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation.</S><S sid = NA ssid = NA>The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1032.txt | Citing Article:  W08-1304.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al.</S><S sid = NA ssid = NA>Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1032.txt | Citing Article:  D10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(1998) gold-standard.</S><S sid = NA ssid = NA>Thus the parser output was never used during this process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1032.txt | Citing Article:  D10-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult.</S><S sid = NA ssid = NA>The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1032.txt | Citing Article:  P08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The features in the model are Penn Treebank.</S><S sid = NA ssid = NA>The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1032.txt | Citing Article:  W08-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b).</S><S sid = NA ssid = NA>In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1032.txt | Citing Article:  W08-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formalism-Independent Parser Evaluation with CCG and DepBank</S><S sid = NA ssid = NA>There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1032.txt | Citing Article:  W08-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formalism-Independent Parser Evaluation with CCG and DepBank</S><S sid = NA ssid = NA>There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1032.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Kaplan et al. (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that “This conversion was relatively straightforward for LFG structures ...</S><S sid = NA ssid = NA>Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1032.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG super tagger; and three, a hierarchical level consisting of CCG derivations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S><S sid = NA ssid = NA>The numerical case is identified using two rules: the num subtype is added if any argument in a GR is assigned the lexical category N/N [num], and if any of the arguments in an ncmod is POS tagged CD. prt is added to an ncmod if the modifiee has any of the verb POS tags and if the modifier has POS tag RP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1032.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results for CCGbank were obtained using the oracle method described above.</S><S sid = NA ssid = NA>3 The CCG Parser declarative sentence is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1032.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000).</S><S sid = NA ssid = NA>The coverage of the parser on DepBank is 100%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1032.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.</S><S sid = NA ssid = NA>Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1032.txt | Citing Article:  D08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al.</S><S sid = NA ssid = NA>A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


