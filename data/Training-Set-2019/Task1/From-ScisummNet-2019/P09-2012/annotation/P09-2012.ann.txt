Citance Number: 1 | Reference Article:  P09-2012.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.</S><S sid = NA ssid = NA>In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-2012.txt | Citing Article:  D11-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.</S><S sid = NA ssid = NA>In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-2012.txt | Citing Article:  D11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009).</S><S sid = NA ssid = NA>prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-2012.txt | Citing Article:  P12-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009).</S><S sid = NA ssid = NA>prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-2012.txt | Citing Article:  P12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.</S><S sid = NA ssid = NA>In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-2012.txt | Citing Article:  P11-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.</S><S sid = NA ssid = NA>In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-2012.txt | Citing Article:  P11-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.</S><S sid = NA ssid = NA>Bayesian Learning of a Tree Substitution Grammar</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-2012.txt | Citing Article:  P11-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bayesian Learning of a Tree Substitution Grammar</S><S sid = NA ssid = NA>In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.</S> | Discourse Facet:  NA | Annotator: Automatic


