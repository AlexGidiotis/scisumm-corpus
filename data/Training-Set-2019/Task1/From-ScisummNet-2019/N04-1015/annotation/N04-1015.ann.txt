Citance Number: 1 | Reference Article:  N04-1015.txt | Citing Article:  W05-1621.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization</S><S sid = NA ssid = NA>In our experiments, content models outperform Lapata?s (2003) state-of-the-art ordering method by a wide margin ? forone domain and performance metric, the gap was 78 percentage points.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1015.txt | Citing Article:  W05-1621.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.</S><S sid = NA ssid = NA>But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1015.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S><S sid = NA ssid = NA>Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1015.txt | Citing Article:  D10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Document-level analysis of text struc ture is an important instance of such work.</S><S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1015.txt | Citing Article:  D08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.</S><S sid = NA ssid = NA>rization: the compression of a document by choosinga subsequence of its sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1015.txt | Citing Article:  D08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Document-level analysis of text struc ture is an important instance of such work.</S><S sid = NA ssid = NA>But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1015.txt | Citing Article:  D08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.</S><S sid = NA ssid = NA>The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1015.txt | Citing Article:  P11-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.</S><S sid = NA ssid = NA>The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1015.txt | Citing Article:  P06-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.</S><S sid = NA ssid = NA>The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1015.txt | Citing Article:  N12-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These methods identify regularities in words (Barzilay and Lee, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.</S><S sid = NA ssid = NA>The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1015.txt | Citing Article:  D12-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>corresponds roughly to the notions of topic and topic change.</S><S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1015.txt | Citing Article:  N09-3014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first describe an efficient, knowledge-lean methodfor learning both a set of topics and the relations be tween topics directly from un-annotated documents.</S><S sid = NA ssid = NA>We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1015.txt | Citing Article:  N09-3014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.</S><S sid = NA ssid = NA>corresponds roughly to the notions of topic and topic change.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1015.txt | Citing Article:  N09-3014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Then, we apply techniques based on content models totwo complex text-processing tasks.</S><S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1015.txt | Citing Article:  N09-3014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S><S sid = NA ssid = NA>corresponds roughly to the notions of topic and topic change.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1015.txt | Citing Article:  S12-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).</S><S sid = NA ssid = NA>How ever, research has shown that texts from the same domain tend to exhibit high similarity (Wray, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1015.txt | Citing Article:  N09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In arbi trary document collections, such patterns might be toovariable to be easily detected by statistical means.</S><S sid = NA ssid = NA>First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1015.txt | Citing Article:  W06-3309.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S><S sid = NA ssid = NA>The development and application of computational models of text structure is a central concern in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1015.txt | Citing Article:  W06-3309.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.</S><S sid = NA ssid = NA>Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1015.txt | Citing Article:  W06-3309.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For this task, we de velop a new content-model-based learning algorithm for sentence selection.</S><S sid = NA ssid = NA>The development and application of computational models of text structure is a central concern in natural language processing.</S> | Discourse Facet:  NA | Annotator: Automatic


