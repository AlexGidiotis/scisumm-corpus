Citance Number: 1 | Reference Article:  P07-1004.txt | Citing Article:  P08-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair.</S><S sid = NA ssid = NA>Experiments showed that putting a large weight on the model trained on labeled data performs best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1004.txt | Citing Article:  P13-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Under source sentence.</S><S sid = NA ssid = NA>In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1004.txt | Citing Article:  N09-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Semi-supervised learning has been previously applied to improve word alignments.</S><S sid = NA ssid = NA>Self-training for SMT was proposed in (Ueffing, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1004.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A comparison with Table 4 shows that transductive learning on the development set and test corpora, adapting the system to their domain and style, is more effective in improving the SMT system than the use of additional source language data.</S><S sid = NA ssid = NA>The proposed method adapts the trained models to the style and domain of the new input.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1004.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such self-translation techniques have been introduced by Ueffing et al (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Self-training for SMT was proposed in (Ueffing, 2006).</S><S sid = NA ssid = NA>For details, see (Ueffing and Ney, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1004.txt | Citing Article:  W11-2132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In follow up work, this approach was refined (Ueffing et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For details, see (Ueffing and Ney, 2007).</S><S sid = NA ssid = NA>We provide a basic description here; for a lected sentence pairs are replaced in each iteration, detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is The models (or features) which are employed by kept fixed throughout the algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1004.txt | Citing Article:  N09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006).</S><S sid = NA ssid = NA>Since re-training the full phrase tables is not feasible here, a (small) additional phrase table, specific to U, was trained and plugged into the SMT system as an additional model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1004.txt | Citing Article:  N09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In one set of experiments, we used the 100K and 150K training sentences filtered according to n-gram coverage over the test set.</S><S sid = NA ssid = NA>In (CallisonBurch et al., 2004), a generative model for word alignment is trained using unsupervised learning on parallel text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1004.txt | Citing Article:  N09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: which we implemented follows the approaches suggested in (Blatz et al., 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score.</S><S sid = NA ssid = NA>These K sampled translations and their associated source sentences make up the additional training data Ti.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1004.txt | Citing Article:  N09-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The SMT system we applied in our experiments is PORTAGE.</S><S sid = NA ssid = NA>Self-training for SMT was proposed in (Ueffing, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1004.txt | Citing Article:  C10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Transductive learning for statistical machine translation</S><S sid = NA ssid = NA>Self-training for SMT was proposed in (Ueffing, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


