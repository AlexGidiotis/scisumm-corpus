Citance Number: 1 | Reference Article:  J92-1004.txt | Citing Article:  P96-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.</S><S sid = NA ssid = NA>TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J92-1004.txt | Citing Article:  W08-0801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TINA: A Natural Language System For Spoken Language Applications</S><S sid = NA ssid = NA>A spoken language system relies on its natural language component to provide the meaning representation of a given sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J92-1004.txt | Citing Article:  P98-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach resembles the work by Grishman et al. (1986) and Hirschman et al.</S><S sid = NA ssid = NA>If the natural language component's computational and memory requirements are not excessive, and if it is organized in such a way that it can easily predict a set of next-word candidates, then it can be incorporated into the active search process of the recognizer, dynamically predicting possible words to follow a hypothesized word sequence, and pruning away hypotheses that cannot be completed in any way.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J92-1004.txt | Citing Article:  W08-0102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Their speech was recorded in a simulation mode in which the speech recognition component was excluded.</S><S sid = NA ssid = NA>The results were essentially the same for the training and the test sentences, as shown in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J92-1004.txt | Citing Article:  A00-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is a problem to be aware of in building grammars from example sentences.</S><S sid = NA ssid = NA>In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding.1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an &quot;understanding&quot; of the intended message.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J92-1004.txt | Citing Article:  I08-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We were able to collect a total, of nearly 5000 utterances in this fashion.</S><S sid = NA ssid = NA>Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J92-1004.txt | Citing Article:  W04-3004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have not yet incorporated probabilities from TINA into the search, but they are used effectively to resort the final output sentence candidates.</S><S sid = NA ssid = NA>TINA: A Natural Language System For Spoken Language Applications</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J92-1004.txt | Citing Article:  N07-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TINA is based on a context-free grammar augmented with a set of features used to enforce syntactic and semantic constraints.</S><S sid = NA ssid = NA>The process of conversion to a new grammar involves parsing the new sentences one by one, and adding context-free rules whenever a parse fails.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J92-1004.txt | Citing Article:  N07-4007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The speech material was then used to train the recognizer component, and the text material was used to train the natural language and back-end components.</S><S sid = NA ssid = NA>By encoding meaning in the structural entities of the parse tree, it becomes feasible to realize probabilistic semantic restrictions in an efficient manner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J92-1004.txt | Citing Article:  C96-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept.</S><S sid = NA ssid = NA>new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J92-1004.txt | Citing Article:  W04-0507.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach resembles the work by Grishman et al. (1986) and Hirschman et al.</S><S sid = NA ssid = NA>Furthermore, the same [do-question] grammar node deals with the yes/no question &quot;Did Mike buy the pies?,&quot; except in this case there is no CURRENTFOCUS and hence no gap.</S> | Discourse Facet:  NA | Annotator: Automatic


