Citance Number: 1 | Reference Article:  P02-1001.txt | Citing Article:  D09-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The calculation of expected counts can be formulated using the expectation semiring frame work of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 has infinitely many) but expected counts derived from the paths.</S><S sid = NA ssid = NA>For example, if every arc had value 1, then expected value would be expected path length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1001.txt | Citing Article:  D09-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 has infinitely many) but expected counts derived from the paths.</S><S sid = NA ssid = NA>For example, if every arc had value 1, then expected value would be expected path length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1001.txt | Citing Article:  W10-1718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S><S sid = NA ssid = NA>It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them “from outside.” ∗A brief version of this work, with some additional material, first appeared as (Eisner, 2001a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1001.txt | Citing Article:  H05-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or epsilon-cycles in FSMs (Eisner, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.</S><S sid = NA ssid = NA>Knight and Graehl, 20If xi and yi are acyclic (e.g., fully observed strings), and f (or rather its FST) has no a : a cycles, then composition will “unroll” f into an acyclic machine.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1001.txt | Citing Article:  D08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use standard algorithms (Eisner, 2002) to compute the path sums as well as their gradients with respect to theta for optimization (section 4.1).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The same semiring may be used to compute gradients.</S><S sid = NA ssid = NA>Alternatively, discard EM and use gradient-based optimization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1001.txt | Citing Article:  D08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used the OpenFST library (Allauzen et al, 2007) to implement all finite-state computations, using the expectation semiring (Eisner, 2002) for training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1998), although such data could also be used; (4) training of branching noisy channels (footnote 7); (5) discriminative training with incomplete data; (6) training of conditional MEMMs (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) on unbounded sequences.</S><S sid = NA ssid = NA>Such approaches have been tried recently in restricted cases (McCallum et al., 2000; Eisner, 2001b; Lafferty et al., 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hyper graphs).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, if every arc had value 1, then expected value would be expected path length.</S><S sid = NA ssid = NA>For other parameterizations, the path must instead yield a vector of arc traversal counts or feature counts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we apply the expectation semiring (Eisner, 2002) to a hyper graph (or packed forest) rather than just a lattice.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Of course, the method of this paper can train such compositions.)</S><S sid = NA ssid = NA>The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The M step uses not individual path probabilities (Fig.</S><S sid = NA ssid = NA>Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.</S><S sid = NA ssid = NA>Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, Eisner (2002, section 5) observes that this is inefficient when n is large.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run.</S><S sid = NA ssid = NA>Per-state joint normalization (Eisner, 2001b, §8.2) is similar but drops the dependence on a.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This follows Eisner (2002), who similarly generalized the forward-backward algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, the forward-backward algorithm (Baum, 1972) trains only Hidden Markov Models, while (Ristad and Yianilos, 1996) trains only stochastic edit distance.</S><S sid = NA ssid = NA>Here, the forward and backward probabilities can be computed in time only O(m + n log n) (Fredman and Tar an, 1987). k-best variants are also possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Multiplication and addition are replaced by binary operations ® and ® on K. Thus ® is used to combine arc weights into a path weight and ® is used to combine the weights of alternative paths.</S><S sid = NA ssid = NA>We have exhibited a training algorithm for parameterized finite-state machines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1001.txt | Citing Article:  D09-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S><S sid = NA ssid = NA>• In many cases of interest, Ti is an acyclic graph.20 Then Tar an’s method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of ⊕ and ⊗ operations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1001.txt | Citing Article:  P04-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This logic can be used with the expectation semiring (Eisner, 2002) to find the maximum likelihood estimates of the parameters of a word-to-word translation model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The same semiring may be used to compute gradients.</S><S sid = NA ssid = NA>Maximum-likelihood estimation guesses 0ˆ to be the 0 maximizing Hi fθ(xi, yi).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1001.txt | Citing Article:  P04-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Eisner (2002) has claimed that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.</S><S sid = NA ssid = NA>A more subtle example is weighted FSAs that approximate PCFGs (Nederhof, 2000; Mohri and Nederhof, 2001), or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1001.txt | Citing Article:  P05-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.</S><S sid = NA ssid = NA>We have exhibited a training algorithm for parameterized finite-state machines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1001.txt | Citing Article:  P05-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.</S><S sid = NA ssid = NA>Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1001.txt | Citing Article:  P09-2069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parameter Estimation For Probabilistic Finite-State Transducers</S><S sid = NA ssid = NA>Such techniques build on our parameter estimation method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1001.txt | Citing Article:  E09-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Eisner (2002) describes the expectation semiring for parameter learning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.</S><S sid = NA ssid = NA>Parameter Estimation For Probabilistic Finite-State Transducers</S> | Discourse Facet:  NA | Annotator: Automatic


