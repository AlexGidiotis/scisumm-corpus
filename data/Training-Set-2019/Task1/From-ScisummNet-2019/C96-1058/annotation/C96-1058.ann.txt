Citance Number: 1 | Reference Article:  C96-1058.txt | Citing Article:  P14-2107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Structures and rules for parsing with the (Eisner, 1996) algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C96-1058.txt | Citing Article:  P14-2107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In that work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>phty~ j J - y , .% (b) The ill __ ~ / .~dachshund It) gol f . ) f COfllel his file Figure 1: (a) A bare-l>ones dependen(-y parse.</S><S sid = NA ssid = NA>1 The main contribution of' the work is to I)ro- pose three distin('t, lexiealist hyl)otheses abou(.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C96-1058.txt | Citing Article:  P01-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>context-free rules Charniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Other researchers, not wishing to abandon context-flee grammar (CI"G) but disillusioned with its lexica\] blind spot, have tried to re-parameterize stochas- tic CI"G in context-sensitive ways (Black et al, 1992) or have augmented the formalism with lex- ical headwords (Magerman, 1995; Collins, 11996).</S><S sid = NA ssid = NA>phty~ j J - y , .% (b) The ill __ ~ / .~dachshund It) gol f . ) f COfllel his file Figure 1: (a) A bare-l>ones dependen(-y parse.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C96-1058.txt | Citing Article:  P09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Viterbi decoding is done using Eisner's algorithm (Eisner, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C96-1058.txt | Citing Article:  D07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C96-1058.txt | Citing Article:  D07-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>In this paper, we 1)resent a \[lexible l)robat)ilistic parser that simultaneously assigns both part-of- sl)eech tags and a bare-bones dependency struc- ture (illustrate.d in l!'igure 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C96-1058.txt | Citing Article:  D09-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In a slightly more general formulation, it was first published by Eisner (1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C96-1058.txt | Citing Article:  P09-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S><S sid = NA ssid = NA>We il\]ustrate how each hypothesis is (:xl)ressed in a depemteney framework, and how each can be used to guide our parser toward its favored so- lution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C96-1058.txt | Citing Article:  W08-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The choice o t 'a simple syntactic structure is deliberate: we would like to ask some basic questions about where h'x- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S><S sid = NA ssid = NA>In recent years, the statistical parsing community has begun to reach out; for syntactic formalisms that recognize the individuality of words, l,ink grammars (Sleator and 'Pemperley, 1991) and lex- icalized tree-adjoining ranunars (Schabes, 1992) have now received stochastic treatments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C96-1058.txt | Citing Article:  N10-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>\]'\]a<:h word points to a single t)arent, the word it modities; the head of the sentence points to the EOS (end-of: sentence) ma.rk.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C96-1058.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Eisner (1996) algorithm with non-projective rewriting and second order features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C96-1058.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>In this paper, we 1)resent a \[lexible l)robat)ilistic parser that simultaneously assigns both part-of- sl)eech tags and a bare-bones dependency struc- ture (illustrate.d in l!'igure 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C96-1058.txt | Citing Article:  D07-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The choice o t 'a simple syntactic structure is deliberate: we would like to ask some basic questions about where h'x- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S><S sid = NA ssid = NA>(b) Constituent structure and sub(:ategoriza- tion may be highlighted by displaying the same de- pendencies as a lexical tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>Other researchers, not wishing to abandon context-flee grammar (CI"G) but disillusioned with its lexica\] blind spot, have tried to re-parameterize stochas- tic CI"G in context-sensitive ways (Black et al, 1992) or have augmented the formalism with lex- ical headwords (Magerman, 1995; Collins, 11996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>The choice o t 'a simple syntactic structure is deliberate: we would like to ask some basic questions about where h'x- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C96-1058.txt | Citing Article:  H05-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>The choice o t 'a simple syntactic structure is deliberate: we would like to ask some basic questions about where h'x- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C96-1058.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The (Eisner, 1996) algorithm is typically used for projective parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We il\]ustrate how each hypothesis is (:xl)ressed in a depemteney framework, and how each can be used to guide our parser toward its favored so- lution.</S><S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C96-1058.txt | Citing Article:  P11-2125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>In this paper, we 1)resent a \[lexible l)robat)ilistic parser that simultaneously assigns both part-of- sl)eech tags and a bare-bones dependency struc- ture (illustrate.d in l!'igure 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C96-1058.txt | Citing Article:  W09-1212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Three New Probabilistic Models For Dependency Parsing: An Exploration</S><S sid = NA ssid = NA>We il\]ustrate how each hypothesis is (:xl)ressed in a depemteney framework, and how each can be used to guide our parser toward its favored so- lution.</S> | Discourse Facet:  NA | Annotator: Automatic


