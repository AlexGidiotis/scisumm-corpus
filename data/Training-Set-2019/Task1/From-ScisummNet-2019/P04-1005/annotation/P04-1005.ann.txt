Citance Number: 1 | Reference Article:  P04-1005.txt | Citing Article:  W05-1519.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.</S><S sid = NA ssid = NA>Testing was performed using data from the parsed version since this data is cleaner, and it enables a direct comparison with earlier work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1005.txt | Citing Article:  W05-1519.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the two approaches seem to have different strengths, a combined model may outperform both of them.</S><S sid = NA ssid = NA>At the end of each repair, a (possibly null) interregnum is appended to the reparandum.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1005.txt | Citing Article:  W05-1519.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.</S><S sid = NA ssid = NA>As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[01]*.mrg files.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1005.txt | Citing Article:  W05-1519.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.</S><S sid = NA ssid = NA>Finally we show the results using the parser language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1005.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of Charniak and Johnson (2001).</S><S sid = NA ssid = NA>There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1005.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A TAG-Based Noisy-Channel Model Of Speech Repairs</S><S sid = NA ssid = NA>We also provide results for our noisy channel model using a bigram language model and a second trigram model where the twenty most likely analyses are rescored.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1005.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Further details of the noisy channel model can be found in Johnson and Charniak (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A TAG-Based Noisy-Channel Model Of Speech Repairs</S><S sid = NA ssid = NA>The noisy channel model using a bigram language model does a slightly worse job at identifying reparandum and interregnum words than the classifier proposed in Charniak and Johnson (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1005.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To improve performance over the standard noisy channel model we use a re-ranker, as previously suggest by Johnson and Charniak (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We measure model performance using standard precision p, recall r and f-score f, measures.</S><S sid = NA ssid = NA>A TAG-Based Noisy-Channel Model Of Speech Repairs</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1005.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As Johnson and Charniak (2004) noted, although this model performs well, a log linear re-ranker can be used to increase performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>So to increase processing speed we only compute analyses for strings of length 12 or less.</S><S sid = NA ssid = NA>We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1005.txt | Citing Article:  P06-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the two approaches seem to have different strengths, a combined model may outperform both of them.</S><S sid = NA ssid = NA>At the end of each repair, a (possibly null) interregnum is appended to the reparandum.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1005.txt | Citing Article:  P06-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work, we use a total of 62 variables, which include 16 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004), an additional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These estimated probability distributions are the linear interpolation of the corresponding empirical distributions from the main sub-corpus using various subsets of conditioning variables (e.g., bigram models are mixed with unigram models, etc.) using Chenâ€™s bucketing scheme Chen and Goodman (1998).</S><S sid = NA ssid = NA>For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1005.txt | Citing Article:  P06-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.</S><S sid = NA ssid = NA>Finally we show the results using the parser language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1005.txt | Citing Article:  W06-1637.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.</S><S sid = NA ssid = NA>A TAG-Based Noisy-Channel Model Of Speech Repairs</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1005.txt | Citing Article:  P08-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[01]*.mrg files.</S><S sid = NA ssid = NA>The corpus also includes punctuation and partial words, which are ignored in both training and evaluation here since we felt that in realistic applications these would not be available in speech recognizer output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-1005.txt | Citing Article:  P08-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In training we ignored all overlapping repairs (i.e., cases where the reparandum of one repair is the repair of another).</S><S sid = NA ssid = NA>Since the weighted grammar just given does not generate the source string X, the score of the parse using the weighted TAG is P(Y |X).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-1005.txt | Citing Article:  D09-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From our training data we estimate a number of conditional probability distributions.</S><S sid = NA ssid = NA>The other distributions are defined over aligned reparandum/repair strings, and are estimated from the aligned repairs extracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P04-1005.txt | Citing Article:  N06-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system.</S><S sid = NA ssid = NA>First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P04-1005.txt | Citing Article:  E09-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A TAG-Based Noisy-Channel Model Of Speech Repairs</S><S sid = NA ssid = NA>It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of Charniak and Johnson (2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P04-1005.txt | Citing Article:  E09-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The output of the JC04 model (Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our experiments below we extract the 20 most likely parses for each sentence.</S><S sid = NA ssid = NA>We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P04-1005.txt | Citing Article:  E09-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al, 1992) data as described in (Johnson and Charniak, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We now describe how the weights on the TAG productions described in subsection 2.2 are estimated from this training data.</S><S sid = NA ssid = NA>The model is trained and tested on the Switchboard disfluency-annotated corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


