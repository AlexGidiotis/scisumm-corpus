Citance Number: 1 | Reference Article:  J08-4004.txt | Citing Article:  W09-1319.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We believe that part of the reluctance to report chance-corrected measures is the difficulty in interpreting them.</S><S sid = NA ssid = NA>Therefore, again, unweighted measures, and in particular K, tend to be used for measuring inter-coder agreement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J08-4004.txt | Citing Article:  P13-1167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One important reason why most agreement results on segmentation are on the lower end of the reliability scale is the fact, known to researchers in discourse analysis from as early as Levin and Moore (1978), that although analysts generally agree on the “bulk” of segments, they tend to disagree on their exact boundaries.</S><S sid = NA ssid = NA>The fact that coders mostly agree on the “bulk” of discourse segments, but tend to disagree on their boundaries, also makes it likely that an all-or-nothing coefficient like K calculated on individual boundaries would underestimate the degree of agreement, suggesting low agreement even among coders whose segmentations are mostly similar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J08-4004.txt | Citing Article:  P13-1167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Observed agreement for all of the unweighted coefficients (S, κ, and π) is calculated by counting the items on which the coders agree (the Artstein and Poesio Inter-Coder Agreement for CL figures on the diagonal of the confusion matrix in Table 4) and dividing by the total number of items.</S><S sid = NA ssid = NA>Survey Article: Inter-Coder Agreement for Computational Linguistics</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J08-4004.txt | Citing Article:  W11-0411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We refer the reader to Krippendorff (1995) for details.</S><S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J08-4004.txt | Citing Article:  W10-1807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We refer the reader to Krippendorff (1995) for details.</S><S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J08-4004.txt | Citing Article:  P13-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recent content analysis practice seems to have settled for even more stringent requirements: A recent textbook, Neuendorf (2002, page 3), analyzing several proposals concerning “acceptable” reliability, concludes that “reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.” This is clearly a fundamental issue.</S><S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J08-4004.txt | Citing Article:  P13-1134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On this task, a (pairwise) percentage agreement of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding to K values of 0.36, 0.37, and 0.67, respectively.</S><S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J08-4004.txt | Citing Article:  W10-1823.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The one proposal for measuring agreement on markable identification we are aware of is the αU coefficient, a non-trivial variant of α proposed by Krippendorff (1995).</S><S sid = NA ssid = NA>A number of coding schemes for dialogue acts have achieved values of K over 0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the 13-tag MapTask coding scheme (Carletta et al. 1997), K = 0.8 for the 42-tag SwitchboardDAMSL scheme (Stolcke et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J08-4004.txt | Citing Article:  W10-1823.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, it is important to keep in mind that achieving good agreement cannot ensure validity: Two observers of the same event may well share the same prejudice while still being objectively wrong.</S><S sid = NA ssid = NA>Having said this, we should point out that, first, in practice the difference between π and κ doesn’t often amount to much (see discussion in Section 4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J08-4004.txt | Citing Article:  W10-1827.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A weighted variant of Cohen’s κ is presented in Cohen (1968).</S><S sid = NA ssid = NA>2.6.2 Cohen’s κw.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J08-4004.txt | Citing Article:  W11-0707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004).</S><S sid = NA ssid = NA>In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J08-4004.txt | Citing Article:  E12-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4).</S><S sid = NA ssid = NA>The problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J08-4004.txt | Citing Article:  C10-2089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Survey Article: Inter-Coder Agreement for Computational Linguistics</S><S sid = NA ssid = NA>Since the mid 1990s, increasing effort has gone into putting semantics and discourse research on the same empirical footing as other areas of computational linguistics (CL).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J08-4004.txt | Citing Article:  P12-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This reproducibility is measured by π.</S><S sid = NA ssid = NA>A second annotation study we carried out (Artstein and Poesio 2006) shows even more clearly the possible side effects of using weighted coefficients.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J08-4004.txt | Citing Article:  P12-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, the interpretation of k is still under discussion (Artstein and Poesio, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>comments and discussion.</S><S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J08-4004.txt | Citing Article:  P10-2059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Observed agreement for all of the unweighted coefficients (S, κ, and π) is calculated by counting the items on which the coders agree (the Artstein and Poesio Inter-Coder Agreement for CL figures on the diagonal of the confusion matrix in Table 4) and dividing by the total number of items.</S><S sid = NA ssid = NA>This is the approach taken by Fleiss (1971).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J08-4004.txt | Citing Article:  N12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The expected agreement is the sum of this joint probability over all the categories k E K. Multi-π is the coefficient that Siegel and Castellan (1988) call K. 2.5.2 Multi-κ.</S><S sid = NA ssid = NA>Fleiss (1971) therefore uses a different type of table which lists each item with the number of judgments it received for each category; Siegel and Castellan (1988) use a similar table, which Di Eugenio and Glass (2004) call an agreement table.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J08-4004.txt | Citing Article:  N12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.</S><S sid = NA ssid = NA>Looking at these two problems in detail is useful for understanding the differences between the coefficients.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J08-4004.txt | Citing Article:  N12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).</S><S sid = NA ssid = NA>All of these proposals are based on a misconception: that Artstein and Poesio Inter-Coder Agreement for CL single-distribution coefficients require similar distributions by the individual annotators in order to work properly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J08-4004.txt | Citing Article:  N12-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The difference between the coefficients is only in the interpretation of “chance agreement”: π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients calculate the chance of agreement among the coders who produced the reliability data.</S><S sid = NA ssid = NA>If observed agreement is measured on the basis of pairwise agreement (the proportion of agreeing judgment pairs), it makes sense to measure expected agreement in terms of pairwise comparisons as well, that is, as the probability that any pair of judgments for an item would be in agreement—or, said otherwise, the probability that two arbitrary coders would make the same judgment for a particular item by chance.</S> | Discourse Facet:  NA | Annotator: Automatic


