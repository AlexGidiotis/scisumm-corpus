Citance Number: 1 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).</S><S sid = NA ssid = NA>Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S><S sid = NA ssid = NA>More over, the fact that our parser uses labeled dependenciesis probably also significant, since the possibility of using information from previously assigned (labeled) de pendencies during parsing seems to have a positive effect on accuracy (Nivre, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).</S><S sid = NA ssid = NA>2.1 Dependency Graphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, in order to maintain the efficiency of the parser, the classifier must also be implemented in such a way that each transition can still be performed in constant time.Previous work in this area includes the use of memory based learning to guide a standard shift-reduce parser(Veenstra and Daelemans, 2000) and the use of support vector machines to guide a deterministic depen dency parser (Yamada and Matsumoto, 2003).</S><S sid = NA ssid = NA>2.4 Memory-Based Learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Instead, we have to simulate the parser on the tree bank in order to derive, for each sentence, the transition sequence corresponding to the correct dependency tree.</S><S sid = NA ssid = NA>Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These settings are the result of extensive experiments partially reported in Nivre et al (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(The final test set has not been used at all in the experiments reported in this paper.)</S><S sid = NA ssid = NA>The results in the first column were obtained with the default settings of the TiMBL package, in particular: ? The IB1 classification algorithm (Aha et al, 1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The attachment score is computed as theproportion of tokens (excluding punctuation) that are as signed the correct head (or no head if the token is a root).</S><S sid = NA ssid = NA>In addition to the word form itself (TOP), we consider its part-of-speech (as assigned by an automatic part-of-speech tagger in a preprocessing phase), the dependency type by which it is related to its head (which may or may not be available in a given configuration depending on whether the head is to the left or to the right of the token in question), and the dependency types by which it is related to its leftmost and rightmost dependent, respectively (where the currentrightmost dependent may or may not be the rightmost de pendent in the complete dependency tree).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-2407.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al, 1999), we will give this measure as well.In order to measure label accuracy, we also define a la beled attachment score, where both the head and the label must be correct, but which is otherwise computed in the same way as the ordinary (unlabeled) attachment score.</S><S sid = NA ssid = NA>Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-2407.txt | Citing Article:  P14-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Moreover, the memory-based approach can easily handle multi-class classification, unlike the support vector machines used by Yamada and Matsumoto (2003).</S><S sid = NA ssid = NA>Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>More over, the fact that our parser uses labeled dependenciesis probably also significant, since the possibility of using information from previously assigned (labeled) de pendencies during parsing seems to have a positive effect on accuracy (Nivre, 2004).</S><S sid = NA ssid = NA>This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are grateful to three anonymous reviewers for constructive com ments on the preliminary version of the paper.</S><S sid = NA ssid = NA>The transitions Left-Arc and Right-Arc are subject to conditions that ensure that the graph conditions Uniquelabel and Single head are satisfied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Distance weighted class voting with inverse distance weighting (Dudani, 1976).</S><S sid = NA ssid = NA>For the experiments reported in this paper, we have used the software package TiMBL (Tilburg MemoryBased Learner), which provides a variety of metrics, al gorithms, and extra functions on top of the classical knearest neighbor classification kernel, such as value distance metrics and distance weighted class voting (Daele mans et al, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-2407.txt | Citing Article:  P06-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).</S><S sid = NA ssid = NA>The unlabeled attachment score is naturally higher, and it is worth noting that the relative differ ence between the MBL lexical model and the other twomodels is much smaller.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-2407.txt | Citing Article:  P05-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.1 Dependency Graphs.</S><S sid = NA ssid = NA>Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-2407.txt | Citing Article:  P05-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.4 Memory-Based Learning.</S><S sid = NA ssid = NA>This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-2407.txt | Citing Article:  P11-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are grateful to three anonymous reviewers for constructive com ments on the preliminary version of the paper.</S><S sid = NA ssid = NA>The transitions Left-Arc and Right-Arc are subject to conditions that ensure that the graph conditions Uniquelabel and Single head are satisfied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-2407.txt | Citing Article:  P11-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The memory-based classifiers used in the experi ments were constructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).</S><S sid = NA ssid = NA>2.4 Memory-Based Learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Memory-Based Dependency Parsing</S><S sid = NA ssid = NA>This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This model, which we will refer to as the MCLE model, is described in more detail in Nivre (2004).</S><S sid = NA ssid = NA>For this purpose we define a number of features that can be used to define different models of parser state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-2407.txt | Citing Article:  W07-2218.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(1991).</S><S sid = NA ssid = NA>Memory-based learning and problem solving is based ontwo fundamental principles: learning is the simple stor age of experiences in memory, and solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


