Citance Number: 1 | Reference Article:  P03-1069.txt | Citing Article:  N04-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The average distance in the orderings produced by our subjects is .58.</S><S sid = NA ssid = NA>This way they ensured that orderings were not influenced by mistakes their system could have made.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-1069.txt | Citing Article:  P14-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.</S><S sid = NA ssid = NA>The model in Section 2.1 was trained on the BLLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-1069.txt | Citing Article:  N09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.</S><S sid = NA ssid = NA>The model in Section 2.1 was trained on the BLLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-1069.txt | Citing Article:  N06-2017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we assess whether this model can be used for multi-document summarization using data from Barzilay et al. (2002).</S><S sid = NA ssid = NA>Probabilistic Text Structuring: Experiments With Sentence Ordering</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-1069.txt | Citing Article:  P13-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem of finding an acceptable ordering does not arise solely in concept-to-text generation but also in the emerging field of text-to-text generation (Barzilay, 2003).</S><S sid = NA ssid = NA>This simplification is reasonable if one has text-to-text generation mind.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-1069.txt | Citing Article:  P13-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.</S><S sid = NA ssid = NA>We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-1069.txt | Citing Article:  P13-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that the noun and verb features do not capture the structure of the sentences to be ordered.</S><S sid = NA ssid = NA>Table 3 gives the average i (T) for all 20 test texts when the following features are used: lemmatized verbs (VL), tensed verbs (VT), lemmatized nouns (NL), lemmatized verbs and nouns (VLNL), tensed verbs and lemmatized nouns (VTNL), verb-related dependencies (VD), noun-related dependencies (ND), verb and noun dependencies (VDND), and all available dependencies (AD).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-1069.txt | Citing Article:  P06-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Investigations into the interpretation of narrative discourse (Asher and Lascarides, 2003) have shown that specific lexical information (e.g., verbs, adjectives) plays an important role in determining the discourse relations between propositions.</S><S sid = NA ssid = NA>Marcu (1997) argues that global coherence can be achieved if constraints on local coherence are satisfied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-1069.txt | Citing Article:  P06-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.</S><S sid = NA ssid = NA>The model in Section 2.1 was trained on the BLLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-1069.txt | Citing Article:  P06-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.</S><S sid = NA ssid = NA>The model in Section 2.1 was trained on the BLLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-1069.txt | Citing Article:  P06-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In contrast, the greedy algorithm of Lapata (2003) makes grave search errors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The greedy algorithm implements a search procedure with a beam of width one.</S><S sid = NA ssid = NA>The input to the the greedy algorithm (see Section 2.2) was a text with a randomized sentence ordering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-1069.txt | Citing Article:  P06-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Fortunately, they propose a simple greedy algorithm that provides an approximate solution which can be easily modified for our task (see also Barzilay et al. 2002).</S><S sid = NA ssid = NA>As in the case of Mellish et al. (1998) we construct an acceptable ordering rather than the best possible one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-1069.txt | Citing Article:  N06-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Adjacency of sentences has been previously used to model local coherence (Lapata, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Subjects were recruited via postings to local between the model and each of the subjects for all features used in Experiment 1.</S><S sid = NA ssid = NA>Marcu (1997) argues that global coherence can be achieved if constraints on local coherence are satisfied.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-1069.txt | Citing Article:  D08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we assess whether this model can be used for multi-document summarization using data from Barzilay et al. (2002).</S><S sid = NA ssid = NA>Although in single document summarization the position of a sentence in a document can provide cues with respect to its ordering in the summary, this is not the case in multidocument summarization where sentences are selected from different documents and must be somehow ordered so as to produce a coherent summary (Barzilay et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P03-1069.txt | Citing Article:  D08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Several improvements can take place with respect to the model.</S><S sid = NA ssid = NA>The model in Section 2.1 was trained on the BLLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P03-1069.txt | Citing Article:  P06-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Probabilistic Text Structuring: Experiments With Sentence Ordering</S><S sid = NA ssid = NA>In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P03-1069.txt | Citing Article:  P06-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 3 gives the average i (T) for all 20 test texts when the following features are used: lemmatized verbs (VL), tensed verbs (VT), lemmatized nouns (NL), lemmatized verbs and nouns (VLNL), tensed verbs and lemmatized nouns (VTNL), verb-related dependencies (VD), noun-related dependencies (ND), verb and noun dependencies (VDND), and all available dependencies (AD).</S><S sid = NA ssid = NA>In this experiment we compare our model’s performance against human judges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P03-1069.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm starts by assigning each vertex v  V a probability.</S><S sid = NA ssid = NA>We express the probability of a text made up of sentences S1...Sn as shown in (1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P03-1069.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We express the probability of a text made up of sentences S1...Sn as shown in (1).</S><S sid = NA ssid = NA>In the training phase our model will learn these constraints from adjacent sentences represented by a set of informative features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P03-1069.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Cartesian product over the features in Si and Si−1 is an attempt to capture inter-sentential dependencies.</S><S sid = NA ssid = NA>We will assume that these features are independent and that P(Si|Si−1) can be estimated from the pairs in the Cartesian product defined over the features expressing sentences Si and Si−1: Assuming that the features are independent again makes parameter estimation easier.</S> | Discourse Facet:  NA | Annotator: Automatic


