Citance Number: 1 | Reference Article:  P07-1106.txt | Citing Article:  P08-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Chinese Segmentation with a Word-Based Perceptron Algorithm</S><S sid = NA ssid = NA>In comparison, our system is based on a single perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1106.txt | Citing Article:  P14-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their insightful comments.</S><S sid = NA ssid = NA>If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1106.txt | Citing Article:  D10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.</S><S sid = NA ssid = NA>Chinese Segmentation with a Word-Based Perceptron Algorithm</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their insightful comments.</S><S sid = NA ssid = NA>If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.</S><S sid = NA ssid = NA>At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furtherfor decoding. more, our approach provides an example of the poSeveral discriminatively trained models have re- tential of search-based discriminative training methcently been applied to the CWS problem.</S><S sid = NA ssid = NA>We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S><S sid = NA ssid = NA>Table 4 shows the accuracy with various features from the model removed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1106.txt | Citing Article:  C10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S><S sid = NA ssid = NA>We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1106.txt | Citing Article:  P12-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their insightful comments.</S><S sid = NA ssid = NA>If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S><S sid = NA ssid = NA>The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.</S><S sid = NA ssid = NA>In comparison, our system is based on a single perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff).</S><S sid = NA ssid = NA>80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1106.txt | Citing Article:  P14-2032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their insightful comments.</S><S sid = NA ssid = NA>If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2 gives the decoding algorithm.</S><S sid = NA ssid = NA>These results demonstrate the imalgorithms such as the Viterbi algorithm can be used portance of word-based features for CWS.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is different from the experiments reported in (Zhang and Clark, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Two sets of experiments were conducted.</S><S sid = NA ssid = NA>Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1106.txt | Citing Article:  C10-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank the anonymous reviewers for their insightful comments.</S><S sid = NA ssid = NA>If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1106.txt | Citing Article:  P13-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2 gives the decoding algorithm.</S><S sid = NA ssid = NA>Hence we are Boolean valued vectors containing the indicator use a beam-search decoder during training and test- features for one element in the sequence. ing; our idea is similar to that of Collins and Roark Denote the global feature vector for segmented (2004) who used a beam-search decoder as part of sentence y with 4b(y) E Rd, where d is the total a perceptron parsing model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1106.txt | Citing Article:  P13-2031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.</S><S sid = NA ssid = NA>We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1106.txt | Citing Article:  P08-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We proposed a word-based CWS model using the discriminative perceptron learning algorithm.</S><S sid = NA ssid = NA>Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1106.txt | Citing Article:  P08-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Chinese Segmentation with a Word-Based Perceptron Algorithm</S><S sid = NA ssid = NA>There is no fixed standard for Chinese word segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


