Citance Number: 1 | Reference Article:  E06-1031.txt | Citing Article:  W06-3112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S><S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1031.txt | Citing Article:  P14-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation</S><S sid = NA ssid = NA>The costs of a long jump are constant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1031.txt | Citing Article:  W07-0734.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1031.txt | Citing Article:  W07-0707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks.</S><S sid = NA ssid = NA>WER, which is based on Levenshtein distance, penalizes the reordering of blocks even more heavily.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1031.txt | Citing Article:  W12-4206.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences.</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1031.txt | Citing Article:  D07-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al., 1997).</S><S sid = NA ssid = NA>This has been implemented by (Leusch et al., 2003) in the inversion word error rate.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1031.txt | Citing Article:  W12-3129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S><S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1031.txt | Citing Article:  W12-3129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1031.txt | Citing Article:  W08-0312.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1031.txt | Citing Article:  W11-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences.</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1031.txt | Citing Article:  W07-0714.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S><S sid = NA ssid = NA>Nevertheless, a higher “amount” of reordering between a candidate translation and a reference translation should still be reflected in a worse evaluation score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1031.txt | Citing Article:  W07-0411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation</S><S sid = NA ssid = NA>The costs of a long jump are constant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1031.txt | Citing Article:  P13-2067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005).</S><S sid = NA ssid = NA>Details on this have been described in (Leusch et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


