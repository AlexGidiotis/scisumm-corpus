Citance Number: 1 | Reference Article:  P03-2026.txt | Citing Article:  W07-1604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Izumi et al (2003) and (2004) used error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = NA ssid = NA>(e.g. article and tense errors) These are different from “error types” (omission or replacement).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P03-2026.txt | Citing Article:  C08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% in a Japanese learner corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section, we would like to describe how we proceeded with error detection in the learner corpus.</S><S sid = NA ssid = NA>Automatic Error Detection In The Japanese Learners' English Spoken Data</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P03-2026.txt | Citing Article:  C08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Izumi et al., 2003) and (Izumi et al, 2004) used an ME approach to classify different grammatical errors in transcripts of Japanese interviews.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We applied different methods to detecting these two kinds of errors.</S><S sid = NA ssid = NA>(e.g. article and tense errors) These are different from “error types” (omission or replacement).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P03-2026.txt | Citing Article:  P08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = NA ssid = NA>This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P03-2026.txt | Citing Article:  P08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A maximum entropy model, using lexical and POS features, is trained in (Izumi et al, 2003) to recognize a variety of errors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Maximum Entropy (ME) model (Jaynes 1957) is a general technique that is used to estimate the probability distributions of data.</S><S sid = NA ssid = NA>The over-riding principle in ME is that when nothing is known, the distribution should be as uniform as possible, i.e., maximum entropy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P03-2026.txt | Citing Article:  W10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Izumi et al (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We did this only for article errors.</S><S sid = NA ssid = NA>Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our error tags contained three pieces of information, i.e., the part of speech, the grammatical/lexical system and the corrected form.</S><S sid = NA ssid = NA>We designed an original error tagset for learners’ grammatical and lexical errors, which were relatively easy to categorize.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The usage of articles has been found to be the most frequent error class in the JLE corpus (Izumi et al, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first examined what kind of errors had been made with articles and found that “a”, “an”, “the” and the absence of articles were often confused.</S><S sid = NA ssid = NA>We prepared special tags for some errors that cannot be categorized into any word class, such as the misordering of words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P03-2026.txt | Citing Article:  N07-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the future, we would like to search for more salient features through a careful study of non-native errors, using error-tagged corpora such as (Izumi et al., 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section, we would like to describe how we proceeded with error detection in the learner corpus.</S><S sid = NA ssid = NA>In this paper, we introduce a method of detecting learners’ errors, and we examine to what extent this could be accomplished using our learner corpus data including error tags that are labeled with the learners’ errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P03-2026.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same.</S><S sid = NA ssid = NA>Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P03-2026.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We based our error annotation scheme on that used in the NICT JLE corpus (Izumi et al, 2003a), whose detailed description is readily available, for example, in Izumi et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since some more detailed context might be necessary to decide whether “a” or “the” must be used, the features we used here might be insufficient.</S><S sid = NA ssid = NA>The following example is a sentence with an error tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P03-2026.txt | Citing Article:  P11-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same.</S><S sid = NA ssid = NA>Here, there is an article missing in front of “telephone”, so this can be considered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P03-2026.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The method (Izumi et al, 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We obtained a better recall and precision rate for omission-type errors.</S><S sid = NA ssid = NA>As we did in detecting omission-type errors, if more than one error category was given, we use two methods of detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P03-2026.txt | Citing Article:  N10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Izumi et al (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Automatic Error Detection In The Japanese Learners' English Spoken Data</S><S sid = NA ssid = NA>The Maximum Entropy (ME) model (Jaynes 1957) is a general technique that is used to estimate the probability distributions of data.</S> | Discourse Facet:  NA | Annotator: Automatic


