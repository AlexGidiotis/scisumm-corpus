Citance Number: 1 | Reference Article:  C04-1146.txt | Citing Article:  P04-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have seen that there is a large amount of variation in the neighbours selected by different measures andtherefore the choice of measure in a given appli cation is likely to be important.</S><S sid = NA ssid = NA>Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1146.txt | Citing Article:  P06-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).</S><S sid = NA ssid = NA>We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1146.txt | Citing Article:  W06-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S><S sid = NA ssid = NA>3.2 Results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1146.txt | Citing Article:  W06-2205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, having discussed a connection between frequency and distributional generality, we might also expect to find that the frequency of the hypernymic term is greater than that of the hyponymicterm.</S><S sid = NA ssid = NA>low freq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1146.txt | Citing Article:  W06-3811.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Characterising Measures Of Lexical Distributional Similarity</S><S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1146.txt | Citing Article:  W06-1406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would liketo thank Adam Kilgarriff and Bill Keller for use ful discussions.</S><S sid = NA ssid = NA>c P (c|w1)2 ? c P (c|w2)2 Jens.-Shan.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1146.txt | Citing Article:  W06-1406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous work on the evaluation of dis tributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be orientedtowards a particular task such as language mod elling (Dagan et al, 1999; Lee, 1999).</S><S sid = NA ssid = NA>Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1146.txt | Citing Article:  W09-0215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This supports the idea of a three-waylinking between distributional generality, rela tive frequency and semantic generality.</S><S sid = NA ssid = NA>Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1146.txt | Citing Article:  W09-0215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus, a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set.</S><S sid = NA ssid = NA>The complete set of 2000 nouns (WScomp) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency > 500), and WSlow contains the nouns ranked 3001-4000 (frequency ? 100).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1146.txt | Citing Article:  P10-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).</S><S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1146.txt | Citing Article:  W11-2128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S><S sid = NA ssid = NA>Characterising Measures Of Lexical Distributional Similarity</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1146.txt | Citing Article:  W11-2128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>4.1 Measuring bias.</S><S sid = NA ssid = NA>An increase in distance correlates with a decrease in similarity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1146.txt | Citing Article:  W12-3402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S><S sid = NA ssid = NA>The cosine measure (Salton and McGill, 1983) returns the cosine of the angle between two vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1146.txt | Citing Article:  D07-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.</S><S sid = NA ssid = NA>To do this, we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1146.txt | Citing Article:  D10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used.</S><S sid = NA ssid = NA>The k nearest neighbours of a target word w are the k words for which similarity with w is greatest.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1146.txt | Citing Article:  D10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.</S><S sid = NA ssid = NA>Another is the ?-skew diver gence measure, which uses the p distribution tosmooth the q distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1146.txt | Citing Article:  P05-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first approach is not ideal since it assumes that the goal of distributional similarity methods is topredict semantic similarity and that the semantic resource used is a valid gold standard.</S><S sid = NA ssid = NA>This will make it possible to predict in advanceof any experimental evaluation which distributional similarity measures might be most appro priate for a particular application.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1146.txt | Citing Article:  P14-2086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).</S><S sid = NA ssid = NA>and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better.</S> | Discourse Facet:  NA | Annotator: Automatic


