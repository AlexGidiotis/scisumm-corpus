Citance Number: 1 | Reference Article:  W04-3252.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).</S><S sid = NA ssid = NA>Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-3252.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).</S><S sid = NA ssid = NA>We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-3252.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.</S><S sid = NA ssid = NA>In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-3252.txt | Citing Article:  I05-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.</S><S sid = NA ssid = NA>In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-3252.txt | Citing Article:  P14-2101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.</S><S sid = NA ssid = NA>Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-3252.txt | Citing Article:  P14-1119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.</S><S sid = NA ssid = NA>Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-3252.txt | Citing Article:  C10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.</S><S sid = NA ssid = NA>The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-3252.txt | Citing Article:  W11-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.</S><S sid = NA ssid = NA>However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-3252.txt | Citing Article:  S10-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.</S><S sid = NA ssid = NA>HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-3252.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.</S><S sid = NA ssid = NA>In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-3252.txt | Citing Article:  C08-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.</S><S sid = NA ssid = NA>Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-3252.txt | Citing Article:  C08-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.</S><S sid = NA ssid = NA>In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-3252.txt | Citing Article:  C08-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.</S><S sid = NA ssid = NA>In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-3252.txt | Citing Article:  C08-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).</S><S sid = NA ssid = NA>It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.</S><S sid = NA ssid = NA>The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.</S><S sid = NA ssid = NA>The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).</S><S sid = NA ssid = NA>In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).</S><S sid = NA ssid = NA>We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.</S><S sid = NA ssid = NA>This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-3252.txt | Citing Article:  D09-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).</S><S sid = NA ssid = NA>Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.</S> | Discourse Facet:  NA | Annotator: Automatic


