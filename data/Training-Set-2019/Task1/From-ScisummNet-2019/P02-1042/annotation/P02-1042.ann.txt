Citance Number: 1 | Reference Article:  P02-1042.txt | Citing Article:  P02-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Building Deep Dependency Structures Using A Wide-Coverage CCG Parser</S><S sid = NA ssid = NA>This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1042.txt | Citing Article:  P14-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, since we want to model the dependencies in such structures, the probability model is defined over these structures rather than the derivation.</S><S sid = NA ssid = NA>The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques (Abney, 1997; Johnson et al., 1999), so we have defined a conditional model, similar to the model of Collins (1996) (see also the conditional model in Eisner (1996b)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1042.txt | Citing Article:  P03-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most recent wide-coverage statistical parsers have used models based on lexical dependencies (e.g.</S><S sid = NA ssid = NA>This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).</S><S sid = NA ssid = NA>Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These derived structures can be of any form we like—for example, they could in principle be standard Penn Treebank structures.</S><S sid = NA ssid = NA>A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Building Deep Dependency Structures Using A Wide-Coverage CCG Parser</S><S sid = NA ssid = NA>A CCG parser can directly build derived structures, including longrange dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The supertagger (described in Clark (2002)) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability category for that word, given the surrounding context.</S><S sid = NA ssid = NA>If there was no such category, all categories spanning the whole string were considered.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Clark et al (2002), evaluation is by precision and recall over dependencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To measure the performance of the parser, we compared the dependencies output by the parser with those in the gold standard, and computed precision and recall figures over the dependencies.</S><S sid = NA ssid = NA>Labelled precision and recall on Section 00 for the most frequent dependency types are shown in Table 2 (for the model without distance measures).9 The columns # deps give the total number of dependencies, first the number put forward by the parser, and second the number in the gold standard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.</S><S sid = NA ssid = NA>We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the word, pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1042.txt | Citing Article:  P04-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results are shown in Table 1, with an additional column giving the category accuracy.</S><S sid = NA ssid = NA>One solution is to consider only the normal-form (Eisner, 1996a) derivation, which is the route taken in Hockenmaier and Steedman (2002b).1 Another problem with the non-standard surface derivations is that the standard PARSEVAL performance measures over such derivations are uninformative (Clark and Hockenmaier, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1042.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper has shown that accurate, efficient widecoverage parsing is possible with CCG.</S><S sid = NA ssid = NA>The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1042.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Clark et al (2002), we augment CCG lexical categories with head and dependency information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Derivations are written as follows, with underlines indicating combinatory reduction and arrows indicating the direction of the application: Formally, a dependency is defined as a 4-tuple: hf f s ha , where hf is the head word of the functor,2 f is the functor category (extended with head and dependency information), s is the argument slot, and ha is the head word of the argument—for example, the following is the object dependency yielded by the first step of derivation (3): The head of the infinitival complement’s subject is identified with the head of the object, using the variable X. Unification then “passes” the head of the object to the subject of the infinitival, as in standard unification-based accounts of control.3 The kinds of lexical items that use the head passing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns.</S><S sid = NA ssid = NA>For example, the following category for the transitive verb bought specifies its first argument as a noun phrase (NP) to its right and its second argument as an NP to its left, and its result as a sentence: For parsing purposes, we extend CCG categories to express category features, and head-word and dependency information directly, as follows: The feature dcl specifies the category’s S result as a declarative sentence, bought identifies its head, and the numbers denote dependency relations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1042.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Derivations are written as follows, with underlines indicating combinatory reduction and arrows indicating the direction of the application: Formally, a dependency is defined as a 4-tuple: hf f s ha , where hf is the head word of the functor,2 f is the functor category (extended with head and dependency information), s is the argument slot, and ha is the head word of the argument—for example, the following is the object dependency yielded by the first step of derivation (3): The head of the infinitival complement’s subject is identified with the head of the object, using the variable X. Unification then “passes” the head of the object to the subject of the infinitival, as in standard unification-based accounts of control.3 The kinds of lexical items that use the head passing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns.</S><S sid = NA ssid = NA>The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1042.txt | Citing Article:  W03-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus we ignore the normalisation factor, thereby simplifying the parsing process.</S><S sid = NA ssid = NA>However, the few that exist (Lin, 1995; Carroll et al., 1998; Collins, 1999) have used either different data or different sets of dependencies (or both).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1042.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Building Deep Dependency Structures Using A Wide-Coverage CCG Parser</S><S sid = NA ssid = NA>The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1042.txt | Citing Article:  C04-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The supertagger (described in Clark (2002)) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability category for that word, given the surrounding context.</S><S sid = NA ssid = NA>However, the dependency structures with high enough PCS to be among the highest probability structures are likely to have similar category sequences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1042.txt | Citing Article:  P03-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, since we want to model the dependencies in such structures, the probability model is defined over these structures rather than the derivation.</S><S sid = NA ssid = NA>We conjecture that it is possible to define a generative model that includes the deep dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1042.txt | Citing Article:  P03-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques (Abney, 1997; Johnson et al., 1999), so we have defined a conditional model, similar to the model of Collins (1996) (see also the conditional model in Eisner (1996b)).</S><S sid = NA ssid = NA>(A similar argument is used by Collins (1996) in the context of his parsing model.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1042.txt | Citing Article:  P03-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recall that a dependency is defined as a 4-tuple: a head of a functor, a functor category, an argument slot, and a head of an argument.</S><S sid = NA ssid = NA>If the end-result of parsing is interpretable predicate-argument structure or the related dependency structure, then the question arises: why build derivation structure at all?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1042.txt | Citing Article:  P03-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.</S><S sid = NA ssid = NA>Figures were calculated for labelled dependencies (LP,LR) and unlabelled dependencies (UP,UR).</S> | Discourse Facet:  NA | Annotator: Automatic


