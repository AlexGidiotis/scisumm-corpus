Citance Number: 1 | Reference Article:  P06-1085.txt | Citing Article:  N07-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Hierarchical Dirichlet processes.</S><S sid = NA ssid = NA>Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1085.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1085.txt | Citing Article:  N10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1085.txt | Citing Article:  N09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).</S><S sid = NA ssid = NA>In our experiments, we used the same corpus that NGS and MBDP were tested on.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1085.txt | Citing Article:  N09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1085.txt | Citing Article:  D09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2001.</S><S sid = NA ssid = NA>Contextual Dependencies In Unsupervised Word Segmentation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1085.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2006.</S><S sid = NA ssid = NA>Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1085.txt | Citing Article:  P08-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.</S><S sid = NA ssid = NA>This richget-richer process creates a power-law distribution on word frequencies (Goldwater et al., 2006), the same sort of distribution found empirically in natural language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The only way the model can capture these dependencies is by assuming that these collocations are in fact words themselves.</S><S sid = NA ssid = NA>This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1085.txt | Citing Article:  P08-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Computational Linguistics, 27(3):351–372.</S><S sid = NA ssid = NA>A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1085.txt | Citing Article:  C10-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).</S><S sid = NA ssid = NA>This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1085.txt | Citing Article:  C10-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).</S><S sid = NA ssid = NA>This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1085.txt | Citing Article:  W12-2304.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Hierarchical Dirichlet processes.</S><S sid = NA ssid = NA>Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1085.txt | Citing Article:  W08-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Utterance boundaries are given in the input to the system; other word boundaries are not.</S><S sid = NA ssid = NA>It is also one of the key problems that human language learners must solve as they are learning language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1085.txt | Citing Article:  W08-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Contextual Dependencies In Unsupervised Word Segmentation</S><S sid = NA ssid = NA>Specifically, this paper demonstrates the importance of contextual dependencies for word segmentation by comparing two probabilistic models that differ only in that the first assumes that the probability of a word is independent of its local context, while the second incorporates bigram dependencies between adjacent words.</S> | Discourse Facet:  NA | Annotator: Automatic


