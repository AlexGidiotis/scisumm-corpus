Citance Number: 1 | Reference Article:  P05-1010.txt | Citing Article:  W05-1512.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.</S><S sid = NA ssid = NA>It is in contrast with our approach where (approximated) posterior probability is optimized.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1010.txt | Citing Article:  P14-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1010.txt | Citing Article:  N10-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These scores are the same as the variational rule scores of Matsuzaki et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The parsing performances were measured using F scores of the parse trees that were obtained by re-ranking of 1000-best parses by a PCFG.</S><S sid = NA ssid = NA>We use to denote the rule probability of rule and to denote the probability with which is generated as a root node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>PCFG-LA is a generative probabilistic model of parse trees.</S><S sid = NA ssid = NA>This paper defines a generative model of parse trees that we call PCFG with latent annotations (PCFG-LA).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the definition below, denotes the non-terminal label of the-th node.</S><S sid = NA ssid = NA>In contrast, our method induces all parameters automatically, except that manually written head-rules are used in binarization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A model created using CENTER-PARENT with was used throughout this experiment.</S><S sid = NA ssid = NA>The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We obtained an observable grammar for each model by reading off grammar rules from the binarized training trees.</S><S sid = NA ssid = NA>We used sections 2 through 20 of the Penn WSJ corpus as training data and section 21 as heldout data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Because exact inference with a PCFG-LA, i.e., selection of the most probable parse, is NP-hard, we are forced to use some approximation of it.</S><S sid = NA ssid = NA>Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To see the degree of dependency of trained models on initializations, four instances of the same model were trained with different initial values of parameters.3 The model used in this experiment was created by CENTER-PARENT binarization and was set to 16.</S><S sid = NA ssid = NA>For each binarization method, PCFG-LA models with different numbers of latent annotation symbols, , and , were trained.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1010.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The relationships between the number of parameters in the models and their parsing performances are shown in Figure 7.</S><S sid = NA ssid = NA>The relationships between the average parse time and parsing performance using the three parsing methods described in Section 3 are shown in Figure 8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1010.txt | Citing Article:  D08-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The data points were made by varying configurable parameters of each method, which control the number of candidate parses.</S><S sid = NA ssid = NA>This model is an extension of PCFG models in which non-terminal symbols are annotated with latent variables.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.</S><S sid = NA ssid = NA>The first method simply limits the number of candidate parse trees compared in Eq.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1010.txt | Citing Article:  N10-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).</S><S sid = NA ssid = NA>Once we have computed and , the parse tree that maximizes is found using a Viterbi algorithm, as in PCFG parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1010.txt | Citing Article:  P12-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this model, an observed parse tree is considered as an incomplete data, and the correplete data) and observed tree (incomplete data). sponding complete data is a tree with latent annotations.</S><S sid = NA ssid = NA>If node has a right sibling , let be the mother node of.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1010.txt | Citing Article:  W12-1904.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The different lines for the second and the third methods correspond to different values of .</S><S sid = NA ssid = NA>Note that models created using different binarization methods have different numbers of parameters for the same .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1010.txt | Citing Article:  D08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.</S><S sid = NA ssid = NA>However, both the memory size and the training time are more than linear in , and the training time for the largest ( ) models was about 15 hours for the models created using CENTER-PARENT, CENTER-HEAD, and LEFT and about 20 hours for the model created using RIGHT.</S> | Discourse Facet:  NA | Annotator: Automatic


