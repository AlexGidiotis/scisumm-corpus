Citance Number: 1 | Reference Article:  N01-1006.txt | Citing Article:  W02-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also present in Table 1 are the results of training Brill's tagger on the same data.</S><S sid = NA ssid = NA>Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N01-1006.txt | Citing Article:  W03-0433.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The experiments presented in Section 4 include ICA in the training time and performance comparisonsï¿½.</S><S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N01-1006.txt | Citing Article:  W03-0433.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The following table shows the above sentence with the assigned chunk tags: and the part-of-speech tags were generated by Brill's tagger (Brill, 1995).</S><S sid = NA ssid = NA>This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N01-1006.txt | Citing Article:  W04-3209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Ramshaw & Marcus' (1999) work in base noun phrase chunking, each word is assigned a chunk tag corresponding to the phrase to which it belongs .</S><S sid = NA ssid = NA>The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N01-1006.txt | Citing Article:  P03-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem is cast as a classification task, and the sentence is reduced to a 4-tuple containing the preposition and the non-inflected base forms of the head words of the verb phrase VP and the two noun phrases NP1 and NP2.</S><S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N01-1006.txt | Citing Article:  C08-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>The experiment was performed with the part-ofspeech data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N01-1006.txt | Citing Article:  W06-1620.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches.</S><S sid = NA ssid = NA>The rules are then applied sequentially to the evaluation set in the order they were learned.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N01-1006.txt | Citing Article:  P03-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N01-1006.txt | Citing Article:  W04-0845.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S><S sid = NA ssid = NA>Transformation Based Learning In The Fast Lane</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N01-1006.txt | Citing Article:  W04-2417.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.</S><S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N01-1006.txt | Citing Article:  W03-1728.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The templates used to generate rules are similar to the ones used by Brill and Resnik (1994) and some include WordNet features.</S><S sid = NA ssid = NA>The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N01-1006.txt | Citing Article:  W03-1728.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Transformation Based Learning In The Fast Lane</S><S sid = NA ssid = NA>This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N01-1006.txt | Citing Article:  W05-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N01-1006.txt | Citing Article:  W03-1812.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results of this tagger are presented to provide a performance comparison with a widely used tagger.</S><S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N01-1006.txt | Citing Article:  W03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N01-1006.txt | Citing Article:  W07-2051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.</S><S sid = NA ssid = NA>For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N01-1006.txt | Citing Article:  W07-2074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>The experiment was performed with the part-ofspeech data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N01-1006.txt | Citing Article:  W03-0806.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Transformation Based Learning In The Fast Lane</S><S sid = NA ssid = NA>Again, the ICA algorithm learns the rules very fast, but has a slightly lower performance than the other two TBL systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N01-1006.txt | Citing Article:  N04-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).</S><S sid = NA ssid = NA>Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N01-1006.txt | Citing Article:  W02-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ramshaw & Marcus (1994) attempted to reduce the training time of the algorithm by making the update process more efficient.</S><S sid = NA ssid = NA>The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster.</S> | Discourse Facet:  NA | Annotator: Automatic


