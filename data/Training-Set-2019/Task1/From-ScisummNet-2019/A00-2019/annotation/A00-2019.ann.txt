Citance Number: 1 | Reference Article:  A00-2019.txt | Citing Article:  P06-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time.</S><S sid = NA ssid = NA>ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A00-2019.txt | Citing Article:  W07-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow and Leacock (2000) try to identify errors on the basis of context, as we do here, and more specifically a 2 word window around the word of interest, from which they consider function words and POS tags.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).</S><S sid = NA ssid = NA>They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A00-2019.txt | Citing Article:  D07-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell (1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem.</S><S sid = NA ssid = NA>Unfortunately, this approach was not effective for error detection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A00-2019.txt | Citing Article:  D07-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow and Leacock (2000) use a mutual information measure in addition to raw frequency of n grams.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here we use this measure for the opposite purpose — to find combinations that occur less often than expected.</S><S sid = NA ssid = NA>From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A00-2019.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Among unsupervised checkers, Chodorow and Leacock (2000) exploits negative evidence from edited textual corpora achieving high precision but low recall, while Tsao and Wible (2009) uses general corpus only.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S><S sid = NA ssid = NA>Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A00-2019.txt | Citing Article:  P11-4005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Chodorow and Leacock (2000) exploit bigrams and trigrams of function words and part-of-speech (PoS) tags, while Sun et al (2007) use labeled sequential patterns of function, time expression, and part-of-speech tags.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994).</S><S sid = NA ssid = NA>Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A00-2019.txt | Citing Article:  W11-1412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, unsupervised systems of (Chodorow and Leacock, 2000) and (Tsao and Wible, 2009) leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (Hermet et al, 2008) and (Gamon and Leacock, 2010) use Web as a corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.</S><S sid = NA ssid = NA>In addition to bigram and trigram measures, ALEK compares the target word's part-ofspeech tag in the word-specific corpus and in the general corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A00-2019.txt | Citing Article:  C10-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).</S><S sid = NA ssid = NA>Precision and recall for the three words are shown below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A00-2019.txt | Citing Article:  W11-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence.</S><S sid = NA ssid = NA>ALEK recognizes all of these types of errors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A00-2019.txt | Citing Article:  P11-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An example is the error detection method (Chodorow and Leacock, 2000), which identifies unnatural sequences of POSs as grammatical errors in the writing of learners.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An Unsupervised Method For Detecting Grammatical Errors</S><S sid = NA ssid = NA>We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our method outperforms Microsoft Word03 and ALEK (Chodorow and Leacock, 2000) from Educational Testing Service (ETS) in some cases.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service.</S><S sid = NA ssid = NA>In both cases, the system recognizes that the target is in the wrong syntactic environment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.</S><S sid = NA ssid = NA>An Unsupervised Method For Detecting Grammatical Errors</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method (Chodorow and Leacock, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.</S><S sid = NA ssid = NA>An Unsupervised Method For Detecting Grammatical Errors</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A00-2019.txt | Citing Article:  P07-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK (Chodorow and Leacock, 2000) method used by ETS.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97.</S><S sid = NA ssid = NA>However, its techniques could be incorporated into a grammar checker for native speakers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A00-2019.txt | Citing Article:  W11-1422.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.</S><S sid = NA ssid = NA>Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A00-2019.txt | Citing Article:  W12-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The filter-based system combines unsupervised detection of a set of possible errors (Chodorow and Leacock, 2000) with hand-crafted filters designed to reduce this set to the largest subset of correctly flagged errors and the smallest possible number of false positives.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times.</S><S sid = NA ssid = NA>The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A00-2019.txt | Citing Article:  W12-2027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chodorow and Leacock (2000) found that low-frequency bigrams (sequences of two lexical categories with a negative log-likelihood) are quite reliable predictors of grammatical errors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The mutual information measures provide candidate errors, but this approach overgenerates — it finds rare, but still quite grammatical, sequences.</S><S sid = NA ssid = NA>If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these ngrams.</S> | Discourse Facet:  NA | Annotator: Automatic


