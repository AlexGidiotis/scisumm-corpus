Citance Number: 1 | Reference Article:  P05-1044.txt | Citing Article:  W05-1505.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>Contrastive Estimation: Training Log-Linear Models On Unlabeled Data</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1044.txt | Citing Article:  W09-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>DEL1SUBSEQ and DEL1WORD are poor because they do not give helpful classes of negative evidence: deleting a word or a short subsequence often does very little damage.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1044.txt | Citing Article:  P08-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.</S><S sid = NA ssid = NA>Of course, the validity of this hypothesis will depend on the form of the neighborhood function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1044.txt | Citing Article:  P10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare CE (using neighborhoods from §4) with EM on POS tagging using unlabeled data.</S><S sid = NA ssid = NA>CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1044.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised model selection.</S><S sid = NA ssid = NA>For each (criterion, dataset) pair, we selected the smoothing trial that gave the highest estimation criterion score on a 5K-word development set (also unlabeled).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1044.txt | Citing Article:  W10-2909.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The log-linear models trained by CE used the same feature set, though the feature weights are no longer log-probabilities and there are no sum-to-one constraints.</S><S sid = NA ssid = NA>To allow more trials, we projected the original 45 tags onto a coarser set of 17 (e.g., RB* ADV).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1044.txt | Citing Article:  P10-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A diluted dictionary adds (tag, word) entries so that rare words are allowed with any tag, simulating zero prior knowledge about the word.</S><S sid = NA ssid = NA>This is good for the LENGTH objective, but not for learning good POS tag sequences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1044.txt | Citing Article:  W11-2403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The learner we describe here takes into account not only the observed positive example, but also a set of similar but deprecated negative examples.</S><S sid = NA ssid = NA>The other reason to use CE is to improve accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1044.txt | Citing Article:  P13-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.</S><S sid = NA ssid = NA>Of course, the validity of this hypothesis will depend on the form of the neighborhood function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1044.txt | Citing Article:  P11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).</S><S sid = NA ssid = NA>This is good for the LENGTH objective, but not for learning good POS tag sequences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1044.txt | Citing Article:  P11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>Because the features can take any form and need not be orthogonal, log-linear models can capture arbitrary dependencies in the data and cleanly incorporate them into a model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1044.txt | Citing Article:  P11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To maximize the neighborhood likelihood (Eq.</S><S sid = NA ssid = NA>The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1044.txt | Citing Article:  P10-2039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.</S><S sid = NA ssid = NA>Of course, the validity of this hypothesis will depend on the form of the neighborhood function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1044.txt | Citing Article:  P09-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing (Smith and Eisner, 2005).</S><S sid = NA ssid = NA>CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1044.txt | Citing Article:  C10-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>Contrastive Estimation: Training Log-Linear Models On Unlabeled Data</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1044.txt | Citing Article:  P12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>Contrastive Estimation: Training Log-Linear Models On Unlabeled Data</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1044.txt | Citing Article:  P12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.</S><S sid = NA ssid = NA>We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1044.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Good neighborhoods, rather, perform well in their own right.</S><S sid = NA ssid = NA>In Smith and Eisner (2005), we define a sentence’s neighborhood to be a set of slightly-altered sentences that use the same lexemes, as suggested at the start of this section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1044.txt | Citing Article:  P08-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).</S><S sid = NA ssid = NA>The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1044.txt | Citing Article:  P13-1099.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 as contrastive estimation (CE).</S><S sid = NA ssid = NA>Contrastive Estimation: Training Log-Linear Models On Unlabeled Data</S> | Discourse Facet:  NA | Annotator: Automatic


