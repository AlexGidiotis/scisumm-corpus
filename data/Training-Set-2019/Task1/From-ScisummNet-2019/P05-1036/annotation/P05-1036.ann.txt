Citance Number: 1 | Reference Article:  P05-1036.txt | Citing Article:  E06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.</S><S sid = NA ssid = NA>This supervised version compresses better than either version of the supervised noisy-channel model that lacks these rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1036.txt | Citing Article:  N07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Let us consider a simplified version of a K&M example, but as reinterpreted for our model: how the noisy channel model assigns a probability of the compressed tree (A) in Figure 3 given the original tree B.</S><S sid = NA ssid = NA>We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1036.txt | Citing Article:  P06-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.</S><S sid = NA ssid = NA>We create joint rules using only the first section (0.mrg) of the Penn Treebank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1036.txt | Citing Article:  P06-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S><S sid = NA ssid = NA>One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1036.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S><S sid = NA ssid = NA>There are several hundred rules of this type, and it is very simple to incorporate into our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1036.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S><S sid = NA ssid = NA>There are several hundred rules of this type, and it is very simple to incorporate into our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1036.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S><S sid = NA ssid = NA>There are several hundred rules of this type, and it is very simple to incorporate into our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1036.txt | Citing Article:  P06-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S><S sid = NA ssid = NA>There are several hundred rules of this type, and it is very simple to incorporate into our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1036.txt | Citing Article:  P06-2019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S><S sid = NA ssid = NA>The K&M model uses parse trees for the sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1036.txt | Citing Article:  P13-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Example 2 shows how unsupervised and semisupervised techniques can be used to improve compression.</S><S sid = NA ssid = NA>As will be shown, this rule is not constraining enough and allows some poor compressions, but it is remarkable that any sort of compression can be achieved without training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1036.txt | Citing Article:  P08-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S><S sid = NA ssid = NA>The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1036.txt | Citing Article:  P08-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.</S><S sid = NA ssid = NA>The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1036.txt | Citing Article:  W11-1601.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).</S><S sid = NA ssid = NA>One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1036.txt | Citing Article:  W11-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.</S><S sid = NA ssid = NA>There are several hundred rules of this type, and it is very simple to incorporate into our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>None of the special rules are applied.</S><S sid = NA ssid = NA>Supervised And Unsupervised Learning For Sentence Compression</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.</S><S sid = NA ssid = NA>Knight and Marcu (henceforth K&M) introduce the task of statistical sentence compression in Statistics-Based Summarization - Step One: Sentence Compression (Knight and Marcu, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1036.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>None of the special rules are applied.</S><S sid = NA ssid = NA>In a pure PCFG this would only include the label of the node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1036.txt | Citing Article:  P13-1151.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).</S><S sid = NA ssid = NA>One of the biggest problems with this model of sentence compression is the lack of appropriate training data.</S> | Discourse Facet:  NA | Annotator: Automatic


