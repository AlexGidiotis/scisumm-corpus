Citance Number: 1 | Reference Article:  P06-1072.txt | Citing Article:  W06-1638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.</S><S sid = NA ssid = NA>We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-1072.txt | Citing Article:  I08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).</S><S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-1072.txt | Citing Article:  P09-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Grammar induction serves as a tidy example for structural annealing.</S><S sid = NA ssid = NA>As δ increases, we penalize long dependencies less.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-1072.txt | Citing Article:  P09-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979).</S><S sid = NA ssid = NA>The central idea of this paper is to gradually change (anneal) the bias δ.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-1072.txt | Citing Article:  P09-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Annealing Structural Bias In Multilingual Weighted Grammar Induction</S><S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-1072.txt | Citing Article:  C08-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).</S><S sid = NA ssid = NA>first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-1072.txt | Citing Article:  P09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Early in learning, local dependencies are emphasized by setting δ « 0.</S><S sid = NA ssid = NA>With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-1072.txt | Citing Article:  P09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that structural annealing does not always outperform fixed-δ training (English and Portuguese).</S><S sid = NA ssid = NA>Experiment: Annealing β.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-1072.txt | Citing Article:  W09-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).</S><S sid = NA ssid = NA>first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-1072.txt | Citing Article:  P08-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We could describe “brokenness” as a feature in the model whose weight, Q, is chosen extrinsically (and time-dependently), rather than empirically—just as was done with S. Annealing β resembles the popular bootstrapping technique (Yarowsky, 1995), which starts out aiming for high precision, and gradually improves coverage over time.</S><S sid = NA ssid = NA>At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-1072.txt | Citing Article:  W09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).</S><S sid = NA ssid = NA>first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-1072.txt | Citing Article:  N10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S><S sid = NA ssid = NA>Experiment: Locality Bias within CE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-1072.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2006) used a structural locality bias, experimenting on five languages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiment: Locality Bias within CE.</S><S sid = NA ssid = NA>We used the EM algorithm to train this model on POS sequences in six languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-1072.txt | Citing Article:  P09-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Performance with unsupervised and supervised model selection across different λ values in add-λ smoothing and three initializers O(0) is reported in Table 1.</S><S sid = NA ssid = NA>In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words—can give guaranteed O(nk2) runtime with little to no loss in accuracy (for simple models).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-1072.txt | Citing Article:  P10-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length > 10.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As originally proposed, CE allowed a redefinition of the implicit negative evidence from “all other sentences” (as in MLE) to “sentences like xi, but perturbed.” Allowing segmentation of the training sentences redefines the positive and negative evidence.</S><S sid = NA ssid = NA>This allows the learner to accept hypotheses that explain the sentences as independent pieces.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-1072.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997).</S><S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-1072.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These alignment classes are called configurations (Smith and Eisner, 2006a, and following).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S><S sid = NA ssid = NA>Following common practice, we always replace words by part-ofspeech (POS) tags before training or testing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-1072.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model.</S><S sid = NA ssid = NA>This method was applied with some success to grammar induction models by Smith and Eisner (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-1072.txt | Citing Article:  D09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.</S><S sid = NA ssid = NA>We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-1072.txt | Citing Article:  W11-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias.</S><S sid = NA ssid = NA>Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM.</S> | Discourse Facet:  NA | Annotator: Automatic


