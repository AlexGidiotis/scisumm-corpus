Citance Number: 1 | Reference Article:  D09-1092.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S><S sid = NA ssid = NA>We use the “left-to-right” method of (Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1092.txt | Citing Article:  P10-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid = NA ssid = NA>We use the “left-to-right” method of (Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1092.txt | Citing Article:  P11-2084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid = NA ssid = NA>For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1092.txt | Citing Article:  E12-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Polylingual Topic Models</S><S sid = NA ssid = NA>Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1092.txt | Citing Article:  D11-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid = NA ssid = NA>These aligned document pairs could then be fed into standard machine translation systems as training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1092.txt | Citing Article:  N12-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we present the polylingual topic model (PLTM).</S><S sid = NA ssid = NA>No paper is exactly comparable to any other paper, but they are all roughly topically similar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1092.txt | Citing Article:  N12-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid = NA ssid = NA>Divergence drops significantly when the proportion of “glue” tuples increases from 0.01 to 0.25.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Polylingual Topic Models</S><S sid = NA ssid = NA>The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Interestingly, all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid = NA ssid = NA>A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid = NA ssid = NA>A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S><S sid = NA ssid = NA>We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S><S sid = NA ssid = NA>We use the “left-to-right” method of (Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use both Jensen-Shannon divergence and cosine distance.</S><S sid = NA ssid = NA>Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the “left-to-right” method of (Wallach et al., 2009).</S><S sid = NA ssid = NA>A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1092.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).</S><S sid = NA ssid = NA>Given a corpus of training and test document tuples—W and W', respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1092.txt | Citing Article:  W12-3117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Polylingual Topic Models</S><S sid = NA ssid = NA>The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1092.txt | Citing Article:  W11-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Polylingual Topic Models</S><S sid = NA ssid = NA>The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1092.txt | Citing Article:  W11-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, the growth of the web, and in particular Wikipedia, has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid = NA ssid = NA>We use the “left-to-right” method of (Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1092.txt | Citing Article:  P14-2110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Polylingual Topic Models</S><S sid = NA ssid = NA>We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D09-1092.txt | Citing Article:  P14-2110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid = NA ssid = NA>We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S> | Discourse Facet:  NA | Annotator: Automatic


