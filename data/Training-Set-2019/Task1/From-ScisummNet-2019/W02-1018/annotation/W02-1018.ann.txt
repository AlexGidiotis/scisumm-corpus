Citance Number: 1 | Reference Article:  W02-1018.txt | Citing Article:  W03-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.</S><S sid = NA ssid = NA>The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-1018.txt | Citing Article:  W03-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This yields conditional probability distributions and which we use for decoding.</S><S sid = NA ssid = NA>This yields conditional probability distributions and which we use for decoding.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-1018.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Marcu and Wong, 2002), a joint probability phrase model is presented.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.</S><S sid = NA ssid = NA>Figure 2: Training algorithm for the phrase-based joint probability model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-1018.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).</S><S sid = NA ssid = NA>Figure 2: Training algorithm for the phrase-based joint probability model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-1018.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).</S><S sid = NA ssid = NA>For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-1018.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.</S><S sid = NA ssid = NA>We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-1018.txt | Citing Article:  C10-2084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.</S><S sid = NA ssid = NA>To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-1018.txt | Citing Article:  P13-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the data.</S><S sid = NA ssid = NA>Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-1018.txt | Citing Article:  W07-0709.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2: Training algorithm for the phrase-based joint probability model.</S><S sid = NA ssid = NA>A Phrase-Based Joint Probability Model For Statistical Machine Translation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-1018.txt | Citing Article:  P09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first iterations estimate the alignment probabilities using Model 1.</S><S sid = NA ssid = NA>The first iterations estimate the alignment probabilities using Model 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-1018.txt | Citing Article:  N07-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Phrase-Based Joint Probability Model For Statistical Machine Translation</S><S sid = NA ssid = NA>We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-1018.txt | Citing Article:  H05-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).</S><S sid = NA ssid = NA>For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-1018.txt | Citing Article:  W11-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) propose a joint probability model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.</S><S sid = NA ssid = NA>Figure 2: Training algorithm for the phrase-based joint probability model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-1018.txt | Citing Article:  P08-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.</S><S sid = NA ssid = NA>3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-1018.txt | Citing Article:  P08-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Indeed, Marcu and Wong (2002) conjectures that none exist.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).</S><S sid = NA ssid = NA>We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-1018.txt | Citing Article:  P08-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.</S><S sid = NA ssid = NA>We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-1018.txt | Citing Article:  P08-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The sentences in the corpus were at most 20 words long.</S><S sid = NA ssid = NA>The sentences in the corpus were at most 20 words long.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-1018.txt | Citing Article:  P08-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) describes an approximation to O.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.</S><S sid = NA ssid = NA>However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-1018.txt | Citing Article:  P05-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f| e) into features of a log-linear model (Och and Ney, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2: Training algorithm for the phrase-based joint probability model.</S><S sid = NA ssid = NA>A Phrase-Based Joint Probability Model For Statistical Machine Translation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-1018.txt | Citing Article:  W06-3123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Phrase-Based Joint Probability Model For Statistical Machine Translation</S><S sid = NA ssid = NA>We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


