Citance Number: 1 | Reference Article:  D11-1125.txt | Citing Article:  P14-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.</S><S sid = NA ssid = NA>We repeated each setting three times, generating different random data each time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1125.txt | Citing Article:  P13-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Throughout all experiments with PRO we choose Γ = 5000, Ξ = 50, and the following step function α for each αz: 6 We used MegaM III, 2004) as a binary classifier in our contrasting synthetic experiment and of the i.e., with all default settings for binary Figure 3 shows that PRO is able to learn nearly perfectly at all dimensionalities from 10 to 1000.</S><S sid = NA ssid = NA>We used the MegaM classifier and sampled as described in Section 4.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1125.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts.</S><S sid = NA ssid = NA>The results of the noisy synthetic experiments, but still The idea of learning from difference vectors also lies at the heart of the MIRA-based approaches (Watanabe et al., 2007; Chiang et al., 2008b) and the approach of Roth et al. (2010), which, similar to our method, uses sampling to select vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1125.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Tuning as Ranking</S><S sid = NA ssid = NA>We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1125.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).</S><S sid = NA ssid = NA>Tuning as Ranking</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1125.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.</S><S sid = NA ssid = NA>We repeated each setting three times, generating different random data each time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1125.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Tillmann and Zhang (2005) used a customized form of multi-class stochastic gradient descent to learn feature weights for an MT model.</S><S sid = NA ssid = NA>In practice, tuning optimizes over a finite subset of source sentences3 and a finite subset of candidate translations as well.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1125.txt | Citing Article:  P13-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007).</S><S sid = NA ssid = NA>Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D11-1125.txt | Citing Article:  N12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.</S><S sid = NA ssid = NA>Tuning as Ranking</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D11-1125.txt | Citing Article:  N12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations.</S><S sid = NA ssid = NA>Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D11-1125.txt | Citing Article:  N12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.8 At the end of every experiment we used the final feature weights to decode a held-out test set and evaluated it with case-sensitive BLEU.</S><S sid = NA ssid = NA>We used the MegaM classifier and sampled as described in Section 4.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D11-1125.txt | Citing Article:  P13-2121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained a 4-gram English language model on the English side of the training data.</S><S sid = NA ssid = NA>We trained a 5-gram English language model on the English side of the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D11-1125.txt | Citing Article:  N12-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hopkins and May (2011) presented a method that uses a binary classifier.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours.</S><S sid = NA ssid = NA>Throughout all experiments with PRO we choose Γ = 5000, Ξ = 50, and the following step function α for each αz: 6 We used MegaM III, 2004) as a binary classifier in our contrasting synthetic experiment and of the i.e., with all default settings for binary Figure 3 shows that PRO is able to learn nearly perfectly at all dimensionalities from 10 to 1000.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D11-1125.txt | Citing Article:  N12-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Tuning as Ranking</S><S sid = NA ssid = NA>Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D11-1125.txt | Citing Article:  N12-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k= 1500), and the sampler samples 5000 pairs (n= 5000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For each source sentence i, the sampler generates F candidate translation pairs hj, j'i, and accepts each pair with probability αi(|g(i,j) − g(i, j')|).</S><S sid = NA ssid = NA>We used the following feature classes in SBMT and PBMT extended scenarios: We used the following feature classes in SBMT extended scenarios only (cf.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D11-1125.txt | Citing Article:  P13-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.</S><S sid = NA ssid = NA>We repeated each setting three times, generating different random data each time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D11-1125.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An advantage of MERT is that it can directly optimize for non-decomposable scoring functions like BLEU.</S><S sid = NA ssid = NA>Tuning as Ranking</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D11-1125.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For each source sentence i, the sampler generates F candidate translation pairs hj, j'i, and accepts each pair with probability αi(|g(i,j) − g(i, j')|).</S><S sid = NA ssid = NA>Among other simplifications, we abstract away the choice of MIRA as the classification method (our approach can use any classification technique that learns a separating hyperplane), and we eliminate the need for oracle translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D11-1125.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to modify MERT so that it scales well to high-dimensionality candidate spaces.</S><S sid = NA ssid = NA>While MERT and MIRA use each iteration’s final weights as a starting point for hill-climbing the next iteration, the pairwise ranking approach has no explicit tie to previous iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D11-1125.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used the MegaM classifier and sampled as described in Section 4.2.</S><S sid = NA ssid = NA>We obtained similar results using P = = 100, and for each a logistic sigmoid function centered at the mean g differential of candidate translation pairs for the ith source sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


