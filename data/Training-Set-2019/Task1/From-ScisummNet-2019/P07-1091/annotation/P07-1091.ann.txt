Citance Number: 1 | Reference Article:  P07-1091.txt | Citing Article:  P14-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).</S><S sid = NA ssid = NA>It should be noted that the second Chinese block “R#, HI” and its English counterpart “at the end of” are not constituents at all.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1091.txt | Citing Article:  C10-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases.</S><S sid = NA ssid = NA>Maximum Entropy (ME) Model, which does the binary classification whether a binary node’s children are inverted or not, based on a set of features over the SL phrases corresponding to the two children nodes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1091.txt | Citing Article:  C08-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The most notable ones are (Xia and McCord, 2004) and (Collins et al., 2005), both of which make use of linguistic syntax in the preprocessing stage.</S><S sid = NA ssid = NA>A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reordering and other factors of translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1091.txt | Citing Article:  C08-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.</S><S sid = NA ssid = NA>A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1091.txt | Citing Article:  D09-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out.</S><S sid = NA ssid = NA>By and large, the experiment results show that no matter what kind of reordering knowledge is used, the preprocessing of syntax-based reordering does greatly improve translation performance, and that the reordering of 3-ary nodes is crucial.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1091.txt | Citing Article:  D09-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The distinction between long and short distance reordering is solely defined by distortion limit.</S><S sid = NA ssid = NA>A terminological remark: In the rest of the paper, we will use the terms global reordering and local reordering in place of long-distance reordering and short-distance reordering respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1091.txt | Citing Article:  W10-3803.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reordering and other factors of translation.</S><S sid = NA ssid = NA>However, there are also reorderings which do not agree with syntactic analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1091.txt | Citing Article:  C10-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To overcome the problem, Li et al (2007) proposed k-best approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).</S><S sid = NA ssid = NA>From this point, the proposed preprocessing model correctly jump to the last phrase “Ili T ÿX/discussed”, while the baseline model fail to do so for the best translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1091.txt | Citing Article:  W08-0406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.</S><S sid = NA ssid = NA>A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1091.txt | Citing Article:  W10-1762.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our experiments are about Chinese-to-English translation.</S><S sid = NA ssid = NA>Maximum Entropy (ME) Model, which does the binary classification whether a binary node’s children are inverted or not, based on a set of features over the SL phrases corresponding to the two children nodes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1091.txt | Citing Article:  W10-1762.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Case-sensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric.</S><S sid = NA ssid = NA>A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1091.txt | Citing Article:  W12-4212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, long-distance reordering is problematic in phrase-based SMT.</S><S sid = NA ssid = NA>Therefore, while short-distance reordering is under the scope of the distance-based model, long-distance reordering is simply out of the question.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1091.txt | Citing Article:  C08-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Shallow parsers should be tried to see if they improve the quality of reordering knowledge.</S><S sid = NA ssid = NA>To avoid this problem, we give up using rewriting patterns and design a form of reordering knowledge which can be directly applied to parse tree nodes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1091.txt | Citing Article:  C08-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Li et al (2007) used a parser to get the syntactic tree of the source language sentence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.</S><S sid = NA ssid = NA>The GIGAWORD corpus is used for training language model.</S> | Discourse Facet:  NA | Annotator: Automatic


