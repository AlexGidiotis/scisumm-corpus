Citance Number: 1 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.</S><S sid = NA ssid = NA>The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S><S sid = NA ssid = NA>However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P10-2041.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S><S sid = NA ssid = NA>However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P10-2041.txt | Citing Article:  P14-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = −0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).</S><S sid = NA ssid = NA>We are aware of two comparable previous approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P10-2041.txt | Citing Article:  P14-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used the text from 1999 through 2008 as in-domain training data, and we used the first 2000 sentences from January 2009 as test data.</S><S sid = NA ssid = NA>To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P10-2041.txt | Citing Article:  W12-3148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lin et al. (1997) and Gao et al.</S><S sid = NA ssid = NA>As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P10-2041.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.</S><S sid = NA ssid = NA>The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P10-2041.txt | Citing Article:  W12-3149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S><S sid = NA ssid = NA>To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lin et al. (1997) and Gao et al.</S><S sid = NA ssid = NA>(2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).</S><S sid = NA ssid = NA>We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To make these language models comparable, and to show the feasibility of optimizing the fit to the in-domain data without training a model on the entire Gigaword corpus, we trained the Gigaword language model for data selection on a random sample of the Gigaword corpus of a similar size to that of the Europarl training data: 1,874,051 sentences, 48,459,945 tokens.</S><S sid = NA ssid = NA>Klakow (2000) estimates a unigram language model from the entire non-domain-specific corpus to be selected from, and scores each candidate text segment from that corpus by the change in the log likelihood of the in-domain data according to the unigram model, if that segment were removed from the corpus used to estimate the unigram model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.</S><S sid = NA ssid = NA>The test set perplexity for the language model trained on the full Gigaword corpus is 135.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P10-2041.txt | Citing Article:  D11-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).</S><S sid = NA ssid = NA>However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P10-2041.txt | Citing Article:  W12-3140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.</S><S sid = NA ssid = NA>For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P10-2041.txt | Citing Article:  P11-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.</S><S sid = NA ssid = NA>Intelligent Selection of Language Model Training Data</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P10-2041.txt | Citing Article:  W12-3137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.</S><S sid = NA ssid = NA>For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P10-2041.txt | Citing Article:  P13-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lin et al. (1997) and Gao et al.</S><S sid = NA ssid = NA>As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P10-2041.txt | Citing Article:  P13-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have empirically evaluated our proposed method for selecting data from a non-domainspecific source to model text in a specific domain.</S><S sid = NA ssid = NA>Equivalently, we can work in the log domain with the quantity log(P(s|I)) − log(P(s|N)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P10-2041.txt | Citing Article:  P13-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.</S><S sid = NA ssid = NA>Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P10-2041.txt | Citing Article:  D12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.</S><S sid = NA ssid = NA>In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.</S> | Discourse Facet:  NA | Annotator: Automatic


