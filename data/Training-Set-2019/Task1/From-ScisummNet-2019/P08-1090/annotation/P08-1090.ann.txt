Citance Number: 1 | Reference Article:  P08-1090.txt | Citing Article:  N09-5001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised Learning of Narrative Event Chains</S><S sid = NA ssid = NA>Narrative chains are partially ordered sets of events centered around a common protagonist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1090.txt | Citing Article:  W10-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>They are related to structured sequences of participants and events that have been called scripts (Schank and Abelson, 1977) or Fillmorean frames.</S><S sid = NA ssid = NA>However, it is worthwhile to construct discrete narrative chains, if only to see whether the combination of event learning and ordering produce scriptlike structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1090.txt | Citing Article:  W10-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised Learning of Narrative Event Chains</S><S sid = NA ssid = NA>This paper induces a new representation of structured knowledge called narrative event chains (or narrative chains).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1090.txt | Citing Article:  W10-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.</S><S sid = NA ssid = NA>We make the following assumption of narrative coherence: verbs sharing coreferring arguments are semantically connected by virtue of narrative discourse structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1090.txt | Citing Article:  W10-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We utilize the definition of PMI described in (Chambers and Jurafsky, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A narrative chain, by definition, includes a partial ordering of events.</S><S sid = NA ssid = NA>The PMI is then defined as between all occurrences of two verbs in the same document.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1090.txt | Citing Article:  W10-0905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For this we need a metric for the relation between an event and a narrative chain.</S><S sid = NA ssid = NA>A distributional score based on how often two events share grammatical arguments (using pointwise mutual information) is used to create this pairwise relation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1090.txt | Citing Article:  P10-1160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised Learning of Narrative Event Chains</S><S sid = NA ssid = NA>Other features include event-event syntactic properties such as the syntactic dominance relations between the two events, as well as new bigram features of tense, aspect and class (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1090.txt | Citing Article:  P10-1158.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper presents work toward a partial ordering and leaves logical constraints as future work.</S><S sid = NA ssid = NA>The second applies a temporal classifier to partially order the connected events.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1090.txt | Citing Article:  E12-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A distributional score based on how often two events share grammatical arguments (using pointwise mutual information) is used to create this pairwise relation.</S><S sid = NA ssid = NA>Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1090.txt | Citing Article:  E12-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present a new cloze task that requires narrative knowledge to solve, the narrative cloze.</S><S sid = NA ssid = NA>We show, using a new evaluation task called narrative cloze, that our protagonist-based method leads to better induction than a verb-only approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1090.txt | Citing Article:  E12-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, we applied state of the art temporal classification to show that sets of events can be partially ordered.</S><S sid = NA ssid = NA>The Timebank corpus does not include obituaries, thus we suffer from sparsity in training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1090.txt | Citing Article:  P10-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used 10 news stories from the 1994 section of the corpus for development.</S><S sid = NA ssid = NA>We used 69 news stories from the 2001 (year selected randomly) section of the corpus for testing (also removed from training).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1090.txt | Citing Article:  N12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Narrative chains are partially ordered sets of events centered around a common protagonist.</S><S sid = NA ssid = NA>A narrative event chain is a partially ordered set of events related by a common protagonist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1090.txt | Citing Article:  N12-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A narrative chain can be viewed as defining the semantic roles of an event, constraining it against roles of the other events in the chain.</S><S sid = NA ssid = NA>For each document, the verb pairs that share coreferring entities are recorded with their dependency types.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1090.txt | Citing Article:  N12-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach is called Protagonist.</S><S sid = NA ssid = NA>A single document may contain more than one narrative (or topic), but the narrative assumption states that a series of argument-sharing verbs is more likely to participate in a narrative chain than those not sharing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1090.txt | Citing Article:  N12-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.</S><S sid = NA ssid = NA>We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006)3, recording all verbs with subject, object, or prepositional typed dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1090.txt | Citing Article:  N12-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These verb/dependency events make up a narrative cloze model.</S><S sid = NA ssid = NA>Our evaluation data is the same 69 documents used in the test set for learning narrative relations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1090.txt | Citing Article:  E12-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.</S><S sid = NA ssid = NA>Taking a cue from Mani et al. (2006), we also increased Timebank’s size by applying transitivity rules to the hand labeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1090.txt | Citing Article:  D12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each pair of events in a gigaword document that share a coreferring argument is treated as a separate ordering classification task.</S><S sid = NA ssid = NA>The PMI is then defined as between all occurrences of two verbs in the same document.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1090.txt | Citing Article:  D12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From each document, the entity involved in the most events was selected as the protagonist.</S><S sid = NA ssid = NA>Learning these prototypical schematic sequences of events is important for rich understanding of text.</S> | Discourse Facet:  NA | Annotator: Automatic


