Citance Number: 1 | Reference Article:  W03-1508.txt | Citing Article:  C04-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second translation system, for converting pinyin sequences to character sequences, has a one-toone mapping between symbols and therefore has no words with zero fertility.</S><S sid = NA ssid = NA>The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W03-1508.txt | Citing Article:  P06-1142.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the IBM model described earlier, these are the words which may be “deleted” by the noisy channel when transforming into .</S><S sid = NA ssid = NA>The IBM source-channel model for statistical machine translation (P. Brown et al., 1993) plays a central role in our system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W03-1508.txt | Citing Article:  N07-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The system was developed to investigate knowledgelight methods for linguistic processing in text retrieval.</S><S sid = NA ssid = NA>The Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) is a research retrieval system developed at the Johns Hopkins University Applied Physics Laboratory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W03-1508.txt | Citing Article:  N07-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that significant improvements in transliteration performance result from this alternate method of data selection.</S><S sid = NA ssid = NA>A small improvement in mAP is obtained by the Haircut system with name transliteration over the system without name transliteration: the improvement from 0.501 to 0.515 is statistically significant at a -value of 0.084.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W03-1508.txt | Citing Article:  H05-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We concede that this performance, while comparable to other systems, is not satisfactory and merits further investigation.</S><S sid = NA ssid = NA>In any event, a need for improvement in transliteration is suggested by this result.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W03-1508.txt | Citing Article:  D09-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We expect to further refine the translation models in the future and apply them in other tasks such as text translation.</S><S sid = NA ssid = NA>In a more intrinsic and direct evaluation, we have found ways to gainfully filter a large but noisy training corpus to augment the training data for our models and improve transliteration accuracy considerably beyond our starting point, e.g., to reduce Pin-yin error rates from 51.1% to 42.5%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W03-1508.txt | Citing Article:  N09-3011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s (e, f)= Pr (f |e) Pr (e).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We estimated a pin-yin language model from the training portion above.</S><S sid = NA ssid = NA>Various smoothing methods have been proposed to combine the contributions for each term based on the document model and also a generic model of the language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W03-1508.txt | Citing Article:  P09-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Several techniques have been proposed in the recent past for name transliteration.</S><S sid = NA ssid = NA>For the phoneme-to-GIF translation model, the “words” which need to be inserted in this manner are syllabic nuclei!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.</S><S sid = NA ssid = NA>Note that while significantly lower error rates have been reported for converting pin-yin to characters in generic Chinese text, ours is a highly specialized subset of transliterated foreign names, where the choice between several characters sharing the same pin-yin symbol is somewhat arbitrary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results are shown in Table 1, where pin-yin error rate is the edit distance between the “correct” pin-yin representation of the correct transliteration and the pin-yin sequence output by the system.</S><S sid = NA ssid = NA>Our results and the corresponding results from Meng et al (2001) are reported in Table 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W03-1508.txt | Citing Article:  P04-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The result of this evaluation is reported in Table 3 against the line “Huge MT (Self),” where we also report the transliteration performance of the so-called Big MT system of Table 1 on this new test set.</S><S sid = NA ssid = NA>We then aligned all the (nearly 1M) training “sentence” pairs with this translation model, and extracted roughly a third of the sentences with an alignment score above a certain tunable threshold ().</S> | Discourse Facet:  NA | Annotator: Automatic


