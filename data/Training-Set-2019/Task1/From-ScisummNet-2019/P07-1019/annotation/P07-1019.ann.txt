Citance Number: 1 | Reference Article:  P07-1019.txt | Citing Article:  W07-0701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since we approach decoding as xR transduction, the process is identical to that of constituency based algorithms (e.g. Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This process can be formalized as a deductive system.</S><S sid = NA ssid = NA>We develop faster approaches for problem based on parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P07-1019.txt | Citing Article:  P14-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The different target sides then constitute a third dimension of the grid, forming a cube of possible combinations (Chiang, 2007).</S><S sid = NA ssid = NA>These two deductive systems represent the search space of decoding without a language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P07-1019.txt | Citing Article:  D10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We push the idea behind this method further and make the following contributions in this paper: Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from −LM decoding.</S><S sid = NA ssid = NA>In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P07-1019.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A hyper graph is analogous to a parse forest (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S><S sid = NA ssid = NA>In other words, we would like to have an estimate of the best item not explored yet (analogous to the heuristic function in A* search).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P07-1019.txt | Citing Article:  W11-2167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring.</S><S sid = NA ssid = NA>The resulting translation t2t1 is the inverted concatenation as specified by the target-side of the SCFG rule with the additional cost c′ being the cost of this rule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P07-1019.txt | Citing Article:  D08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999).</S><S sid = NA ssid = NA>We set the decoder phrase-table limit to 100 as suggested in (Koehn, 2004) and the distortion limit to 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P07-1019.txt | Citing Article:  D12-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a).</S><S sid = NA ssid = NA>At the lowest level of search error, the relative speed-up from cube growing and cube pruning compared with full-integration is by a factor of 9.8 and 4.1, respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P07-1019.txt | Citing Article:  P10-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In practice, one must prune the search space aggressively to reduce it to a reasonable size.</S><S sid = NA ssid = NA>These two deductive systems represent the search space of decoding without a language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P07-1019.txt | Citing Article:  W11-2142.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S><S sid = NA ssid = NA>In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P07-1019.txt | Citing Article:  N12-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The experiments in this paper use trigram models.</S><S sid = NA ssid = NA>This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P07-1019.txt | Citing Article:  P11-1128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S><S sid = NA ssid = NA>Forest Rescoring: Faster Decoding with Integrated Language Models</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P07-1019.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>See Figure 2 for the pseudocode for cube pruning.</S><S sid = NA ssid = NA>Although much faster than full-integration, cube pruning still computes a fixed amount of +LM items at each node, many of which will not be useful for arriving at the 1-best hypothesis at the root.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P07-1019.txt | Citing Article:  W11-2211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In hierarchical phrase-based translation (Chiang, 2005) a weighted synchronous context-free grammar is induced from parallel text, the search is based on CYK+ parsing (Chappelier and Rajman, 1998) and typically carried out using the cube pruning algorithm (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We establish in this section a unified framework for translation with an integrated n-gram language model in both phrase-based systems and syntaxbased systems based on synchronous context-free grammars (SCFGs).</S><S sid = NA ssid = NA>By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P07-1019.txt | Citing Article:  D08-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest Rescoring: Faster Decoding with Integrated Language Models</S><S sid = NA ssid = NA>Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P07-1019.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a sub translation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004).</S><S sid = NA ssid = NA>Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P07-1019.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The complexity of this dynamic programming algorithm for g-gram decoding is O (2nn2|V|g-1) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999).</S><S sid = NA ssid = NA>The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P07-1019.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a novel extension of cube pruning called cube growing, and shown how both can be seen as general forest rescoring techniques applicable to both phrase-based and syntax-based decoding.</S><S sid = NA ssid = NA>Cube pruning (Chiang, 2007) reduces the search space significantly based on the observation that when the above method is combined with beam search, only a small fraction of the possible +LM items at a node will escape being pruned, and moreover we can select with reasonable accuracy those top-k items without computing all possible items first.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P07-1019.txt | Citing Article:  W12-3137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We utilize the cube pruning algorithm (Huang and Chiang, 2007) for decoding and optimize the model weights with MERT.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1).</S><S sid = NA ssid = NA>The weights for the log-linear model are tuned on a separate development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P07-1019.txt | Citing Article:  P11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation.</S><S sid = NA ssid = NA>Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P07-1019.txt | Citing Article:  N09-2036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang and Chiang (2007) describe a variation of cube pruning called cube growing, and they apply it to a source-tree to target string translator.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007).</S><S sid = NA ssid = NA>In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


