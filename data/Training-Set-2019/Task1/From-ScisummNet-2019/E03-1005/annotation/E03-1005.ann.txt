Citance Number: 1 | Reference Article:  E03-1005.txt | Citing Article:  P12-2001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S><S sid = NA ssid = NA>As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E03-1005.txt | Citing Article:  N06-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Data-Oriented Parsing (DOP)'s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S><S sid = NA ssid = NA>Bod (2001, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E03-1005.txt | Citing Article:  D07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the following section first results of SL-DOP and LS-DOP with a compact PCFG-reduction. we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).</S><S sid = NA ssid = NA>We will refer to this model as Simplicity-DOP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E03-1005.txt | Citing Article:  D07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent.</S><S sid = NA ssid = NA>Bod (2001, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E03-1005.txt | Citing Article:  P11-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E03-1005.txt | Citing Article:  P04-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our best performing model is more accurate than all these previous models except (Bod, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E03-1005.txt | Citing Article:  P04-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S><S sid = NA ssid = NA>The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E03-1005.txt | Citing Article:  P08-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S><S sid = NA ssid = NA>As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E03-1005.txt | Citing Article:  W06-3603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S><S sid = NA ssid = NA>As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E03-1005.txt | Citing Article:  E06-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, (Bod, 2003) changes the way frequencies fi are counted, with a similar effect.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>Similarly, there are ci + 1 possibilities on the right branch.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E03-1005.txt | Citing Article:  E06-2025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S><S sid = NA ssid = NA>As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E03-1005.txt | Citing Article:  W06-2905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.</S><S sid = NA ssid = NA>It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E03-1005.txt | Citing Article:  W06-2905.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We approximated the most probable parse as follows (following (Bod, 2003)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>Goodman's construction is as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E03-1005.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This result is only slightly higher than the highest reported result for this test-set, Bod's (.907) (Bod,2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E03-1005.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S><S sid = NA ssid = NA>As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E03-1005.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization, which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn, and since 0<p<1, the derivation with the fewest subtrees has the greatest probability.</S><S sid = NA ssid = NA>In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E03-1005.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S><S sid = NA ssid = NA>An Efficient Implementation Of A New DOP Model</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E03-1005.txt | Citing Article:  P07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Of course, it is well-known that a supervised parser's f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.</S><S sid = NA ssid = NA>One instantiation of DOP which has received considerable interest is the model known as DOP12 (Bod 1992).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E03-1005.txt | Citing Article:  W04-0305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.</S><S sid = NA ssid = NA>This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E03-1005.txt | Citing Article:  P06-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (2001, 2003).</S><S sid = NA ssid = NA>Thus, there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation with probability 1/a.</S> | Discourse Facet:  NA | Annotator: Automatic


