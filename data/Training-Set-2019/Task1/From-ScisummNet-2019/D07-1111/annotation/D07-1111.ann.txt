Citance Number: 1 | Reference Article:  D07-1111.txt | Citing Article:  W07-2220.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This technique, first proposed by Sagae and Lavie (2006), was used in the highest scoring system in both the multilingual track (Hall et al, 2007a) and the domain adaptation track (Sagae and Tsujii, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1044 task (Nivre et al, 2007), which differed from the 2006 edition by featuring two separate tracks, one in multilingual parsing, and a new track on domain adaptation for dependency parsers.</S><S sid = NA ssid = NA>Our system?s accuracy was the highest in the domain adaptation track (with labeled attachment score of 81.06%), and only 0.43% below the top scoring system in the multilingual parsing track (our average labeled attachment score over the ten languages was 79.89%).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1111.txt | Citing Article:  W07-2220.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best performing (closed class) system in the domain adaptation track used a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis (Sagae and Tsujii, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.</S><S sid = NA ssid = NA>SVM models using the out-of-domain labeled training data; 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1111.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.</S><S sid = NA ssid = NA>We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1111.txt | Citing Article:  W11-1816.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lastly we used a native dependency parser, the GENIA Dependency parser (GDep) by Sagae and Tsujii (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles</S><S sid = NA ssid = NA>We then show how we extend the deterministic parser into a best first probabilistic parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1111.txt | Citing Article:  W09-1403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Acknowledgements We thank the shared task organizers and treebank providers.</S><S sid = NA ssid = NA>For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1111.txt | Citing Article:  P13-2104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>See (Nivre et al, 2007).</S><S sid = NA ssid = NA>1044 task (Nivre et al, 2007), which differed from the 2006 edition by featuring two separate tracks, one in multilingual parsing, and a new track on domain adaptation for dependency parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1111.txt | Citing Article:  W09-1417.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The parse trees were produced by the GDep parser (Sagae and Tsujii, 2007) and supplied by the challenge organisers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then used each of the models to parse the.</S><S sid = NA ssid = NA>We entered a system based on the approach de scribed in this paper in the CoNLL 2007 shared trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1111.txt | Citing Article:  D10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While the TURKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the JULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007), the TOKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The likely reason for this difference is that over 80% of the dependencies in the Turkish data set have the head to the right of 1048 the dependent, while only less than 4% have the head to the left.</S><S sid = NA ssid = NA>For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1111.txt | Citing Article:  D10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>GDep (Sagae and Tsujii, 2007), a native dependency parser.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles</S><S sid = NA ssid = NA>We then show how we extend the deterministic parser into a best first probabilistic parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1111.txt | Citing Article:  P08-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.</S><S sid = NA ssid = NA>In addi tion to deciding the direction of a reduce action, the label of the newly formed dependency arc must also be decided.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1111.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Tsujii (2007) used the co-training technique to improve performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be ob tained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algo rithms are used, although this is not pursued here); and, finally, 4.</S><S sid = NA ssid = NA>We then provide an analysis of the results obtained with our system, and discuss possible improve ments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1111.txt | Citing Article:  D09-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Tsujii (2007) presented an co training approach for dependency parsing adaptation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first describe our approach to multilingual dependency parsing, fol lowed by our approach for domain adaptation.</S><S sid = NA ssid = NA>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1111.txt | Citing Article:  D12-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shift reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>pendency Parsing Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm based on the LR algorithm (Knuth, 1965).</S><S sid = NA ssid = NA>commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D07-1111.txt | Citing Article:  C08-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then show how we extend the deterministic parser into a best first probabilistic parser.</S><S sid = NA ssid = NA>commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D07-1111.txt | Citing Article:  W10-3011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For sentences in the dataset, their dependency structures are extracted using GENIA Dependency parser (Sagae and Tsujii, 2007), and phrase structure using Brown self-trained biomedical parser (McClosky, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>parser domain adaptation using unlabeled data in the target domain.</S><S sid = NA ssid = NA>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D07-1111.txt | Citing Article:  P11-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.</S><S sid = NA ssid = NA>In addi tion to deciding the direction of a reduce action, the label of the newly formed dependency arc must also be decided.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D07-1111.txt | Citing Article:  W11-0314.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Interes tingly, the ensemble used in the multilingual track also produced good results on the development set for the domain adaptation data, without the use of the unlabeled data at all, with a score of 81.9 (al though the ensemble is more expensive to run).</S><S sid = NA ssid = NA>A similar idea that may be more effective, but requires more effort, is to add parsers based on dif ferent approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D07-1111.txt | Citing Article:  W11-0215.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDepbeta2 (Sagae and Tsujii, 2007) parsers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>word to the beginning of every sentence, which is used as the head of every word in the dependency structure that does not have a head in the sentence.</S><S sid = NA ssid = NA>See (Nivre et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D07-1111.txt | Citing Article:  W07-0604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.</S><S sid = NA ssid = NA>pendency Parsing Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm based on the LR algorithm (Knuth, 1965).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D07-1111.txt | Citing Article:  W07-0604.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See Sagae and Tsujii (2007) for more information on the parser.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then show how we extend the deterministic parser into a best first probabilistic parser.</S><S sid = NA ssid = NA>Instead, we use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string.</S> | Discourse Facet:  NA | Annotator: Automatic


