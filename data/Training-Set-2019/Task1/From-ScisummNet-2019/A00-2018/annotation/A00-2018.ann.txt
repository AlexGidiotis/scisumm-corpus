Citance Number: 1 | Reference Article:  A00-2018.txt | Citing Article:  P00-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['59','190'] | Reference Text:  <S sid = 59 ssid = >As it stands, this last equation is pretty much content-free.</S><S sid = 190 ssid = >It is to this project that our future parsing work will be devoted.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A00-2018.txt | Citing Article:  N10-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000).</S> | Reference Offset:  ['30','91'] | Reference Text:  <S sid = 30 ssid = >Thus we would use p(L2 I L1, M, 1, t, h, H).</S><S sid = 91 ssid = >As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A00-2018.txt | Citing Article:  W11-0610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank.</S> | Reference Offset:  ['1','5'] | Reference Text:  <S sid = 1 ssid = >We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid = 5 ssid = >We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A00-2018.txt | Citing Article:  W06-3119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus.</S> | Reference Offset:  ['91','115'] | Reference Text:  <S sid = 91 ssid = >As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid = 115 ssid = >As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A00-2018.txt | Citing Article:  N03-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997).</S> | Reference Offset:  ['124','131'] | Reference Text:  <S sid = 124 ssid = >We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid = 131 ssid = >This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A00-2018.txt | Citing Article:  N06-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article.</S> | Reference Offset:  ['91','92'] | Reference Text:  <S sid = 91 ssid = >As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid = 92 ssid = >For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A00-2018.txt | Citing Article:  C04-1180.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003).</S> | Reference Offset:  ['176','178'] | Reference Text:  <S sid = 176 ssid = >That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S><S sid = 178 ssid = >The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A00-2018.txt | Citing Article:  W05-0638.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000).</S> | Reference Offset:  ['107','175'] | Reference Text:  <S sid = 107 ssid = >The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid = 175 ssid = >This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A00-2018.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis.</S> | Reference Offset:  ['88','91'] | Reference Text:  <S sid = 88 ssid = >While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.</S><S sid = 91 ssid = >As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A00-2018.txt | Citing Article:  P05-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus.</S> | Reference Offset:  ['1','5'] | Reference Text:  <S sid = 1 ssid = >We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid = 5 ssid = >We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows.</S> | Reference Offset:  ['19','102'] | Reference Text:  <S sid = 19 ssid = >The method we use follows that of [10].</S><S sid = 102 ssid = >Performance on the test corpus is measured using the standard measures from [5,9,10,17].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser.</S> | Reference Offset:  ['18','19'] | Reference Text:  <S sid = 18 ssid = >The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].</S><S sid = 19 ssid = >The method we use follows that of [10].</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically.</S> | Reference Offset:  ['155','174'] | Reference Text:  <S sid = 155 ssid = >For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S><S sid = 174 ssid = >We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task.</S> | Reference Offset:  ['177','181'] | Reference Text:  <S sid = 177 ssid = >The results reported here disprove this conjecture.</S><S sid = 181 ssid = >Neither of these results were anticipated at the start of this research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >First, because of the different definition of a correctly identified constituent in the parser's output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000)).</S> | Reference Offset:  ['126','177'] | Reference Text:  <S sid = 126 ssid = >This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S><S sid = 177 ssid = >The results reported here disprove this conjecture.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A00-2018.txt | Citing Article:  P04-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000).</S> | Reference Offset:  ['51','169'] | Reference Text:  <S sid = 51 ssid = >Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S><S sid = 169 ssid = >Up to this point all the models considered in this section are tree-bank grammar models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A00-2018.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents.</S> | Reference Offset:  ['98','171'] | Reference Text:  <S sid = 98 ssid = >L and R are conditioned on three previous labels so we are using a third-order Markov grammar.</S><S sid = 171 ssid = >As already noted, our best model uses a Markov-grammar approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  A00-2018.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)).</S> | Reference Offset:  ['81','144'] | Reference Text:  <S sid = 81 ssid = >Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.</S><S sid = 144 ssid = >This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  A00-2018.txt | Citing Article:  H05-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions.</S> | Reference Offset:  ['1','152'] | Reference Text:  <S sid = 1 ssid = >We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid = 152 ssid = >So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  A00-2018.txt | Citing Article:  P04-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size.</S> | Reference Offset:  ['165','167'] | Reference Text:  <S sid = 165 ssid = >Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid = 167 ssid = >Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.</S> | Discourse Facet:  NA | Annotator: Automatic


