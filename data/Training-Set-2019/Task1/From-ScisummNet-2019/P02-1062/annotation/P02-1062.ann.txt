Citance Number: 1 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The previous tag, and the previous two tags (bigram and trigram features).</S><S sid = NA ssid = NA>We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction): The word being tagged, the previous word, and the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S><S sid = NA ssid = NA>We applied the voted perceptron and boosting algorithms to the data described in section 2.3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.</S><S sid = NA ssid = NA>This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P02-1062.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S><S sid = NA ssid = NA>For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P02-1062.txt | Citing Article:  P13-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S><S sid = NA ssid = NA>For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S><S sid = NA ssid = NA>The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Freund & Schapire 1999) describe a refinement of the perceptron, the voted perceptron.</S><S sid = NA ssid = NA>The second approach uses the voted perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P02-1062.txt | Citing Article:  I08-4025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The boosting algorithm chooses the feature/update pair which is optimal in terms of minimizing the loss function, i.e., and then makes the update .</S><S sid = NA ssid = NA>This will generate a feature string for each of the entities in a candidate, this time using the values rather than .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P02-1062.txt | Citing Article:  N10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We will use “feature templates” to describe the features that we used.</S><S sid = NA ssid = NA>We define We assume a set of additional features, for .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P02-1062.txt | Citing Article:  N10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S><S sid = NA ssid = NA>For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P02-1062.txt | Citing Article:  W03-0435.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach has been used earlier by (Collins, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).</S><S sid = NA ssid = NA>The second approach uses the voted perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P02-1062.txt | Citing Article:  P09-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We applied the voted perceptron and boosting algorithms to the data described in section 2.3.</S><S sid = NA ssid = NA>Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P02-1062.txt | Citing Article:  D08-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The detailed algorithm can be found in (Collins, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000).</S><S sid = NA ssid = NA>See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P02-1062.txt | Citing Article:  W06-3607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus the maximumentropy tagger we used represents a serious baseline for the task.</S><S sid = NA ssid = NA>As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P02-1062.txt | Citing Article:  N09-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S><S sid = NA ssid = NA>For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P02-1062.txt | Citing Article:  N09-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.</S><S sid = NA ssid = NA>For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins (2002) includes a number of interesting contextual predicates for NER.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise.</S><S sid = NA ssid = NA>Conceptually, the candidate is represented by a large number of features for where is the number of distinct feature strings in training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise.</S><S sid = NA ssid = NA>We take the entity to span words inclusive in the candidate. is seen from words to inclusive in a segmentation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P02-1062.txt | Citing Article:  W03-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.</S><S sid = NA ssid = NA>(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P02-1062.txt | Citing Article:  D11-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each character in the word is mapped to its , but repeated consecutive character types are not repeated in the mapped string.</S><S sid = NA ssid = NA>The word with each character mapped to its type, but repeated consecutive character types are not repeated in the mapped string.</S> | Discourse Facet:  NA | Annotator: Automatic


