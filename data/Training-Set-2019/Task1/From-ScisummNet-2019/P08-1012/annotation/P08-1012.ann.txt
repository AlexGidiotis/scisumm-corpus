Citance Number: 1 | Reference Article:  P08-1012.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models.</S><S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-1012.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Before the training starts, we apply the non-compositional constraints over the pruned bitext space to further constrain the space of phrase pairs.</S><S sid = NA ssid = NA>On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-1012.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our pruning differs from Zhang and Gildea (2005) in two major ways.</S><S sid = NA ssid = NA>There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-1012.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The sole difference between EM and VB with a sparse prior Î± is that the raw fractional counts c are replaced by exp(Ïˆ(c + Î±)), an operation that resembles smoothing.</S><S sid = NA ssid = NA>For small values of Î± the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more likely ones.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-1012.txt | Citing Article:  P09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases.</S><S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-1012.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time.</S><S sid = NA ssid = NA>The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-1012.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The last author was supported by NSF IIS-0546554.</S><S sid = NA ssid = NA>Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-1012.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The last author was supported by NSF IIS-0546554.</S><S sid = NA ssid = NA>Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-1012.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used a variant of the phrasal ITG described by Zhang et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S><S sid = NA ssid = NA>In the end, a Viterbi pass for the phrasal ITG is executed to produce the non-compositional phrasal alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-1012.txt | Citing Article:  N10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004).</S><S sid = NA ssid = NA>From this alignment, phrase pairs are extracted in the usual manner, and a phrase-based translation system is trained.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-1012.txt | Citing Article:  D09-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First of all, we do not have to run a GIZA++ aligner.</S><S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-1012.txt | Citing Article:  P10-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007).</S><S sid = NA ssid = NA>Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-1012.txt | Citing Article:  N12-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>C is our unique pre-terminal for generating terminal multi-word pairs: We parameterize our probabilistic model in the manner of a PCFG: we associate a multinomial distribution with each nonterminal, where each outcome in this distribution corresponds to an expansion of that nonterminal.</S><S sid = NA ssid = NA>Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-1012.txt | Citing Article:  D09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S><S sid = NA ssid = NA>500 Chinese-English pairs from this set were manually aligned and used as a gold standard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P08-1012.txt | Citing Article:  D09-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As pointed out by Johnson (2007), in effect this expression adds to c a small value that asymptotically approaches Î± â€” 0.5 as c approaches oc, and 0 as c approaches 0.</S><S sid = NA ssid = NA>Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P08-1012.txt | Citing Article:  W10-2915.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method.</S><S sid = NA ssid = NA>These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P08-1012.txt | Citing Article:  W10-2915.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.</S><S sid = NA ssid = NA>We chose Variational Bayes, for its procedural similarity to EM and ease of implementation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P08-1012.txt | Citing Article:  C10-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing</S><S sid = NA ssid = NA>We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P08-1012.txt | Citing Article:  P12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).</S><S sid = NA ssid = NA>This linear time algorithm allows us to compute span pruning in O(n3) time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P08-1012.txt | Citing Article:  P11-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior.</S><S sid = NA ssid = NA>Second, Dirichlet distributions with small, non-zero parameters place more probability mass on multinomials on the edges or faces of the probability simplex, distributions with fewer non-zero parameters.</S> | Discourse Facet:  NA | Annotator: Automatic


