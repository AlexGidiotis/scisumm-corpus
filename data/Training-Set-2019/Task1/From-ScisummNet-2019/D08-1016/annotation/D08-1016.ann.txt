Citance Number: 1 | Reference Article:  D08-1016.txt | Citing Article:  P14-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1016.txt | Citing Article:  P14-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function.</S><S sid = NA ssid = NA>TREE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>(The method is approximate because a first-order parser must equally penalize all parses containing e', even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If we had simply replaced the global TRIGRAM factor with its subfactors in the full factor graph, we would have had to resort to Generalized BP (Yedidia et al., 2004) to obtain the same exact results.</S><S sid = NA ssid = NA>For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>A family of hard binary constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Fig.</S><S sid = NA ssid = NA>It is convenient to visualize an undirected factor graph (Fig.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A hard global constraint on all the Lij variables at once.</S><S sid = NA ssid = NA>Given the BP architecture, do we even need the hard TREE constraint?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Fig.</S><S sid = NA ssid = NA>It is convenient to visualize an undirected factor graph (Fig.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But how about global factors?</S><S sid = NA ssid = NA>Finally, we can take advantage of improvements to BP proposed in the context of other applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1016.txt | Citing Article:  D10-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.</S><S sid = NA ssid = NA>Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1016.txt | Citing Article:  D12-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.</S><S sid = NA ssid = NA>Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1016.txt | Citing Article:  D12-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We will give specialized algorithms for handling these summations more efficiently.</S><S sid = NA ssid = NA>For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1016.txt | Citing Article:  D12-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>LINK.</S><S sid = NA ssid = NA>Let F be a factor and V be one of its neighboring variables.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1016.txt | Citing Article:  D11-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>Our experiments use the same features as McDonald et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1016.txt | Citing Article:  P13-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.</S><S sid = NA ssid = NA>Our experiments use the same features as McDonald et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1016.txt | Citing Article:  D12-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency Parsing by Belief Propagation</S><S sid = NA ssid = NA>Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1016.txt | Citing Article:  D12-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We employ stochastic gradient descent (Bottou, 2003), since this does not require us to compute the objective function itself but only to (approximately) estimate its gradient as explained above.</S><S sid = NA ssid = NA>(Parenthetical numbers show that the harm is compounded if the weaker constraints are used in training as well; even though this matches training to test conditions, it may suffer more from BP’s approximate gradients.)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1016.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>More complex models would widen BP’s advantage.</S><S sid = NA ssid = NA>All results use 5 iterations of BP.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1016.txt | Citing Article:  P11-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All results use 5 iterations of BP.</S><S sid = NA ssid = NA>17But taking it = 1 gives the same results, up to a constant.</S> | Discourse Facet:  NA | Annotator: Automatic


