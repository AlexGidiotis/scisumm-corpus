Citance Number: 1 | Reference Article:  A88-1019.txt | Citing Article:  H89-2012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Consider the following three examples where pronunciation depends on part of speech.</S><S sid = NA ssid = NA>For example, Fidditch has the following lexical disambiguation rule: which says that a preposition is more likely than a noun before a noun phrase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A88-1019.txt | Citing Article:  D12-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The proposed method makes use of a table that givvs the probabilities of an open and close bracket between all pairs of parts of speech.</S><S sid = NA ssid = NA>Recall that precedence parsing makes use of a table that says whether to insert an open or close bracket between any two categories (terminal or nonterminal).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A88-1019.txt | Citing Article:  P92-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A program has been written which tags each word in an input sentence with the most likely part of speech.</S><S sid = NA ssid = NA>A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A88-1019.txt | Citing Article:  W01-0712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first work on this topic was done back in the eighties (Church, 1988).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unfortunately, this is unlikely to work.</S><S sid = NA ssid = NA>The present work developed independently from the LOB project.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A88-1019.txt | Citing Article:  W93-0306.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Church's Parts of speech [Church, 1988] performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The stochastic parser is given a sequence of parts of speech as input and is asked to insert brackets corresponding to the beginning and end of noun phrases.</S><S sid = NA ssid = NA>Similar stochastic methods have been applied to locate simple noun phrases with very high accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A88-1019.txt | Citing Article:  W93-0306.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The appendix in [Church, 1988] lists the analysis of a small text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A small sample of the output is given in the appendix.</S><S sid = NA ssid = NA>A small 400 word sample is presented in the Appendix, and is judged to be 99.5% correct.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A88-1019.txt | Citing Article:  E95-1003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is quite feasible using statistical taggers like those of Garside (1987), Church (1988) or Foster (1991) which achieve performance upwards of 97% on unrestricted text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text</S><S sid = NA ssid = NA>Statistical ngram models were quite popular in the 1950s, and have been regaining popularity over the past few years.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A88-1019.txt | Citing Article:  P03-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The shallow parser constructs Verb Groups (VGs) and basic Noun Phrases (NPs), also called BaseNPs [Church 1988].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The table says, for example, that there is no chance of starting a noun phrases after an article (all five entries on the AT row are 0) and that there is a large probability of starting a noun phrase between a verb and an noun (the entry in These probabilities were estimated from about 40,000 words (11,000 noun phrases) of training material selected from the Brown Corpus.</S><S sid = NA ssid = NA>If the parser is going to accept noun phrases of the form: Similarly, the parser probably also has to accept &quot;bird&quot; as an intransitive verb, since there is nothing syntactically wrong with: These part of speech assignments aren't wrong; they are just extremely improbable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A88-1019.txt | Citing Article:  A00-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most lexical disambiguation rules in Fidditch can be reformulated in terms of bigram and trigram statistics in this way.</S><S sid = NA ssid = NA>One might have thought that ngram models weren't adequate for the task since it is wellknown that they are inadequate for determining grammaticality: &quot;We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A88-1019.txt | Citing Article:  P07-2053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As we said at the out 211 set, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Consider the following pair described in [Marcus]: where it appears that the parser needs to look past an arbitrarily long noun phrase in order to correctly analyze &quot;have,&quot; which could be either a tenseless main verb (imperative) or a tensed auxiliary verb (question).</S><S sid = NA ssid = NA>Unfortunately, this is unlikely to work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A88-1019.txt | Citing Article:  P98-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Hajji, Hladk~, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is considerably smaller than the contextual entropy, the conditional entropy of the part of speech given the next two parts of speech.</S><S sid = NA ssid = NA>The proposed stochastic approach is largely compatible with this; the proposed approach 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A88-1019.txt | Citing Article:  N03-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left ,e.g., Church (1988)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is considerably smaller than the contextual entropy, the conditional entropy of the part of speech given the next two parts of speech.</S><S sid = NA ssid = NA>In the case of the Brown Tagged Corpus, the lexical entropy, the conditional entropy of the part of speech given the word is about 0.25 bits per part of speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A88-1019.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 2: Change in the polarity of the sentences citing Church (1988) paper how the way a published work is perceived by the research community over time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Perhaps the most important application of tagging programs is as a tool for future research.</S><S sid = NA ssid = NA>Introductory texts are full of ambiguous sentences such as where no amount of syntactic parsing will help.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A88-1019.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 2 shows the result of this analysis when applied to the work of Kenneth Church (1988) on part-of-speech tagging.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similar stochastic methods have been applied to locate simple noun phrases with very high accuracy.</S><S sid = NA ssid = NA>Unfortunately, this is unlikely to work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A88-1019.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, the analysis illustrated in Figure 2 shows that the work of Ken Church (1988) on part-of-speech tagging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unfortunately, this is unlikely to work.</S><S sid = NA ssid = NA>Consider, for example, the input sequence: NN VB.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A88-1019.txt | Citing Article:  W97-0201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similar stochastic methods have been applied to locate simple noun phrases with very high accuracy.</S><S sid = NA ssid = NA>Introductory texts are full of ambiguous sentences such as where no amount of syntactic parsing will help.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A88-1019.txt | Citing Article:  A94-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The program uses a linear time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word j), and (b) contextual probabilities (probability of observing part of speech i given k previous parts of speech).</S><S sid = NA ssid = NA>Most lexical disambiguation rules in Fidditch can be reformulated in terms of bigram and trigram statistics in this way.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  A88-1019.txt | Citing Article:  A94-1006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the case of the Brown Tagged Corpus, the lexical entropy, the conditional entropy of the part of speech given the word is about 0.25 bits per part of speech.</S><S sid = NA ssid = NA>Consider once again the sentence, &quot;I see a bird.&quot; The problem is to find an assignment of parts of speech to words that optimizes both lexical and contextual probabilities, both of which are estimated from the Tagged Brown Corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  A88-1019.txt | Citing Article:  A97-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These are but three of the many constructions which would sound more natural if the synthesizer had access to accurate part of speech information.</S><S sid = NA ssid = NA>Furthermore, the particular subclass of such processes that produce norder statistical approximations to English do not come closer, with increasing n, to matching the output of an English grammar.&quot; [Chomsky, p. 113] Chomslcy's conclusion was based on the observation that constructions such as: have long distance dependencies that span across any fixed length window n. Thus, ngram models are clearly inadequate for many natural language applications.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  A88-1019.txt | Citing Article:  P94-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The algorithm runs in lockstep with a part-of-speech tagger (Church, 1988), which is used for deciding possible word replacements.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This entropy is estimated to be about 2 bits per part of speech. assumes that it is almost always sufficient to assign each word a unique &quot;best&quot; part of speech (and this can be accomplished with a very efficient linear time dynamic programming algorithm).</S><S sid = NA ssid = NA>The program uses a linear time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word j), and (b) contextual probabilities (probability of observing part of speech i given k previous parts of speech).</S> | Discourse Facet:  NA | Annotator: Automatic


