Citance Number: 1 | Reference Article:  P08-2007.txt | Citing Article:  D08-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem.</S><S sid = NA ssid = NA>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P08-2007.txt | Citing Article:  P09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given a weighted sentence pair, high scoring phrases are linked together greedily to reach an initial alignment.</S><S sid = NA ssid = NA>Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P08-2007.txt | Citing Article:  D10-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Complexity of Phrase Alignment Problems</S><S sid = NA ssid = NA>Marcu and Wong (2002) describes an approximation to O.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P08-2007.txt | Citing Article:  P09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.</S><S sid = NA ssid = NA>In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P08-2007.txt | Citing Article:  W09-1804.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP).</S><S sid = NA ssid = NA>Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P08-2007.txt | Citing Article:  W10-2913.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP).</S><S sid = NA ssid = NA>On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P08-2007.txt | Citing Article:  D09-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first define the notion of a partition: UiSi = T means Si are pairwise disjoint and cover T. Then, we can formally define the set of bijective phrase alignments: Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al.</S><S sid = NA ssid = NA>DeNero et al. (2006) instead proposes an exponential-time dynamic program to systematically explore A, which can in principle solve either O or £.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P08-2007.txt | Citing Article:  D08-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.</S><S sid = NA ssid = NA>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P08-2007.txt | Citing Article:  D08-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.</S><S sid = NA ssid = NA>In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P08-2007.txt | Citing Article:  P11-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem.</S><S sid = NA ssid = NA>Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P08-2007.txt | Citing Article:  P10-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, they both require small data sets due to computational expense.</S><S sid = NA ssid = NA>Let a weighted sentence pair additionally include a real-valued function 0 : {eij}x{fkl} —* R, which scores links.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P08-2007.txt | Citing Article:  P11-2044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first define the notion of a partition: UiSi = T means Si are pairwise disjoint and cover T. Then, we can formally define the set of bijective phrase alignments: Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al.</S><S sid = NA ssid = NA>In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P08-2007.txt | Citing Article:  P11-2044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.</S><S sid = NA ssid = NA>Sentences per hour on a four-core server Frequency of optimal solutions found Frequency of e-optimal solutions found Using an off-the-shelf ILP solver,4 we were able to quickly and reliably find the globally optimal phrase alignment under 0(eij, fkl) derived from the Moses pipeline (Koehn et al., 2007).5 Table 1 shows that finding the optimal phrase alignment is accurate and efficient.6 Hence, this simple search technique effectively addresses the intractability challenges inherent in evaluating new phrase alignment ideas.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P08-2007.txt | Citing Article:  W10-3813.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited.</S><S sid = NA ssid = NA>For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs.</S> | Discourse Facet:  NA | Annotator: Automatic


