Citance Number: 1 | Reference Article:  D07-1104.txt | Citing Article:  D07-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Due to space constraints, details and proof of correctness are available in Lopez (2007a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Due to space constraints, details and proof of correctness are available in Lopez (2007a).</S><S sid = NA ssid = NA>8 Searching for a?b is potentially very expensive, so we put all available information to work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1104.txt | Citing Article:  D07-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, in machine translation most features can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, in machine translation most fea tures can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b).</S><S sid = NA ssid = NA>Recently, Lopez and Resnik (2006) showed that most of the features used in standard phrase-based models do not help very much.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1104.txt | Citing Article:  P14-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We built grammars using its implementation of the suffix array extraction method described in Lopez (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>using the phrase extraction method of Koehn et al (2003).</S><S sid = NA ssid = NA>of the sentence in the suffix array.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1104.txt | Citing Article:  W08-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al, 2003) to obtain word alignments, a translation model, language models, and the optimal weights for combining these mod els, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To generate alignments,we used GIZA++ (Och and Ney, 2003).</S><S sid = NA ssid = NA>We believe that the latter approach has several important applications (?7).So far, these techniques have focused on phrase based models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1104.txt | Citing Article:  E09-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lopez (2007) extracts rules on-the-fly from the training bi text during decoding, searching efficiently for rule patterns using suffix arrays.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On-the-fly lookup using suffix arrays involves an added complication when the rules are in form uXv or uXvXw.</S><S sid = NA ssid = NA>Load the source training text F , the suffix array.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1104.txt | Citing Article:  P10-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Joshua (Li et al, 2009) is an implementation of Hiero (Chiang, 2007) using a suffix-array-based grammar extraction approach (Lopez, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>using the phrase extraction method of Koehn et al (2003).</S><S sid = NA ssid = NA>Some recent models permit discontiguous phrases (Chiang, 2007; Quirket al, 2005; Simard et al, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1104.txt | Citing Article:  W09-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Load the source training text F , the suffix array.</S><S sid = NA ssid = NA>of the sentence in the suffix array.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1104.txt | Citing Article:  W09-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this system, we use the GIZA++toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To generate alignments,we used GIZA++ (Och and Ney, 2003).</S><S sid = NA ssid = NA>We believe that the latter approach has several important applications (?7).So far, these techniques have focused on phrase based models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1104.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To generate alignments,we used GIZA++ (Och and Ney, 2003).</S><S sid = NA ssid = NA>A major difference between contiguous phrase based models and hierarchical phrase-based models is the number of rules that potentially apply to an input sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D07-1104.txt | Citing Article:  W10-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Hierarchical Phrase-Based Translation with Suffix Arrays</S><S sid = NA ssid = NA>We consider the hierarchical translation model ofChiang (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D07-1104.txt | Citing Article:  W10-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Besides storing the whole grammar locally in memory, other approaches have been developed, such as suffix arrays, which lookup and extract rules on the fly from the phrase table (Lopez, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In phrase-based models, this prob lem can be addressed by storing the training data in memory and using a suffix array asan efficient index to quickly lookup and extract rules on the fly.</S><S sid = NA ssid = NA>On-the-fly lookup using suffix arrays involves an added complication when the rules are in form uXv or uXvXw.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D07-1104.txt | Citing Article:  P09-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och,2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formally, this model is a syn chronous context-free grammar.</S><S sid = NA ssid = NA>We consider the hierarchical translation model ofChiang (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D07-1104.txt | Citing Article:  N09-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In phrase-based models, this prob lem can be addressed by storing the training data in memory and using a suffix array asan efficient index to quickly lookup and extract rules on the fly.</S><S sid = NA ssid = NA>7 This approach requires a separate inverted index for each n, up to the maximum n used by the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Lopez, 2007) proposed an extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, it cannot be used for discontiguous substrings.</S><S sid = NA ssid = NA>3 For the purposes of this paper, we adhere to therestrictions described by Chiang (2007) for rules ex tracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>984</S><S sid = NA ssid = NA>Theoreti cally, the worst case for this algorithm occurs when all elements of both sets resolve to the same hash bucket, and we must compare all elements of one set with all elements of the other set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The basis of the method in (Lopez, 2007) is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, it cannot be used for discontiguous substrings.</S><S sid = NA ssid = NA>Binary search enables fast lookup of contiguous substrings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>There is also an exponential number of discontinuous substrings, but (Lopez, 2007) only consider substrings of bounded size, limiting this problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, it cannot be used for discontiguous substrings.</S><S sid = NA ssid = NA>Binary search enables fast lookup of contiguous substrings.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This hypergraph will not only fit the same role as the Prefix Tree of (Lopez, 2007), but also will allow us to easily implement different search strategies for flexible search (section 6).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5.4.3 Prefix Trees and Suffix Links Our search optimizations are easily captured in a prefix tree data structure augmented with suffix links.Formally, a prefix tree is an unminimized determin istic finite-state automaton that recognizes all of thepatterns in some set.</S><S sid = NA ssid = NA>Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This allows in turn to compute by intersection the occurrences of discontinuous treelets, much like what is done in (Lopez, 2007) for discontinuous strings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5.2 Fast Intersection.</S><S sid = NA ssid = NA>This results in a superlinear algorithm for intersection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D07-1104.txt | Citing Article:  D11-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In practice, the intersection operation will be implemented using merge and binary merge algorithms (Baeza-Yates and Salinger, 2005), following (Lopez, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005).</S><S sid = NA ssid = NA>For collocations of frequent and rare patterns, we use a fast set intersection method for sorted sets called double binary search (Baeza-Yates, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


