Citance Number: 1 | Reference Article:  E06-1042.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.</S><S sid = NA ssid = NA>Dictionarybased systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1042.txt | Citing Article:  W07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Literal precision is defined as (correct literals in literal cluster / size of literal cluster).</S><S sid = NA ssid = NA>If there are no literals, literal recall is 100%; literal precision is 100% if there are no nonliterals in the literal cluster and 0% otherwise.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.</S><S sid = NA ssid = NA>Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, TroFi makes use of an existing similarity-based word-sense disambiguation algorithm developed by (Karov & Edelman, 1998), henceforth KE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.</S><S sid = NA ssid = NA>If SuperTags are used, we also move or remove feature sets whose SuperTag trigram indicates phrasal verbs (verb-particle expressions).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this experiment alone, we get an average f-score of 46.3% for the sum of similarities results – a 9.4% improvement over the high similarity results (36.9%) and a 16.9% improvement over the baseline (29.4%).</S><S sid = NA ssid = NA>We calculated a second baseline using a simple attraction algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.</S><S sid = NA ssid = NA>The target verbs may not, and typically do not, appear in the feedback sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Literal recall is defined as (correct literals in literal cluster /total correct literals).</S><S sid = NA ssid = NA>Literal precision is defined as (correct literals in literal cluster / size of literal cluster).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Annotations are from testing or from active learning during example-base construction.</S><S sid = NA ssid = NA>It is important to note that in building the example base, we used TroFi with an Active Learning component (see (Birke, 2005)) which improved our average f-score from 53.8% to 64.9% on the original 25 target words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>NA indicates scores that were not calculated by Birke and Sarkar (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We calculated two baselines for each word.</S><S sid = NA ssid = NA>We calculated a second baseline using a simple attraction algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1042.txt | Citing Article:  D11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.</S><S sid = NA ssid = NA>The target verbs may not, and typically do not, appear in the feedback sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1042.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.</S><S sid = NA ssid = NA>Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, TroFi makes use of an existing similarity-based word-sense disambiguation algorithm developed by (Karov & Edelman, 1998), henceforth KE.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1042.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is helpful in cases where the same set of features can be used as part of both literal and nonliteral expressions.</S><S sid = NA ssid = NA>Since, in addition, the feedback sets are collected automatically, they are very noisy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1042.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) also used WSD.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is important to note that in building the example base, we used TroFi with an Active Learning component (see (Birke, 2005)) which improved our average f-score from 53.8% to 64.9% on the original 25 target words.</S><S sid = NA ssid = NA>The reason is that the DoKMIE are unlikely to list all possible instances of nonliteral language and because knowing that an expression can be used nonliterally does not mean that you can tell when it is being used nonliterally.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1042.txt | Citing Article:  D08-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) requires WordNet.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When we use WordNet as a source of example sentences, or of seed words for pulling sentences out of the WSJ, for building the literal feedback set, we cannot tell if the WordNet synsets, or the collected feature sets, are actually literal.</S><S sid = NA ssid = NA>Consequently we take them as primary and use them to scrub the WordNet synsets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1042.txt | Citing Article:  E09-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.</S><S sid = NA ssid = NA>We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1042.txt | Citing Article:  C10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language</S><S sid = NA ssid = NA>In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1042.txt | Citing Article:  C10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Nonliteral is then anything that is “not literal”, including most tropes, such as metaphors, idioms, as well phrasal verbs and other anomalous expressions that cannot really be seen as literal.</S><S sid = NA ssid = NA>Note that the KE algorithm concentrates on similarities in the way sentences use the target literal or nonliteral word, not on similarities in the meanings of the sentences themselves.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1042.txt | Citing Article:  C10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004).</S><S sid = NA ssid = NA>The target sets contain from 1 to 115 manually annotated sentences for each verb.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1042.txt | Citing Article:  C10-2078.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.</S><S sid = NA ssid = NA>We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1042.txt | Citing Article:  P10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The TroFi algorithm requires a target set (called original set in (Karov & Edelman, 1998)) – the set of sentences containing the verbs to be classified into literal or nonliteral – and the seed sets: the literal feedback set and the nonliteral feedback set.</S><S sid = NA ssid = NA>If the similarity is above this threshold, we label a target-word sentence as literal or nonliteral.</S> | Discourse Facet:  NA | Annotator: Automatic


