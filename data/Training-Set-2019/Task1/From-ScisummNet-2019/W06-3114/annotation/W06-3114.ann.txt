Citance Number: 1 | Reference Article:  W06-3114.txt | Citing Article:  W06-3120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006).</S> | Reference Offset:  ['35','57'] | Reference Text:  <S sid = 35 ssid = >For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid = 57 ssid = >We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-3114.txt | Citing Article:  D07-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English.</S> | Reference Offset:  ['6','8'] | Reference Text:  <S sid = 6 ssid = >English was again paired with German, French, and Spanish.</S><S sid = 8 ssid = >The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-3114.txt | Citing Article:  C08-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference.</S> | Reference Offset:  ['8','9'] | Reference Text:  <S sid = 8 ssid = >The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid = 9 ssid = >Training and testing is based on the Europarl corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-3114.txt | Citing Article:  W07-0718.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006).</S> | Reference Offset:  ['90','171'] | Reference Text:  <S sid = 90 ssid = >Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S><S sid = 171 ssid = >While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-3114.txt | Citing Article:  P07-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the bitext-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006).</S> | Reference Offset:  ['5','6'] | Reference Text:  <S sid = 5 ssid = >• We evaluated translation from English, in addition to into English.</S><S sid = 6 ssid = >English was again paired with German, French, and Spanish.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-3114.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator.</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-3114.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001).</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-3114.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3).</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-3114.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006).</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-3114.txt | Citing Article:  W07-0738.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','178'] | Reference Text:  <S sid = 44 ssid = >We computed BLEU scores for each submission with a single reference translation.</S><S sid = 178 ssid = >HR0011-06-C-0022.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-3114.txt | Citing Article:  D07-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the same method described in (Koehn and Monz, 2006) to perform the significance test.</S> | Reference Offset:  ['49','52'] | Reference Text:  <S sid = 49 ssid = >Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid = 52 ssid = >Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-3114.txt | Citing Article:  D07-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006).</S> | Reference Offset:  ['123','130'] | Reference Text:  <S sid = 123 ssid = >For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S><S sid = 130 ssid = >This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-3114.txt | Citing Article:  W08-0406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002).</S> | Reference Offset:  ['14','26'] | Reference Text:  <S sid = 14 ssid = >There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid = 26 ssid = >Most of these groups follow a phrase-based statistical approach to machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-3114.txt | Citing Article:  W11-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality.</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-3114.txt | Citing Article:  D07-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006).</S> | Reference Offset:  ['5','126'] | Reference Text:  <S sid = 5 ssid = >• We evaluated translation from English, in addition to into English.</S><S sid = 126 ssid = >The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-3114.txt | Citing Article:  D07-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006).</S> | Reference Offset:  ['20','58'] | Reference Text:  <S sid = 20 ssid = >For statistics on this test set, refer to Figure 1.</S><S sid = 58 ssid = >We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-3114.txt | Citing Article:  P07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006).</S> | Reference Offset:  ['0','90'] | Reference Text:  <S sid = 0 ssid = >Manual And Automatic Evaluation Of Machine Translation Between European Languages</S><S sid = 90 ssid = >Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-3114.txt | Citing Article:  P07-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >described in (Koehn and Monz, 2006).</S> | Reference Offset:  ['49','107'] | Reference Text:  <S sid = 49 ssid = >Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid = 107 ssid = >Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-3114.txt | Citing Article:  E12-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006).</S> | Reference Offset:  ['63','175'] | Reference Text:  <S sid = 63 ssid = >Many human evaluation metrics have been proposed.</S><S sid = 175 ssid = >Replacing this with an ranked evaluation seems to be more suitable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-3114.txt | Citing Article:  W09-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).</S> | Reference Offset:  ['39','140'] | Reference Text:  <S sid = 39 ssid = >However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid = 140 ssid = >We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


