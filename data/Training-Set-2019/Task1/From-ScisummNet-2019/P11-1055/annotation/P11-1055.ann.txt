Citance Number: 1 | Reference Article:  P11-1055.txt | Citing Article:  P14-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Distant supervision is provided by the following constraint: every relation instance r (e 1, e 2) K must be expressed by at least one sentence in S (e 1, e 2), the set of sentences that mention both e 1 and e 2 (Hoffmann et al, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given the set of matches, define Σ to be set of NY Times sentences with two matched phrases, E to be the set of Freebase entities which were mentioned in one or more sentences, Δ to be the set of Freebase facts whose arguments, e1 and e2 were mentioned in a sentence in Σ, and R to be set of relations names used in the facts of Δ.</S><S sid = NA ssid = NA>We use the set of sentence-level features described by Riedel et al. (2010), which were originally developed by Mintz et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-1055.txt | Citing Article:  P14-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We then extract relation instances from each parse and apply the greedy inference algorithm from Hoffmann et al, (2011) to identify the best set of parses that satisfy the distant supervision constraint.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the set of sentence-level features described by Riedel et al. (2010), which were originally developed by Mintz et al.</S><S sid = NA ssid = NA>We apply our model to learn extractors for NY Times text using weak supervision from Freebase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-1055.txt | Citing Article:  D12-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our work is closest to Hoffmann et al 2011). They address the same problem we do (binary relation extraction) with a MIML model, but they make two approximations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In general, the corpuslevel extraction problem is easier, since it need only make aggregate predictions, perhaps using corpuswide statistics.</S><S sid = NA ssid = NA>However, unlike the previous work, we did not make use of any features that explicitly aggregate these properties across multiple mention instances.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-1055.txt | Citing Article:  D12-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, our implementation has several advantages over the original model: (a) we model each relation mention independently, whereas Mintz et al collapsed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al2011)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A relation mention is a sequence of text (including one or more entity mentions) which states that some ground fact r(e) is true.</S><S sid = NA ssid = NA>While the Riedel et al. approach does include a model of which sentences express relations, it makes significant use of aggregate features that are primarily designed to do entity-level relation predictions and has a less detailed model of extractions at the individual sentence level.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-1055.txt | Citing Article:  D12-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hoffmann - This is the "MultiR" model, whic h performed the best in (Hoffmann et al2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used T = 50 iterations, which performed best in development experiments.</S><S sid = NA ssid = NA>The systems include the original results reported by Riedel et al. (2010) as well as our new model (MULTIR).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-1055.txt | Citing Article:  P12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al, 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi instance learning system for relation extraction (see Section 7).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles).</S><S sid = NA ssid = NA>We will make use of the Mintz et al. (2009) sentence-level features in the expeiments, as described in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-1055.txt | Citing Article:  P12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compared the following methods: logistic regression with the labeled data cleaned by the proposed method (PROP), logistic regression with the standard DS labeled data (LR), and MultiR propose din (Hoffmann et al, 2011) as a state-of-the-art multi instance learning system.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiments show improvements for both Multi-instance learning was introduced in order to sentential and aggregate (corpus level) extraction, combat the problem of ambiguously-labeled train- and demonstrate that the approach is computationing data when predicting the activity of differ- ally efficient. ent drugs (Dietterich et al., 1997).</S><S sid = NA ssid = NA>To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision (Riedel et al., 2010), using the same data and features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-1055.txt | Citing Article:  P13-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al (2011), and use their implementation of MULTIR4 with 50 training iterations as our baseline.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We follow the approach of Riedel et al. (2010) for generating weak supervision data, computing features, and evaluating aggregate extraction.</S><S sid = NA ssid = NA>We used T = 50 iterations, which performed best in development experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-1055.txt | Citing Article:  P13-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the same datasets as in Hoffmann et al (2011) and Riedel et al (2010), which include 3-years of New York Times articles aligned with Freebase.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles).</S><S sid = NA ssid = NA>The systems include the original results reported by Riedel et al. (2010) as well as our new model (MULTIR).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-1055.txt | Citing Article:  P13-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 3 shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by Hoffmann et al (2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 4 shows approximate precision / recall curves for MULTIR and SOLOR computed against manually generated sentence labels, as defined in Section 6.3.</S><S sid = NA ssid = NA>Figure 4 shows approximate precision / recall curves for three systems computed with aggregate metrics (Section 6.3) that test how closely the extractions match the facts in Freebase.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-1055.txt | Citing Article:  P13-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al (2011) extracted a relation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then report precision and recall for each system on this set of sampled sentences.</S><S sid = NA ssid = NA>Let SM be the sentences where MULTIR extracted an instance of relation r E R, and let Sr be the sentences that match the arguments of a fact about relation r in A.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-1055.txt | Citing Article:  D12-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See Yao et al (2010) and Hoffmann et al (2011) for examples of such models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the set of sentence-level features described by Riedel et al. (2010), which were originally developed by Mintz et al.</S><S sid = NA ssid = NA>We used the same data sets as Riedel et al. (2010) for weak supervision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P11-1055.txt | Citing Article:  P14-2117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al (2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach is related to the multi-instance learning approach of Riedel et al. (2010), in that both models include sentence-level and aggregate random variables.</S><S sid = NA ssid = NA>To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision (Riedel et al., 2010), using the same data and features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P11-1055.txt | Citing Article:  P14-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By using the contents of a database to heuris- expressed in this material are those of the author(s) tically label a training corpus, we may be able to and do not necessarily reflect the view of the Air 549 Force Research Laboratory (AFRL).</S><S sid = NA ssid = NA>We will make use of the Mintz et al. (2009) sentence-level features in the expeiments, as described in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P11-1055.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This constraint is identical to the multiple deterministic-OR constraint used by Hoffmann et al (2011) to train a sentential relation extractor.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations.</S><S sid = NA ssid = NA>We used the same data sets as Riedel et al. (2010) for weak supervision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P11-1055.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The original formulation of the factors permitted tractable inference (Hoffmann et al 2011), but the Extracts function and the factors preclude efficient inference.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The model is undirected and includes repeated factors for making sentence level predictions as well as globals factors for aggregating these choices.</S><S sid = NA ssid = NA>The systems include the original results reported by Riedel et al. (2010) as well as our new model (MULTIR).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P11-1055.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, we apply EXTRACTS to each parse, then use the greedy approximate inference procedure from Hoffmann et al 2011) for the factors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the set of sentence-level features described by Riedel et al. (2010), which were originally developed by Mintz et al.</S><S sid = NA ssid = NA>Finally, we report running time comparisons.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P11-1055.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare our semantic parser to MULTIR (Hoffmann et al 2011), which is a state-of the-art weakly supervised relation extractor.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The knowledge-based weakly supervised learning problem takes as input (1) E, a training corpus, (2) E, a set of entities mentioned in that corpus, (3) R, a set of relation names, and (4), A, a set of ground facts of relations in R. As output the learner produces an extraction model.</S><S sid = NA ssid = NA>To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision (Riedel et al., 2010), using the same data and features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P11-1055.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Experimental results show that our trained semantic parser extracts binary relations as well asa state-of-the-art weakly supervised relation extractor (Hoffmann et al2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we restrict our attention to binary relations.</S><S sid = NA ssid = NA>The knowledge-based weakly supervised learning problem takes as input (1) E, a training corpus, (2) E, a set of entities mentioned in that corpus, (3) R, a set of relation names, and (4), A, a set of ground facts of relations in R. As output the learner produces an extraction model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P11-1055.txt | Citing Article:  P14-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The MultiR system allows entity tuples to have more than one relations, but still predicts each entity tuple locally (Hoffmann et al, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An entity mention is a contiguous sequence of textual tokens denoting an entity.</S><S sid = NA ssid = NA>We use eZ E E to denote both an entity and its name (i.e., the tokens in its mention).</S> | Discourse Facet:  NA | Annotator: Automatic


