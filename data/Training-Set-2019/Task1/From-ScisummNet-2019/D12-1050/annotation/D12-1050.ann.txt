Citance Number: 1 | Reference Article:  D12-1050.txt | Citing Article:  P13-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Socher et al. (2011a) and Socher et al.</S><S sid = NA ssid = NA>Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D12-1050.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Blacoe and Lapata (2012) compare count and predict representations as input to composition functions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.</S><S sid = NA ssid = NA>These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D12-1050.txt | Citing Article:  P14-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Socher et al. (2011a) and Socher et al.</S><S sid = NA ssid = NA>Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D12-1050.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 3 summarizes the performance of the various models on the phrase similarity dataset.</S><S sid = NA ssid = NA>A Comparison of Vector-based Representations for Semantic Composition</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D12-1050.txt | Citing Article:  P13-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We acknowledge the support of EPSRC through project grant EP/I032916/1.</S><S sid = NA ssid = NA>Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): where W ∈ Rd×|D |is a matrix of parameters to be learned.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D12-1050.txt | Citing Article:  P14-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap.</S><S sid = NA ssid = NA>For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (“con”) or subtraction (“sub”); (b) a vector encoding which words appear therein (“enc”); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D12-1050.txt | Citing Article:  P14-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Socher et al. (2011a) and Socher et al.</S><S sid = NA ssid = NA>This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).</S> | Discourse Facet:  NA | Annotator: Automatic


