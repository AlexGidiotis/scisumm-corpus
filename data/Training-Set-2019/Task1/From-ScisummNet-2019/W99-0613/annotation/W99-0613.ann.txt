Citance Number: 1 | Reference Article:  W99-0613.txt | Citing Article:  N01-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999).</S> | Reference Offset:  ['0','28'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Models For Named Entity Classification</S><S sid = 28 ssid = >(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W99-0613.txt | Citing Article:  N01-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting).</S> | Reference Offset:  ['174','247'] | Reference Text:  <S sid = 174 ssid = >The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.</S><S sid = 247 ssid = >N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W99-0613.txt | Citing Article:  W04-3234.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['127','256'] | Reference Text:  <S sid = 127 ssid = >The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid = 256 ssid = >The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W99-0613.txt | Citing Article:  W03-1509.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on.</S> | Reference Offset:  ['16','134'] | Reference Text:  <S sid = 16 ssid = >Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid = 134 ssid = >This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W99-0613.txt | Citing Article:  C02-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus.</S> | Reference Offset:  ['91','127'] | Reference Text:  <S sid = 91 ssid = >There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S><S sid = 127 ssid = >The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W99-0613.txt | Citing Article:  C02-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify.</S> | Reference Offset:  ['10','77'] | Reference Text:  <S sid = 10 ssid = >The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S><S sid = 77 ssid = >In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W99-0613.txt | Citing Article:  W06-2204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification.</S> | Reference Offset:  ['18','250'] | Reference Text:  <S sid = 18 ssid = >But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S><S sid = 250 ssid = >Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W99-0613.txt | Citing Article:  W09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.</S> | Reference Offset:  ['15','163'] | Reference Text:  <S sid = 15 ssid = >The task can be considered to be one component of the MUC (MUC-6, 1995) named entity task (the other task is that of segmentation, i.e., pulling possible people, places and locations from text before sending them to the classifier).</S><S sid = 163 ssid = >We now describe the CoBoost algorithm for the named entity problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W99-0613.txt | Citing Article:  W03-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['127','256'] | Reference Text:  <S sid = 127 ssid = >The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid = 256 ssid = >The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W99-0613.txt | Citing Article:  W03-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999).</S> | Reference Offset:  ['143','234'] | Reference Text:  <S sid = 143 ssid = >Each xt E 2x is the set of features constituting the ith example.</S><S sid = 234 ssid = >88,962 (spelling,context) pairs were extracted as training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W99-0613.txt | Citing Article:  E09-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention).</S> | Reference Offset:  ['0','16'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Models For Named Entity Classification</S><S sid = 16 ssid = >Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W99-0613.txt | Citing Article:  W10-3504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999).</S> | Reference Offset:  ['97','199'] | Reference Text:  <S sid = 97 ssid = >It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid = 199 ssid = >Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W99-0613.txt | Citing Article:  W07-1712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).</S> | Reference Offset:  ['153','251'] | Reference Text:  <S sid = 153 ssid = >Schapire and Singer show that the training error is bounded above by Thus, in order to greedily minimize an upper bound on training error, on each iteration we should search for the weak hypothesis ht and the weight at that minimize Z.</S><S sid = 251 ssid = >In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W99-0613.txt | Citing Article:  W09-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['127','256'] | Reference Text:  <S sid = 127 ssid = >The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid = 256 ssid = >The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W99-0613.txt | Citing Article:  W09-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998).</S> | Reference Offset:  ['6','27'] | Reference Text:  <S sid = 6 ssid = >The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).</S><S sid = 27 ssid = >The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999).</S> | Reference Offset:  ['3','7'] | Reference Text:  <S sid = 3 ssid = >The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid = 7 ssid = >Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999).</S> | Reference Offset:  ['34','59'] | Reference Text:  <S sid = 34 ssid = >The AdaBoost algorithm was developed for supervised learning.</S><S sid = 59 ssid = >The features are used to represent each example for the learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here.</S> | Reference Offset:  ['8','88'] | Reference Text:  <S sid = 8 ssid = >Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid = 88 ssid = >We can now compare this algorithm to that of (Yarowsky 95).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W99-0613.txt | Citing Article:  P12-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use Collins and Singer (1999) for our exact specification of Yarowsky.</S> | Reference Offset:  ['80','88'] | Reference Text:  <S sid = 80 ssid = >The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.</S><S sid = 88 ssid = >We can now compare this algorithm to that of (Yarowsky 95).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W99-0613.txt | Citing Article:  P12-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This is not clearly specified in Collins and Singer (1999), but is used for DL-CoTrain in the same paper.</S> | Reference Offset:  ['91','127'] | Reference Text:  <S sid = 91 ssid = >There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S><S sid = 127 ssid = >The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


