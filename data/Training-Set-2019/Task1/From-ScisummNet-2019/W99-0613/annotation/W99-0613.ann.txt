Citance Number: 1 | Reference Article:  W99-0613.txt | Citing Article:  N01-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S><S sid = NA ssid = NA>Unsupervised Models For Named Entity Classification</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W99-0613.txt | Citing Article:  N01-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.</S><S sid = NA ssid = NA>N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W99-0613.txt | Citing Article:  W04-3234.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid = NA ssid = NA>The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W99-0613.txt | Citing Article:  W03-1509.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid = NA ssid = NA>This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W99-0613.txt | Citing Article:  C02-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid = NA ssid = NA>There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W99-0613.txt | Citing Article:  C02-1154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1.</S><S sid = NA ssid = NA>The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W99-0613.txt | Citing Article:  W06-2204.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid = NA ssid = NA>But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W99-0613.txt | Citing Article:  W09-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We now describe the CoBoost algorithm for the named entity problem.</S><S sid = NA ssid = NA>The task can be considered to be one component of the MUC (MUC-6, 1995) named entity task (the other task is that of segmentation, i.e., pulling possible people, places and locations from text before sending them to the classifier).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W99-0613.txt | Citing Article:  W03-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid = NA ssid = NA>The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W99-0613.txt | Citing Article:  W03-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>88,962 (spelling,context) pairs were extracted as training data.</S><S sid = NA ssid = NA>Each xt E 2x is the set of features constituting the ith example.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W99-0613.txt | Citing Article:  E09-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised Models For Named Entity Classification</S><S sid = NA ssid = NA>Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W99-0613.txt | Citing Article:  W10-3504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.</S><S sid = NA ssid = NA>It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W99-0613.txt | Citing Article:  W07-1712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Schapire and Singer show that the training error is bounded above by Thus, in order to greedily minimize an upper bound on training error, on each iteration we should search for the weak hypothesis ht and the weight at that minimize Z.</S><S sid = NA ssid = NA>In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W99-0613.txt | Citing Article:  W09-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid = NA ssid = NA>The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W99-0613.txt | Citing Article:  W09-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid = NA ssid = NA>The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S><S sid = NA ssid = NA>The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The AdaBoost algorithm was developed for supervised learning.</S><S sid = NA ssid = NA>The features are used to represent each example for the learning algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W99-0613.txt | Citing Article:  W06-2207.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid = NA ssid = NA>We can now compare this algorithm to that of (Yarowsky 95).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W99-0613.txt | Citing Article:  P12-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use Collins and Singer (1999) for our exact specification of Yarowsky.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.</S><S sid = NA ssid = NA>We can now compare this algorithm to that of (Yarowsky 95).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W99-0613.txt | Citing Article:  P12-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is not clearly specified in Collins and Singer (1999), but is used for DL-CoTrain in the same paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid = NA ssid = NA>There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S> | Discourse Facet:  NA | Annotator: Automatic


