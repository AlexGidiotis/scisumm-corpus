Citance Number: 1 | Reference Article:  W00-1303.txt | Citing Article:  N01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We evaluate the relationship between the beam width and the parsing accuracy.</S><S sid = NA ssid = NA>Table 2 shows the result of passing accuracy under the condition k = 5 (beam width), and d = 3 (dimension of the polynomial functions used for the kernel function).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W00-1303.txt | Citing Article:  N01-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our experiments, the accuracy of 89.09% is achieved using same training data.</S><S sid = NA ssid = NA>The best parsing accuracy is achieved at k = 5 and the best sentence accuracy is achieved at k = 5 and k = 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W00-1303.txt | Citing Article:  P10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Generally, the optimal solution Db„t can be identified by using bottom-up algorithm such as CYK algorithm.</S><S sid = NA ssid = NA>However, the SVMs method achieve a high accuracy not only on the training data but also on the test data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W00-1303.txt | Citing Article:  I05-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The result shows that Japanese dependency analysis can be effectively performed by use of SVMs due to its good generalization and nonoverfitting characteristics.</S><S sid = NA ssid = NA>— 1)} by D, where Dep(i) = j means that the chunk bi depends on (modifies) the chunk bi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W00-1303.txt | Citing Article:  W02-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kudo and Matsumoto (2000) used the sigmoid function to obtain pseudo probabilities in SVMs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We obtain Db„t taking into all the combination of these probabilities.</S><S sid = NA ssid = NA>For the kernel function, we used the polynomial function (9).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W00-1303.txt | Citing Article:  W02-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of feature called dynamic features, which are created dynamically during the parsing process.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We refer to the features that are added incrementally during the parsing process as dynamic features.</S><S sid = NA ssid = NA>This is due to a good characteristic of SVMs to cope with the data sparseness problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W00-1303.txt | Citing Article:  W02-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the kernel function, we used the polynomial function (9).</S><S sid = NA ssid = NA>Among the many kinds of Kernel functions available, we will focus on the d-th polynomial kernel: Use of d-th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d. Using a Kernel function, we can rewrite the decision function as:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W00-1303.txt | Citing Article:  W02-2016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our model outperforms Uchimoto's model as far as the accuracies are compared.</S><S sid = NA ssid = NA>We believe that our model is better than others from the viewpoints of coverage and consistency, since our model learns the combination of features without increasing the computational complexity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W00-1303.txt | Citing Article:  D07-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Haruno et al (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis.</S><S sid = NA ssid = NA>Uchimoto (Uchimoto et al., 1999) and Sekine (Sekine et al., 2000) report that using Kyoto University Corpus for their training and testing, they achieve around 87.2% accuracy by building statistical model based on Maximum Entropy framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W00-1303.txt | Citing Article:  D07-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Therefore, our methods analyze a sentence backwards as in Sekine (2000) and Kudo and Matsumoto (2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sekine suggests an efficient parsing technique for Japanese sentences that parses from the end of a sentence(Sekine et al., 2000).</S><S sid = NA ssid = NA>Sekine (Sekine et al., 2000) gives an interesting report about the relationship between the beam width and the parsing accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W00-1303.txt | Citing Article:  D07-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The result shows that Japanese dependency analysis can be effectively performed by use of SVMs due to its good generalization and nonoverfitting characteristics.</S><S sid = NA ssid = NA>— 1)} by D, where Dep(i) = j means that the chunk bi depends on (modifies) the chunk bi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W00-1303.txt | Citing Article:  C04-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis.</S><S sid = NA ssid = NA>Sekine suggests an efficient parsing technique for Japanese sentences that parses from the end of a sentence(Sekine et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W00-1303.txt | Citing Article:  W06-2920.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kudo and Matsumoto (2000) describe a dependency parser for Japanese and Yamada and Matsumoto (2003) an extension for English.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The parser achieves 86.52% accuracy for test data even with small training data (1172 sentences).</S><S sid = NA ssid = NA>Japanese Dependency Structure Analysis Based On Support Vector Machines</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W00-1303.txt | Citing Article:  C02-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we propose an application of SVMs to Japanese dependency structure analysis.</S><S sid = NA ssid = NA>The result shows that Japanese dependency analysis can be effectively performed by use of SVMs due to its good generalization and nonoverfitting characteristics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W00-1303.txt | Citing Article:  C04-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kudo and Matsumoto (2000) also used the same backward beam search together with SVMs rather than ME.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As we describe later we apply a beam search for parsing, and it is possible to keep several intermediate solutions while suppressing the combinatorial explosion.</S><S sid = NA ssid = NA>For the training data, we used exactly the same data that they used in order to make a fair comparison.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W00-1303.txt | Citing Article:  C04-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The result shows that Japanese dependency analysis can be effectively performed by use of SVMs due to its good generalization and nonoverfitting characteristics.</S><S sid = NA ssid = NA>— 1)} by D, where Dep(i) = j means that the chunk bi depends on (modifies) the chunk bi.</S> | Discourse Facet:  NA | Annotator: Automatic


