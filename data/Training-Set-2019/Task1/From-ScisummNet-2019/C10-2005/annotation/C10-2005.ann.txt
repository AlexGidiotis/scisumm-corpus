Citance Number: 1 | Reference Article:  C10-2005.txt | Citing Article:  D11-1052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For polarity detection, they select the positive examples for the training data from the tweets containing good emoticonsand negative examples from tweets contain ing bad emoticons.</S><S sid = NA ssid = NA>One exception is TwitterSentiment (Go et al., 2009), for instance, which considers tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and builds a classifier using unigrams and bigrams as features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C10-2005.txt | Citing Article:  P11-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.</S><S sid = NA ssid = NA>To illustrate which of the proposed features are more effective for this task, the top-5 features in terms of information gain, based on our trainingdata, are: positive polarity, link, strong subjec tive, upper case and verbs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C10-2005.txt | Citing Article:  W11-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also investigated the influence of the size oftraining data on classification performance.</S><S sid = NA ssid = NA>As we show later, this boosts the classification performance, mainlybecause it removes tweets labeled as subjective by the data sources but are in fact objec tive; 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005).</S><S sid = NA ssid = NA>A variety of features have been exploited on the problem of sentiment detection (Pang and Lee, 2004; Pang et al, 2002; Wiebe et al, 1999; Wiebeand Riloff, 2005; Riloff et al, 2006) including unigrams, bigrams, part-of-speech tags etc. A natural choice would be to use the raw word represen tation (n-grams) as features, since they obtained good results in previous works (Pang and Lee, 2004; Pang et al, 2002) that deal with large texts.However, as we want to perform sentiment detection on very short messages (tweets), this strategy might not be effective, as shown in our ex periments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages.</S><S sid = NA ssid = NA>3.1 Features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Second, wepresent some modifications in the proposed fea tures that are more suitable for this task.</S><S sid = NA ssid = NA>In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>training set, we remove from this set tweets that contain the top-n opinion words in the subjectivity training set, e.g., words as cool, suck, awesome etc. As we show in Section 4, this process is in fact able to remove certain noisy in the training data,leading to a better performing subjectivity classi fier.</S><S sid = NA ssid = NA>However, looking at our training data almost half of the occurrences of this word appears in the positive set and the other half inthe negative set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3.1 Features.</S><S sid = NA ssid = NA>An other common characteristic of some of them isthe use of n-grams as features to create their mod els.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3.1 Features.</S><S sid = NA ssid = NA>An other common characteristic of some of them isthe use of n-grams as features to create their mod els.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C10-2005.txt | Citing Article:  P11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3.1 Features.</S><S sid = NA ssid = NA>Meta-features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C10-2005.txt | Citing Article:  E12-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3.2 Subjectivity Classifier.</S><S sid = NA ssid = NA>Finally, since the data qual ity provided by TwitterSentiment is better than the3For this experiment, we used the TwitterSA(single) con figuration.</S> | Discourse Facet:  NA | Annotator: Automatic


