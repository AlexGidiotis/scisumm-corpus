Citance Number: 1 | Reference Article:  W02-1011.txt | Citing Article:  W03-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup.3 We selected only reviews where the author rating was expressed either with stars or some numerical value (other conventions varied too widely to allow for automatic processing).</S><S sid = NA ssid = NA>Ratings were automatically extracted and converted into one of three categories: positive, negative, or neutral.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-1011.txt | Citing Article:  P14-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our aim in this work was to examine whether it suffices to treat sentiment classification simply as a special case of topic-based categorization (with the two “topics” being positive sentiment and negative sentiment), or whether special sentiment-categorization methods need to be developed.</S><S sid = NA ssid = NA>For the work described in this paper, we concentrated only on discriminating between positive and negative sentiment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-1011.txt | Citing Article:  C10-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al, 2002), e.g., favorable or unfavorable, positive or negative.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our aim in this work was to examine whether it suffices to treat sentiment classification simply as a special case of topic-based categorization (with the two “topics” being positive sentiment and negative sentiment), or whether special sentiment-categorization methods need to be developed.</S><S sid = NA ssid = NA>Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-1011.txt | Citing Article:  P09-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Nigam et al., 1999).</S><S sid = NA ssid = NA>We used documents from the movie-review corpus described in Section 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-1011.txt | Citing Article:  N09-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Automatic identification of subjective content of ten relies on word indicators, such as unigrams (Pang et al, 2002) or predetermined sentiment lexica (Wilson et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Nigam et al., 1999).</S><S sid = NA ssid = NA>Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-1011.txt | Citing Article:  W10-0216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation.</S><S sid = NA ssid = NA>We applied these procedures to uniformlydistributed data, so that the random-choice baseline result would be 50%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-1011.txt | Citing Article:  W11-1717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the purpose of this work, we follow the definition of Pang et al (2002) and Turney (2002) and consider a binary classification task for output labels as positive and negative.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.</S><S sid = NA ssid = NA>For the work described in this paper, we concentrated only on discriminating between positive and negative sentiment.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-1011.txt | Citing Article:  W11-1717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone.</S><S sid = NA ssid = NA>Most previous research on sentiment-based classification has been at least partially knowledge-based.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-1011.txt | Citing Article:  W11-1717.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We choose to use SVM since it performs the best for sentiment classification (Pang et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Nigam et al., 1999).</S><S sid = NA ssid = NA>Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-1011.txt | Citing Article:  W06-1639.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>MaxEnt feature/class functions Fi,c only reflects the presence or absence of a feature, rather than directly incorporating feature frequency.</S><S sid = NA ssid = NA>In order to investigate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting ni(d) to 1 if and only feature fi appears in d, and reran Naive Bayes and SVMlight on these new vectors.11 As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for feature presence, not feature frequency.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-1011.txt | Citing Article:  H05-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).</S><S sid = NA ssid = NA>(Nigam et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-1011.txt | Citing Article:  W12-3712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Each has proven to be effective in previous sentiment analysis studies (Pang et al, 2002), so as this experiment is rooted in sentiment classification, these methods were also assumed to perform well in this cross-discourse setting.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The philosophies behind these three algorithms are quite different, but each has been shown to be effective in previous text categorization studies.</S><S sid = NA ssid = NA>However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-1011.txt | Citing Article:  W12-3712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These results support experiments carried out for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but differs from sentiment classification results from Pang et al (2002) that suggest that term-based models perform better than the frequency-based alternative.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most previous research on sentiment-based classification has been at least partially knowledge-based.</S><S sid = NA ssid = NA>However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-1011.txt | Citing Article:  W12-3712.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pang et al (2002) applied these classifiers to the movie review domain, which produced good results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results produced via machine learning techniques are quite good in comparison to the humangenerated baselines discussed in Section 4.</S><S sid = NA ssid = NA>We used documents from the movie-review corpus described in Section 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-1011.txt | Citing Article:  C10-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation.</S><S sid = NA ssid = NA>We applied these procedures to uniformlydistributed data, so that the random-choice baseline result would be 50%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-1011.txt | Citing Article:  D11-1147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previouswork has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thumbs Up? Sentiment Classification Using Machine Learning Techniques</S><S sid = NA ssid = NA>Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-1011.txt | Citing Article:  W06-1650.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Turney (2002) makes a similar point, noting that for reviews, “the whole is not necessarily the sum of the parts”.)</S><S sid = NA ssid = NA>Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-1011.txt | Citing Article:  C10-2153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al, 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Fi,c is a feature/class function for feature fi and class c, defined as follows:6 class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense.</S><S sid = NA ssid = NA>Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-1011.txt | Citing Article:  P13-2088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We do this because we already have enough training data, so there is no need to resort to cross-validation (Pang et al, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All results reported below, as well as the baseline results from Section 4, are the average three-fold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune).</S><S sid = NA ssid = NA>(Nigam et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-1011.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Applications of text categorization, such as sentiment classification (Pang et al, 2002), are now required to run on multiple languages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sentiment classification would also be helpful in business intelligence applications (e.g.</S><S sid = NA ssid = NA>Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.</S> | Discourse Facet:  NA | Annotator: Automatic


