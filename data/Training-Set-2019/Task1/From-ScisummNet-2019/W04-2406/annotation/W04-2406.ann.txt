Citance Number: 1 | Reference Article:  W04-2406.txt | Citing Article:  N04-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.</S><S sid = NA ssid = NA>In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-2406.txt | Citing Article:  P11-1148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.</S><S sid = NA ssid = NA>Then we describe our approach to the evaluation of unsupervised word sense discrimination.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-2406.txt | Citing Article:  P10-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).</S><S sid = NA ssid = NA>(Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-2406.txt | Citing Article:  P10-4006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We employ two different representations of the context in which a target word occurs.</S><S sid = NA ssid = NA>The second order experiments in this paper use two different types of features, co–occurrences and bigrams, defined as they are in the first order experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-2406.txt | Citing Article:  W11-0317.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When given smaller amounts of data like SENSEVAL-2, second order context vectors and a hybrid clustering method like Repeated Bisections perform better.</S><S sid = NA ssid = NA>This suggests that the second order context vectors (SC) have an advantage over the first order vectors for small training data as is found among the 24 SENSEVAL-2 words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-2406.txt | Citing Article:  W06-2502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A detailed description can be found in Purandare and Pedersen (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>What follows is a concise description of each configuration.</S><S sid = NA ssid = NA>This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-2406.txt | Citing Article:  W06-2502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The method is described in Purandare and Pedersen (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Sch¨utze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty’s method) fares well.</S><S sid = NA ssid = NA>As such this is a hybrid method that combines a hierarchical divisive approach with partitioning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-2406.txt | Citing Article:  W06-2502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The data is particularly challenging for unsupervised algorithms due to the large number of fine grained senses, generally 8 to 12 per word.</S><S sid = NA ssid = NA>This table shows the number of words for which an experiment performed better than the the majority class, broken down by part of speech.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-2406.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.</S><S sid = NA ssid = NA>In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-2406.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).</S><S sid = NA ssid = NA>Table 1 shows that the top 3 experiments for each of the mixed-words are all second order vectors (SC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-2406.txt | Citing Article:  W06-3814.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.</S><S sid = NA ssid = NA>Second order context vectors are designed to identify such relationships, in that exact matching is not required, but rather words that occur in similar contexts will have similar vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-2406.txt | Citing Article:  W06-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.</S><S sid = NA ssid = NA>This paper also proposes and evaluates several extensions to these techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-2406.txt | Citing Article:  W06-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that PB1 and SC1 use co–occurrence features, while PB3 and SC3 rely on bigram features.</S><S sid = NA ssid = NA>Pedersen and Bruce use a small number of local features that include co–occurrence and part of speech information near the target word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-2406.txt | Citing Article:  W06-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).</S><S sid = NA ssid = NA>This paper also proposes and evaluates several extensions to these techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-2406.txt | Citing Article:  W06-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>He selects features based on their frequency counts or log-likelihood ratios in this corpus.</S><S sid = NA ssid = NA>While there has been some previous work in sense discrimination (e.g., (Sch¨utze, 1992), (Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998), (Sch¨utze, 1998), (Fukumoto and Suzuki, 1999)), by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning a meaning to a word from a predefined set of possibilities.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-2406.txt | Citing Article:  W06-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Second order context vectors succeed in such cases because they find indirect second order co– occurrences of feature words and hence describe the context more extensively than the first order representations.</S><S sid = NA ssid = NA>From this, we observe that with both first order and second order context vectors Repeated Bisections is more effective than UPGMA.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-2406.txt | Citing Article:  P11-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.</S><S sid = NA ssid = NA>Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-2406.txt | Citing Article:  W06-1669.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>And when the number of senses is greater than the number of clusters, some senses will not be assigned to any cluster.</S><S sid = NA ssid = NA>The small volume of data combined with large number of possible senses leads to very small set of examples for most of the senses.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-2406.txt | Citing Article:  W06-1669.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These mix-words were created in order to provide data that included both fine grained and coarse grained distinctions.</S><S sid = NA ssid = NA>This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-2406.txt | Citing Article:  W10-4169.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).</S><S sid = NA ssid = NA>Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


