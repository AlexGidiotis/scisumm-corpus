Citance Number: 1 | Reference Article:  D07-1080.txt | Citing Article:  D08-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Indeed, Liang et al (2006) employed an averaged perceptron algorithm in which ? value was always set to one.</S><S sid = NA ssid = NA>Tillmann and Zhang (2006) and Liang et al (2006) solved the problem by introducing a sentence-wise BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D07-1080.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Online Large-Margin Training for Statistical Machine Translation</S><S sid = NA ssid = NA>The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D07-1080.txt | Citing Article:  D12-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>L(e?, e?; et) ?(e?, e?)</S><S sid = NA ssid = NA>) ||2 + ? e?,e? ?(e?, e?)L(e?, e?; et) ? ?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D07-1080.txt | Citing Article:  D08-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>3.2 Sparse Features.</S><S sid = NA ssid = NA>An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al,2006a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D07-1080.txt | Citing Article:  W11-2135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>4.1 Margin Infused Relaxed Algorithm.</S><S sid = NA ssid = NA>The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D07-1080.txt | Citing Article:  W10-1757.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sparse features used in reranking are extracted according to (Watanabe et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al,2006a).</S><S sid = NA ssid = NA>3.2 Sparse Features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D07-1080.txt | Citing Article:  W10-1757.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>and Arabic.</S><S sid = NA ssid = NA>Thedevelopment set comes from the MT2003 Arabic English NIST evaluation test set consisting of 663 sentences in the news domain with four reference translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D07-1080.txt | Citing Article:  W10-1757.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).</S><S sid = NA ssid = NA>MIRA is successfully employed in dependency parsing (McDonald et al, 2005) or the joint-labeling/chunking task (Shimizu and Haas,2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D07-1080.txt | Citing Article:  D12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Online Large-Margin Training for Statistical Machine Translation</S><S sid = NA ssid = NA>Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


