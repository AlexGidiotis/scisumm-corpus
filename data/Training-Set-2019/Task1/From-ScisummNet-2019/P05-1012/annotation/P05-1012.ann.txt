Citance Number: 1 | Reference Article:  P05-1012.txt | Citing Article:  P05-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The recent advances in parsing have achieved parsers with O(n3) time complexity without the grammar constant (McDonald et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, there is a large grammar constant, which is typically in the thousands for treebank parsers.</S><S sid = NA ssid = NA>Yet, they can be parsed in O(n3) time (Eisner, 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1012.txt | Citing Article:  W08-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1012.txt | Citing Article:  C08-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A dependency-based system using MST Parser (McDonald et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Czech parser of Collins et al. (1999) was run on a different data set and most other dependency parsers are evaluated using English.</S><S sid = NA ssid = NA>We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1012.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This may be partially compensated for by including features about the surrounding words (McDonald et al., 2005), but any feature templates which would be identical across the two contexts will be in tension.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005).</S><S sid = NA ssid = NA>Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1012.txt | Citing Article:  P07-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1012.txt | Citing Article:  P07-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1012.txt | Citing Article:  P07-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1012.txt | Citing Article:  P07-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Many of the features above were introduced in McDonald et al (2005a); specifically, the node type, inside, and edge features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Features of the first type look at words that occur between a child and its parent.</S><S sid = NA ssid = NA>These features represent a system of backoff from very specific features over words and partof-speech tags to less sparse features over just partof-speech tags.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1012.txt | Citing Article:  D07-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In turn, those features were inspired by successful previous work in first order dependency parsing (McDonald et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The following work on dependency parsing is most relevant to our research.</S><S sid = NA ssid = NA>We described a successful new method for training dependency parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1012.txt | Citing Article:  P09-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although (McDonald et al, 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This feature took the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child.</S><S sid = NA ssid = NA>These features are added for both the entire words as well as the 5-gram prefix if the word is longer than 5 characters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1012.txt | Citing Article:  W12-3160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1012.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al, 2005a) or local parsing decision scores (Hall et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated (Riezler et al., 2002; Clark and Curran, 2004; Collins and Roark, 2004; Taskar et al., 2004).</S><S sid = NA ssid = NA>A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints (Taskar et al., 2003; Taskar et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1012.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al (2005a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison.</S><S sid = NA ssid = NA>All the experiments presented here use k = 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1012.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1012.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1012.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare conditional training of a non projective edge-factored parsing model to the online MIRA training used by McDonald et al (2005b).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P(y|x) of the training data, performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation and thus may suffer from the label bias problem (Lafferty et al., 2001).</S><S sid = NA ssid = NA>Online Large-Margin Training Of Dependency Parsers</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1012.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins, 1999; Charniak, 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto, 2003).</S><S sid = NA ssid = NA>The best phrase-structure parsing models represent generatively the joint probability P(x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1012.txt | Citing Article:  P13-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used CoNLL 03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al, 1994) converted to dependency trees for DEPAR (McDonald et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We tested our methods experimentally on the English Penn Treebank (Marcus et al., 1993) and on the Czech Prague Dependency Treebank (Hajiˇc, 1998).</S><S sid = NA ssid = NA>A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints (Taskar et al., 2003; Taskar et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1012.txt | Citing Article:  P13-2004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>L2PA is also known as a loss augmented variant of one best MIRA, well-known in DEPAR (McDonald et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is a well known discriminative training trick — using the suggestions of a generative system to influence decisions.</S><S sid = NA ssid = NA>One question that can be asked is how justifiable is the k-best MIRA approximation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1012.txt | Citing Article:  P11-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205456, 0205448, and 0428193.</S><S sid = NA ssid = NA>The more errors a tree has, the farther away its score will be from the score of the correct tree.</S> | Discourse Facet:  NA | Annotator: Automatic


