Citance Number: 1 | Reference Article:  P11-1060.txt | Citing Article:  D11-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning.</S> | Reference Offset:  ['9','165'] | Reference Text:  <S sid = 9 ssid = >As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid = 165 ssid = >These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P11-1060.txt | Citing Article:  P13-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance.</S> | Reference Offset:  ['20','132'] | Reference Text:  <S sid = 20 ssid = >At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S><S sid = 132 ssid = >Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P11-1060.txt | Citing Article:  P13-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments.</S> | Reference Offset:  ['9','20'] | Reference Text:  <S sid = 9 ssid = >As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid = 20 ssid = >At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P11-1060.txt | Citing Article:  P13-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Learning Dependency-Based Compositional Semantics</S><S sid = 21 ssid = >The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P11-1060.txt | Citing Article:  P13-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011).</S> | Reference Offset:  ['34','71'] | Reference Text:  <S sid = 34 ssid = >Figure 2(a) shows an example of a DCS tree.</S><S sid = 71 ssid = >Let z be a DCS tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P11-1060.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','174'] | Reference Text:  <S sid = 54 ssid = >A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid = 174 ssid = >598</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P11-1060.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world.</S> | Reference Offset:  ['7','165'] | Reference Text:  <S sid = 7 ssid = >Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid = 165 ssid = >These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P11-1060.txt | Citing Article:  P13-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011).</S> | Reference Offset:  ['163','165'] | Reference Text:  <S sid = 163 ssid = >Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S><S sid = 165 ssid = >These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P11-1060.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al 2011) or even a binary correct/incorrect signal (Clarke et al 2010).</S> | Reference Offset:  ['9','20'] | Reference Text:  <S sid = 9 ssid = >As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid = 20 ssid = >At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P11-1060.txt | Citing Article:  N12-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form.</S> | Reference Offset:  ['10','115'] | Reference Text:  <S sid = 10 ssid = >However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid = 115 ssid = >After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy pθ(T)(y  |x, z ∈ ˜ZL,θ(T)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P11-1060.txt | Citing Article:  P12-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.</S> | Reference Offset:  ['2','132'] | Reference Text:  <S sid = 2 ssid = >In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S sid = 132 ssid = >Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P11-1060.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011).</S> | Reference Offset:  ['0','25'] | Reference Text:  <S sid = 0 ssid = >Learning Dependency-Based Compositional Semantics</S><S sid = 25 ssid = >We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P11-1060.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1).</S> | Reference Offset:  ['25','94'] | Reference Text:  <S sid = 25 ssid = >We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid = 94 ssid = >We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P11-1060.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries.</S> | Reference Offset:  ['12','167'] | Reference Text:  <S sid = 12 ssid = >We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S sid = 167 ssid = >In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P11-1060.txt | Citing Article:  P14-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable.</S> | Reference Offset:  ['71','84'] | Reference Text:  <S sid = 71 ssid = >Let z be a DCS tree.</S><S sid = 84 ssid = >There are three cases: Extraction (d.ri = E) In the basic version, the denotation of a tree was always the set of consistent values of the root node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P11-1060.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available.</S> | Reference Offset:  ['9','20'] | Reference Text:  <S sid = 9 ssid = >As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid = 20 ssid = >At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P11-1060.txt | Citing Article:  D11-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011).</S> | Reference Offset:  ['7','165'] | Reference Text:  <S sid = 7 ssid = >Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid = 165 ssid = >These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P11-1060.txt | Citing Article:  P13-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model.</S> | Reference Offset:  ['132','138'] | Reference Text:  <S sid = 132 ssid = >Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid = 138 ssid = >Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P11-1060.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011).</S> | Reference Offset:  ['0','159'] | Reference Text:  <S sid = 0 ssid = >Learning Dependency-Based Compositional Semantics</S><S sid = 159 ssid = >The closest work to ours is Clarke on compositional semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P11-1060.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >See Liang et al (2011) for work in representing lambda calculus expressions with trees.</S> | Reference Offset:  ['17','20'] | Reference Text:  <S sid = 17 ssid = >The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S><S sid = 20 ssid = >At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S> | Discourse Facet:  NA | Annotator: Automatic


