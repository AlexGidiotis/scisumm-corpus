Citance Number: 1 | Reference Article:  P00-1041.txt | Citing Article:  W09-2807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.</S><S sid = NA ssid = NA>More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P00-1041.txt | Citing Article:  W04-3216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It does so by building statistical models for content selection and surface realization.</S><S sid = NA ssid = NA>We then tested mixtures of the lexical and POS models, lexical and positional models, and all three models combined together.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P00-1041.txt | Citing Article:  P07-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Headline Generation Based On Statistical Translation</S><S sid = NA ssid = NA>More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P00-1041.txt | Citing Article:  N07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, to simplify parameter estimation for the content selection model, we can assume that the likelihood of a word in the summary is independent of other words in the summary.</S><S sid = NA ssid = NA>Headline Generation Based On Statistical Translation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P00-1041.txt | Citing Article:  N07-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As can be seen in Table 2,7 Although adding the POS information alone does not seem to provide any benefit, positional information does.</S><S sid = NA ssid = NA>In the simplest, zerolevel model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search (Forney, 1973) to efficiently find a near-optimal summary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P00-1041.txt | Citing Article:  P10-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Headline Generation Based On Statistical Translation</S><S sid = NA ssid = NA>When used in combination, each of the additional information sources seems to improve the overall model of summary generation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P00-1041.txt | Citing Article:  P10-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Banko et al (2000) propose a bag-of-words model for headline generation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Headline Generation Based On Statistical Translation</S><S sid = NA ssid = NA>As can be seen, even with such an impoverished language model, the system does quite well: when the generated headlines are four words long almost one in every five has all of its words matched in the article s actual headline.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P00-1041.txt | Citing Article:  P10-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Banko et al (2000), we approximated the length distribution with a Gaussian.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2 shows the distribution of headline length.</S><S sid = NA ssid = NA>As can be seen, a Gaussian distribution could also model the likely lengths quite accurately.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P00-1041.txt | Citing Article:  W03-0501.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).</S><S sid = NA ssid = NA>More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P00-1041.txt | Citing Article:  D10-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It does so by building statistical models for content selection and surface realization.</S><S sid = NA ssid = NA>Headline Generation Based On Statistical Translation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P00-1041.txt | Citing Article:  N12-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper has presented an alternative to extractive summarization: an approach that makes it possible to generate coherent summaries that are shorter than a single sentence and that attempt to conform to a particular style.</S><S sid = NA ssid = NA>Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P00-1041.txt | Citing Article:  C10-2105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Banko et al (2000) uses beam search to identify approximate solutions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the simplest, zerolevel model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search (Forney, 1973) to efficiently find a near-optimal summary.</S><S sid = NA ssid = NA>2 Other statistical models might require the use of a different heuristic search algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P00-1041.txt | Citing Article:  W03-1202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In a similar vein, a summarizer can be considered to be ‘translating’ between two languages: one verbose and the other succinct (Berger and Lafferty, 1999; Witbrock and Mittal, 1999).</S><S sid = NA ssid = NA>More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P00-1041.txt | Citing Article:  P13-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Given good training corpora, this approach can also be used to generate headlines from a variety of formats: in one case, we experimented with corpora that contained Japanese documents and English headlines.</S><S sid = NA ssid = NA>Most previous work on summarization has focused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P00-1041.txt | Citing Article:  P13-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These extracts are then arranged in a linear order (usually the same order as in the original document) to form a summary document.</S><S sid = NA ssid = NA>The simplest model for document length is a fixed length based on document genre.</S> | Discourse Facet:  NA | Annotator: Automatic


