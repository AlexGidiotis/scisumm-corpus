Citance Number: 1 | Reference Article:  W04-1013.txt | Citing Article:  P04-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = NA ssid = NA>We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W04-1013.txt | Citing Article:  P14-2052.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used ROUGE (Lin, 2004) as an evaluation criterion.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries.</S><S sid = NA ssid = NA>In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W04-1013.txt | Citing Article:  N10-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S><S sid = NA ssid = NA>Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W04-1013.txt | Citing Article:  S12-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We call the skip-bigram-based Fmeasure, i.e.</S><S sid = NA ssid = NA>To achieve this, we extend ROUGE-S with the addition of unigram as counting unit.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W04-1013.txt | Citing Article:  C08-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S><S sid = NA ssid = NA>For example, if we set dskip to 0 then ROUGE-S is equivalent to bigram overlap Fmeasure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W04-1013.txt | Citing Article:  N06-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>He used LCS as an approximate string matching algorithm.</S><S sid = NA ssid = NA>The extended version is called ROUGE-SU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W04-1013.txt | Citing Article:  W08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Please see Papineni et al. (2001) for details about BLEU.</S><S sid = NA ssid = NA>Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W04-1013.txt | Citing Article:  N12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries.</S><S sid = NA ssid = NA>ROUGE: A Package For Automatic Evaluation Of Summaries</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W04-1013.txt | Citing Article:  W08-1808.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Therefore, only Rlcs is considered.</S><S sid = NA ssid = NA>Therefore, only Rwlcs is considered.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W04-1013.txt | Citing Article:  P13-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>ROUGE: A Package For Automatic Evaluation Of Summaries</S><S sid = NA ssid = NA>A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W04-1013.txt | Citing Article:  N09-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The author would like to thank the anonymous reviewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.</S><S sid = NA ssid = NA>N=2, for the purpose of explanation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W04-1013.txt | Citing Article:  N07-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The author would like to thank the anonymous reviewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.</S><S sid = NA ssid = NA>N=2, for the purpose of explanation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W04-1013.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S><S sid = NA ssid = NA>The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W04-1013.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We call the skip-bigram-based Fmeasure, i.e.</S><S sid = NA ssid = NA>We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W04-1013.txt | Citing Article:  W06-1643.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using multiple references improved performance though not much.</S><S sid = NA ssid = NA>Given M references, we compute the best score over M sets of M-1 references.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W04-1013.txt | Citing Article:  W06-1643.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, how to achieve high correlation with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic.</S><S sid = NA ssid = NA>In the single document summarization tasks we had over 100 samples; while we only had about 30 samples in the multi-document tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W04-1013.txt | Citing Article:  D11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Please see Papineni et al. (2001) for details about BLEU.</S><S sid = NA ssid = NA>Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W04-1013.txt | Citing Article:  D11-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.</S><S sid = NA ssid = NA>ROUGE: A Package For Automatic Evaluation Of Summaries</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W04-1013.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We found that ROUGE-1, ROUGE-L, ROUGESU4 and 9, and ROUGE-W were very good measures in this category, ROUGE-N with N > 1 performed significantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1.</S><S sid = NA ssid = NA>These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W04-1013.txt | Citing Article:  P10-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used ROUGE (Lin, 2004) for evaluating the content of summaries.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries.</S><S sid = NA ssid = NA>We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</S> | Discourse Facet:  NA | Annotator: Automatic


