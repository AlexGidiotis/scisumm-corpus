Citance Number: 1 | Reference Article:  P06-2101.txt | Citing Article:  W06-2929.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>Conditional random fields: Probabilistic models for segmenting labeling sequence data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-2101.txt | Citing Article:  D08-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).</S><S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-2101.txt | Citing Article:  D10-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.</S><S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-2101.txt | Citing Article:  D08-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-2101.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2003.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-2101.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.</S><S sid = NA ssid = NA>2003.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-2101.txt | Citing Article:  W09-0424.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.</S><S sid = NA ssid = NA>Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi: This “smoothed” objective is now continuous and differentiable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-2101.txt | Citing Article:  W11-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-2101.txt | Citing Article:  W11-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-2101.txt | Citing Article:  W11-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>We again use a Taylor series to approximate the expected log brevity penalty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-2101.txt | Citing Article:  W08-0304.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>Final results are reported for a larger, disjoint test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-2101.txt | Citing Article:  W08-0304.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.</S><S sid = NA ssid = NA>2003.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-2101.txt | Citing Article:  N12-1026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-2101.txt | Citing Article:  N12-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P06-2101.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2005.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P06-2101.txt | Citing Article:  P11-1071.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.</S><S sid = NA ssid = NA>2003.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P06-2101.txt | Citing Article:  D07-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have seen that annealed minimum risk training provides a useful alternative to maximum likelihood and minimum error training.</S><S sid = NA ssid = NA>For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P06-2101.txt | Citing Article:  D07-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P06-2101.txt | Citing Article:  N10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P06-2101.txt | Citing Article:  W10-1721.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>N. A. Smith and J. Eisner.</S><S sid = NA ssid = NA>2006.</S> | Discourse Facet:  NA | Annotator: Automatic


