Citance Number: 1 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A baseline statistical parser is used to generate N-best output both for its training set and for test data sentences.</S><S sid = NA ssid = NA>Discriminative Reranking For Natural Language Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question): For this reason we describe these approaches as “Joint log-linear models.”The probability of a tree xi,j is Here Z is the (infinite) set of possible trees, and the denominator cannot be calculated explicitly.</S><S sid = NA ssid = NA>The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our work, we included all features described in (Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The following types of features were included in the model.</S><S sid = NA ssid = NA>Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5.4.3 Efficiency Gains.</S><S sid = NA ssid = NA>In this section we explore the empirical gains in efficiency seen on the parsing data sets in this article.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J05-1003.txt | Citing Article:  H05-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The number of such parses varies sentence by sentence.</S><S sid = NA ssid = NA>For example, the iterative scaling procedure described above must be applied for a number of features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J05-1003.txt | Citing Article:  P07-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Same as Rules, but also including the entire rule above the rule.</S><S sid = NA ssid = NA>Two-level rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J05-1003.txt | Citing Article:  D11-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that the output of our baseline parser produces syntactic trees with headword annotations (see Collins [1999]) for a description of the rules used to find headwords).</S><S sid = NA ssid = NA>The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse: where again, gpÄ is one if p is true, zero otherwise.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J05-1003.txt | Citing Article:  W07-1202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>There is also work on discriminative models for parse reranking (Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Discriminative Reranking For Natural Language Parsing</S><S sid = NA ssid = NA>This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J05-1003.txt | Citing Article:  W08-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Discriminative Reranking For Natural Language Parsing</S><S sid = NA ssid = NA>Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J05-1003.txt | Citing Article:  W08-2103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.</S><S sid = NA ssid = NA>We now describe the form of these updates.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>They are important for a few reasons.</S><S sid = NA ssid = NA>For example, a score of 101.5 indicates a 1.5% increase in this score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In general, a separate set of instances is used in cross-validation to choose the stopping point, that is, to decide on the number of features in the model.</S><S sid = NA ssid = NA>The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Johnson et al. (1999) and Riezler et al.</S><S sid = NA ssid = NA>We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).</S><S sid = NA ssid = NA>The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J05-1003.txt | Citing Article:  N12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.</S><S sid = NA ssid = NA>It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J05-1003.txt | Citing Article:  W08-0130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use Si,j to refer to this weight.</S><S sid = NA ssid = NA>Assuming we greedily pick a single feature with some weight to update the model, and given that the current parameter settings are ¯a, the optimal feature/weight pair (k*, d*) is Note that this is essentially the idea behind the “boosting”approach to feature selection introduced in section 3.3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J05-1003.txt | Citing Article:  P11-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]).</S><S sid = NA ssid = NA>We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J05-1003.txt | Citing Article:  P08-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Same as Rules, but also including the entire rule above the rule.</S><S sid = NA ssid = NA>The reranking models in this article were originally introduced in Collins (2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J05-1003.txt | Citing Article:  D07-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).</S><S sid = NA ssid = NA>Discriminative Reranking For Natural Language Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J05-1003.txt | Citing Article:  W12-1623.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We define GEN(x)ÎY to be the set of candidates for a given input x.</S><S sid = NA ssid = NA>The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method.</S> | Discourse Facet:  NA | Annotator: Automatic


