Citance Number: 1 | Reference Article:  J03-1002.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.3.1 Model 6.</S><S sid = NA ssid = NA>This notation indicates that five iterations of Model 1, five iterations of HMM, three iterations of Model 3, three iterations of Model 4, and three iterations of Model 6 are performed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J03-1002.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The persons conducting the annotation are asked to specify alignments of two different kinds: an S (sure) alignment, for alignments that are unambiguous, and a P (possible) alignment, for ambiguous alignments.</S><S sid = NA ssid = NA>This can be accomplished by forming the intersection of the sure alignments (S = S1∩S2) and the union of the possible alignments (P = P1∪P2), respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J03-1002.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These definitions of precision, recall and the AER are based on the assumption that a recall error can occur only if an S alignment is not found and a precision error can occur only if the found alignment is not even P. The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.</S><S sid = NA ssid = NA>Tables 15 and 16 show precision, recall, and alignment error rate for the last iteration of Model 6 for both translation directions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J03-1002.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.1.2 Heuristic Models.</S><S sid = NA ssid = NA>Instead, heuristic models are usually used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J03-1002.txt | Citing Article:  P04-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.3.1 Model 6.</S><S sid = NA ssid = NA>These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J03-1002.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.</S><S sid = NA ssid = NA>Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J03-1002.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.</S><S sid = NA ssid = NA>Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J03-1002.txt | Citing Article:  H05-1085.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).</S><S sid = NA ssid = NA>In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J03-1002.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The entries for each language can be a single word or an entire phrase.</S><S sid = NA ssid = NA>In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J03-1002.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001).</S><S sid = NA ssid = NA>Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J03-1002.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001).</S><S sid = NA ssid = NA>Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J03-1002.txt | Citing Article:  W12-4202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 1 offers an overview of the properties of the various alignment models.</S><S sid = NA ssid = NA>Table 8 shows the computing time for performing one iteration of the EM algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J03-1002.txt | Citing Article:  W12-3131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To solve this problem, we perform training in both translation directions (source to target, target to source).</S><S sid = NA ssid = NA>The baseline alignment model does not allow a source word to be aligned with more than one target word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J03-1002.txt | Citing Article:  W12-3131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2.1.2 Heuristic Models.</S><S sid = NA ssid = NA>This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J03-1002.txt | Citing Article:  W10-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996).</S><S sid = NA ssid = NA>In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J03-1002.txt | Citing Article:  D10-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From this association score matrix, the word alignment is then obtained by applying suitable heuristics.</S><S sid = NA ssid = NA>Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J03-1002.txt | Citing Article:  P11-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.</S><S sid = NA ssid = NA>The method with a varying µ gives worse results, but this method has one fewer parameter to be optimized on held-out data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J03-1002.txt | Citing Article:  P11-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>So far, refined statistical alignment models have in general been rarely used.</S><S sid = NA ssid = NA>In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J03-1002.txt | Citing Article:  P11-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).</S><S sid = NA ssid = NA>In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J03-1002.txt | Citing Article:  P14-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.</S><S sid = NA ssid = NA>These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


