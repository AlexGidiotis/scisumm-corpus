Citance Number: 1 | Reference Article:  N06-1014.txt | Citing Article:  N06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task.</S><S sid = NA ssid = NA>To our knowledge, this is the lowest published unsupervised AER result, and it is competitive with supervised approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N06-1014.txt | Citing Article:  N06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We maintain both high precision and recall.</S><S sid = NA ssid = NA>Intersection eliminates the spurious alignments, but at the expense of recall.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N06-1014.txt | Citing Article:  P14-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Alignment By Agreement</S><S sid = NA ssid = NA>We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N06-1014.txt | Citing Article:  P14-1139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Alignment By Agreement</S><S sid = NA ssid = NA>One can classify these six models into two groups: sequence-based models (models 1, 2, and HMM) and fertility-based models (models 3, 4, and 5).1 Whereas the sequence-based models are tractable and easily implemented, the more accurate fertility-based models are intractable and thus require approximation methods which are difficult to implement.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N06-1014.txt | Citing Article:  W12-3115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.</S><S sid = NA ssid = NA>We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N06-1014.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have described an efficient and fully unsupervised method of producing state-of-the-art word alignments.</S><S sid = NA ssid = NA>.. , c(4), c(> 5).3 We use three sets of distortion parameters, one for transitioning into the first state, one for transitioning out of the last state, and one for all other transitions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N06-1014.txt | Citing Article:  P11-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For these results, the threshold used for posterior decoding was tuned on the development set.</S><S sid = NA ssid = NA>Then we used the validation set and the first 100 sentences of the test set as our development set to tune our models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N06-1014.txt | Citing Article:  P11-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The E→F model gives 0 probability to any many-to-one alignments and the F→E model gives 0 probability to any one-to-many alignments.</S><S sid = NA ssid = NA>Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N06-1014.txt | Citing Article:  N10-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each model was trained for 5 iterations, using the same training regimen as in Och and Ney (2003). and joint training across different size training sets and different models, evaluated on the development set.</S><S sid = NA ssid = NA>We also observed that jointly trained HMMs converged very quickly—in 5 iterations—and did not exhibit overfitting with increased iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N06-1014.txt | Citing Article:  W12-3105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following past work, we initialized the translation probabilities of model 1 uniformly over word pairs that occur together in some sentence pair.</S><S sid = NA ssid = NA>Each z can be thought of as an element in the set of generalized alignments, where any subset of word pairs may be aligned (Och and Ney, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N06-1014.txt | Citing Article:  W08-0303.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task.</S><S sid = NA ssid = NA>On the other hand, if the edge (i+2, j+2) were included, that penalty would be mitigated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N06-1014.txt | Citing Article:  D10-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).</S><S sid = NA ssid = NA>We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N06-1014.txt | Citing Article:  D10-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).</S><S sid = NA ssid = NA>We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N06-1014.txt | Citing Article:  D10-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The HMM aligner used in this work was due to Liang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Typically, the Viterbi alignment argmaxz p(z  |x) is used.</S><S sid = NA ssid = NA>The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N06-1014.txt | Citing Article:  P07-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Word alignment is an important component of a complete statistical machine translation pipeline (Koehn et al., 2003).</S><S sid = NA ssid = NA>While AER is only a weak indicator of final translation quality in many current translation systems, we hope that more accurate alignments can eventually lead to improvements in the end-to-end translation process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N06-1014.txt | Citing Article:  D08-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As suggested by Liang et al (2006), we can group the distortion parameters into a few buckets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We parameterize the distortion c(·) using a multinomial distribution over 11 offset buckets c(<_ −5), c(−4),.</S><S sid = NA ssid = NA>The distortion parameters pd(aj = i0 | aj− = i) depend on the particular model (we write aj = 0 to denote the event that the j-th French word where p0 is the null-word probability and c(·) contains the distortion parameters for each offset argudepending on the length of the English sentence, which we found to be more effective than using a constant p0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N06-1014.txt | Citing Article:  D08-1084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained models 1, 2, and HMM on the Hansards data.</S><S sid = NA ssid = NA>We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sentences, a validation set of 37 sentences, and a test set of 447 sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N06-1014.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sentences, a validation set of 37 sentences, and a test set of 447 sentences.</S><S sid = NA ssid = NA>We trained models 1, 2, and HMM on the Hansards data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N06-1014.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035.</S><S sid = NA ssid = NA>By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N06-1014.txt | Citing Article:  N10-1140.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We performed word alignment using a cross EM word aligner (Liang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our case, we would have to sum over the set of alignments where each word in English is aligned to at most one word in French and each word in French is aligned to at most one word in English.</S><S sid = NA ssid = NA>Word alignment is an important component of a complete statistical machine translation pipeline (Koehn et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


