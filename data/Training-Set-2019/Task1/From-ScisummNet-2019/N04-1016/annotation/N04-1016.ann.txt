Citance Number: 1 | Reference Article:  N04-1016.txt | Citing Article:  W04-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.</S><S sid = NA ssid = NA>All search terms were submitted to the search engine in lower case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1016.txt | Citing Article:  P11-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.</S><S sid = NA ssid = NA>In these cases, we set the web count to a large constant (108).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1016.txt | Citing Article:  W06-1701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.</S><S sid = NA ssid = NA>All search terms were submitted to the search engine in lower case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1016.txt | Citing Article:  W06-1620.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The reason for this seems to be that the web is much larger than the BNC (about 1000 times); the size seems to compensate for the fact that simple heuristics were used to obtain web counts, and for the noise inherent in web data.</S><S sid = NA ssid = NA>He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1016.txt | Citing Article:  D07-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This result is consistent with Keller and Lapata’s (2003) findings that the web yields better counts than the BNC.</S><S sid = NA ssid = NA>Co-occurrence frequencies were estimated from the web using inflected queries (see Section 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1016.txt | Citing Article:  W06-1664.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Aside from counting bigrams, various tasks are attainable using web based models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection.</S><S sid = NA ssid = NA>For three tasks (candidate selection for MT, adjective ordering, and compound noun bracketing), we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1016.txt | Citing Article:  W06-1664.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rather, in our opinion, web-based models should be used as a new baseline for NLP tasks.</S><S sid = NA ssid = NA>We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1016.txt | Citing Article:  P07-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The results are compared against two state of the art approaches: a supervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a web based probabilistic model (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We found that in all but one case, web-based models fail to significantly outperform the state of the art.</S><S sid = NA ssid = NA>Our results were less encouraging when it comes to comparisons with state-of-the-art models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1016.txt | Citing Article:  P07-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For all but two tasks (candidate selection for MT and noun countability detection) we found that simple, unsupervised models perform significantly better when ngram frequencies are obtained from the web rather than from a standard large corpus.</S><S sid = NA ssid = NA>The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1016.txt | Citing Article:  P07-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models: a supervised model, Semantic Scattering (SS), (Moldovan and Badulescu, 2005), and a web-based unsupervised model (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare this model both against a baseline (same model, but parameters estimated on the BNC) and against state-of-the-art models from the literature, which are either supervised (i.e., use annotated training data) or unsupervised but rely on taxonomies to recreate missing counts.</S><S sid = NA ssid = NA>We found that in all but one case, web-based models fail to significantly outperform the state of the art.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1016.txt | Citing Article:  P07-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lauer uses eight prepositions for the paraphrasing task (of, for, in, at, on, from, with, about).</S><S sid = NA ssid = NA>Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1016.txt | Citing Article:  P07-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.</S><S sid = NA ssid = NA>We also found that there was no significant difference between the best Altavista model and the best model reported by Malouf, a supervised method using positional probability estimates from the BNC and morphological variants.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.</S><S sid = NA ssid = NA>The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The conceptbased model (see (7)) achieved an accuracy of 28% on this test set, whereas its lexicalized version reached an accuracy of 40% (see Table 11).</S><S sid = NA ssid = NA>On the BNC, the simple unigram model performs best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model).</S><S sid = NA ssid = NA>Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier’s encyclopedia, a corpus of 8 million words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.</S><S sid = NA ssid = NA>We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 3 compares the web-based models against the BNC models.</S><S sid = NA ssid = NA>We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1016.txt | Citing Article:  W05-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.</S><S sid = NA ssid = NA>Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1016.txt | Citing Article:  D07-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are grateful to Tim Baldwin, Silviu Cucerzan, Mark Lauer, Rob Malouf, Detelef Prescher, and Adwait Ratnaparkhi for making their data sets available.</S><S sid = NA ssid = NA>This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1016.txt | Citing Article:  N09-2018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.</S><S sid = NA ssid = NA>We concentrated solely on countable and uncountable nouns, as they account for the vast majority of the data.</S> | Discourse Facet:  NA | Annotator: Automatic


