Citance Number: 1 | Reference Article:  C04-1111.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel et al (2004) proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We propose an algorithm for learning highly scalable lexico-POS patterns.</S><S sid = NA ssid = NA>We learn lexico-POS patterns in an automatic way.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C04-1111.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our biggest challenge as we venture to the terascale is to use our new found wealth not only to build better systems, but to im prove our understanding of language.</S><S sid = NA ssid = NA>Approximate processing time on a single Pentium-4 2.5 GHz machine.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C04-1111.txt | Citing Article:  W09-0805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.</S><S sid = NA ssid = NA>We will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern-based approach is best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C04-1111.txt | Citing Article:  W09-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lately there has been a lot of interest in acquiring such text patterns using a set of hypernymy examples ,e.g. Pantel et al (2004) and Snow (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>771Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision.</S><S sid = NA ssid = NA>Girju et al (2003) improved upon Berland and Charniak's work using a machine learning filter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C04-1111.txt | Citing Article:  P08-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, following Pantel et al (2004), we assume that the recall of the baseline is 1 and estimate the relative recall RRS|B of the system S with respect to the baseline B using their respective precision scores PS and PB and number of instances extracted by them |S| and |B| as: RRS|B= PS? |S|PB? |B| 6.3 Gold Standard.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5.3 Recall.</S><S sid = NA ssid = NA>5.3.1 Relative recall Although it is impossible to know the number of is-a relationships in any non-trivial corpus, it is possible to compute the recall of a system relative to another system?s recall.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C04-1111.txt | Citing Article:  E09-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al,2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.</S><S sid = NA ssid = NA>We will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern-based approach is best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C04-1111.txt | Citing Article:  P06-3011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>KAON Text-To-Onto (Maedche and Staab, 2004) applies text mining algorithms for English and German texts to semi-automatically create an ontology, which includes algorithms for term extraction, for concept association extraction and for ontology pruning. Pattern-based approaches to extract hy ponym/hypernym relationships range from hand-crafted lexico-syntactic patterns (Hearst, 1992) to the automatic discovery of such patterns by e.g. a minimal edit distance algorithm (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns.</S><S sid = NA ssid = NA>2.1 Pattern-based approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C04-1111.txt | Citing Article:  P09-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel et al (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Towards Terascale Semantic Acquisition</S><S sid = NA ssid = NA>She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C04-1111.txt | Citing Article:  P08-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Many relationship classification methods utilize some language-dependent preprocessing, like deep or shallow parsing, part of speech tagging and 228 named entity annotation (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.</S><S sid = NA ssid = NA>We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C04-1111.txt | Citing Article:  W06-0502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Useful semantic relations can be extracted from large corpora using relatively simple patterns (e.g., (Pantel et al, 2004)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From the Information Extraction point of view neither of these patterns is very useful.</S><S sid = NA ssid = NA>This paper compares and contrasts two methods for extracting is-a relations from corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C04-1111.txt | Citing Article:  D09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.</S><S sid = NA ssid = NA>We will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern-based approach is best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C04-1111.txt | Citing Article:  P06-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003).</S><S sid = NA ssid = NA>Our pattern-based algorithm is very similar to the one used by Hearst.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C04-1111.txt | Citing Article:  P06-2075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, the pattern& quot; NP1 ,a|an NP2& quot;, ranked among the top IS-A pat terns by (Pantel et al, 2004), can represent both apposition (entailing) and a list of co-hyponyms (non-entailing).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>PRECISION MRR Pat.</S><S sid = NA ssid = NA>For example, Table 4 shows three randomly selected names for the pat tern-based system on the 15GB dataset.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C04-1111.txt | Citing Article:  W09-1703.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1, although (Pantel et al, 2004) demonstrated how to scale up their algorithms for the Web.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this light, we see an interesting need to de velop fast, robust, and scalable methods to mine semantic information from the Web.</S><S sid = NA ssid = NA>Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as ma chine translation (Och and Ney 2002), information extraction (Etzioni et al 2004), and question an swering (Brill et al 2001).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C04-1111.txt | Citing Article:  C10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pantel et al, (2004) proposed, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper compares and contrasts two methods for extracting is-a relations from corpora.</S><S sid = NA ssid = NA>Re cently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic de pendency features for each noun.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C04-1111.txt | Citing Article:  D09-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Patterns were shown to be very useful in all sorts of lexical acquisition tasks, giving high precision results at relatively low computational costs (Pantel et al, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We need to find patterns with relatively high occurrence and high precision.</S><S sid = NA ssid = NA>This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high preci sion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C04-1111.txt | Citing Article:  D09-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our biggest challenge as we venture to the terascale is to use our new found wealth not only to build better systems, but to im prove our understanding of language.</S><S sid = NA ssid = NA>Approximate processing time on a single Pentium-4 2.5 GHz machine.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C04-1111.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.</S><S sid = NA ssid = NA>We will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern-based approach is best.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C04-1111.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Pantel et al 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an in stance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co clustering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as ma chine translation (Och and Ney 2002), information extraction (Etzioni et al 2004), and question an swering (Brill et al 2001).</S><S sid = NA ssid = NA>Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C04-1111.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note how small they are, when compared to (Pantel et al 2004), which took 4 days for a smaller corpus using the same CPU.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On the 6 GB corpus, the co-occurrence approach took approximately 47 single Pentium-4 2.5 GHz processor days to complete, whereas it took the pattern-based approach only four days to complete on 6 GB and 10 days on 15 GB.</S><S sid = NA ssid = NA>TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger 2 days 125 days NP Chunker 3 days 214 days Dependency Parser 56 days 10.2 years Syntactic Parser 5.8 years 388.4 years 772 where n is the number of elements to be clustered, cef is the frequency count of word e in grammatical context f, and N is the total frequency count of all features of all words.</S> | Discourse Facet:  NA | Annotator: Automatic


