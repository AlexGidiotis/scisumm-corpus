Citance Number: 1 | Reference Article:  N06-2033.txt | Citing Article:  P07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parser Combination By Reparsing</S><S sid = NA ssid = NA>We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N06-2033.txt | Citing Article:  P07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.</S><S sid = NA ssid = NA>Six dependency parsers were used in our combination experiments, as described below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N06-2033.txt | Citing Article:  P07-1079.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.</S><S sid = NA ssid = NA>We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N06-2033.txt | Citing Article:  P14-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The large-margin parser described in (McDonald et al., 2005) was used with no alterations.</S><S sid = NA ssid = NA>Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N06-2033.txt | Citing Article:  P11-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The deterministic shift-reduce parsing algorithm of (Nivre & Scholz, 2004) was used to create two parsers2, one that processes the input sentence from left-to-right (LR), and one that goes from right-toleft (RL).</S><S sid = NA ssid = NA>Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N06-2033.txt | Citing Article:  P11-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research.</S><S sid = NA ssid = NA>This is done in a two stage process of reparsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N06-2033.txt | Citing Article:  P11-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.</S><S sid = NA ssid = NA>We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N06-2033.txt | Citing Article:  D08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).</S><S sid = NA ssid = NA>Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N06-2033.txt | Citing Article:  P11-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The large-margin parser described in (McDonald et al., 2005) was used with no alterations.</S><S sid = NA ssid = NA>Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N06-2033.txt | Citing Article:  P11-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The parsers that were used in the constituent reparsing experiments are: (1) Charniak and Johnson’s (2005) reranking parser; (2) Henderson’s (2004) synchronous neural network parser; (3) Bikel’s (2002) implementation of the Collins (1999) model 2 parser; and (4) two versions of Sagae and Lavie’s (2005) shift-reduce parser, one using a maximum entropy classifier, and one using support vector machines.</S><S sid = NA ssid = NA>Balancing precision and recall is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N06-2033.txt | Citing Article:  P11-2126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This group of features are completely identical to those used in Sagae and Lavie (2006a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Six dependency parsers were used in our combination experiments, as described below.</S><S sid = NA ssid = NA>Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N06-2033.txt | Citing Article:  W10-2927.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.</S><S sid = NA ssid = NA>Six dependency parsers were used in our combination experiments, as described below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N06-2033.txt | Citing Article:  N09-2064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees.</S><S sid = NA ssid = NA>Setting 1 is the emulation of Henderson and Brill’s voting.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Balancing precision and recall is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts.</S><S sid = NA ssid = NA>In other words, if a constituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.</S><S sid = NA ssid = NA>Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Setting 1 is the emulation of Henderson and Brill’s voting.</S><S sid = NA ssid = NA>Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In setting 2, t is set for balancing precision and recall.</S><S sid = NA ssid = NA>However, the scheme heavily favors precision over recall.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To solve this issue, Sagae and Lavie (2006) use a threshold to balance them.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By changing the threshold t we can control the precision/recall tradeoff.</S><S sid = NA ssid = NA>We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N06-2033.txt | Citing Article:  D09-1161.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.</S><S sid = NA ssid = NA>In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N06-2033.txt | Citing Article:  D07-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Therefore, we achieve additional parser diversity with the same algorithm, simply by varying the direction of parsing.</S><S sid = NA ssid = NA>Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).</S> | Discourse Facet:  NA | Annotator: Automatic


