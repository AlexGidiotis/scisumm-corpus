Citance Number: 1 | Reference Article:  C08-1018.txt | Citing Article:  P09-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and inser tion.</S><S sid = NA ssid = NA>Sentence Compression Beyond Word Deletion</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C08-1018.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.</S><S sid = NA ssid = NA>?he, he?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C08-1018.txt | Citing Article:  W11-1602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details).</S><S sid = NA ssid = NA>We hope that some of the work described here might be of relevance to other gen eration tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C08-1018.txt | Citing Article:  D10-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section we present our extensions of Cohnand Lapata?s (2007) model.</S><S sid = NA ssid = NA>(3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C08-1018.txt | Citing Article:  D11-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Firstly, the synchronous grammar provides expressive power to model consistent syntactic effects such as reordering, changes in nonterminal categories and lexical substitution.</S><S sid = NA ssid = NA>Sentence Compression Beyond Word Deletion</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C08-1018.txt | Citing Article:  N10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline.</S><S sid = NA ssid = NA>Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C08-1018.txt | Citing Article:  N10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We may, however, want to re tain the meaning of the sequence while rendering the sentence shorter, and this is precisely what our model can achieve, e.g., by allowing substitutions.As far as grammaticality is concerned, our abstractive model is numerically better than the extrac tive baseline but the difference is not statistically significant.</S><S sid = NA ssid = NA>Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C08-1018.txt | Citing Article:  D11-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Deletions accounted for 67% of rewrite operations, substitutions for 27%, and insertions for 6%.</S><S sid = NA ssid = NA>sentences were compressed automatically by our model and the baseline.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C08-1018.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Training The model is trained using SVM struct , a large margin method for structured output problems (Joachims, 2005; Tsochantaridis et al, 2005).</S><S sid = NA ssid = NA>Each grammar rule is assigned a weight, and these weights are learnt in discriminative training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C08-1018.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).</S><S sid = NA ssid = NA>Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C08-1018.txt | Citing Article:  P10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All our experiments used the data from this annotator.</S><S sid = NA ssid = NA>From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C08-1018.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sentence Compression Beyond Word Deletion</S><S sid = NA ssid = NA>Unfortunately,none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C08-1018.txt | Citing Article:  W09-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Sentence Compression Beyond Word Deletion</S><S sid = NA ssid = NA>The future of the nation is in your hands.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C08-1018.txt | Citing Article:  W11-1610.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.</S><S sid = NA ssid = NA>?he, he?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C08-1018.txt | Citing Article:  W09-2807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003).</S><S sid = NA ssid = NA>Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C08-1018.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The first abstractive compression method was proposed by Cohn and Lapata (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.</S><S sid = NA ssid = NA>Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C08-1018.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences.</S><S sid = NA ssid = NA>Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C08-1018.txt | Citing Article:  W11-2701.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our materials thus con sisted of 90 (30 ? 3) source-target sentences.</S><S sid = NA ssid = NA>Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C08-1018.txt | Citing Article:  D09-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results.</S><S sid = NA ssid = NA>We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C08-1018.txt | Citing Article:  W11-1611.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.</S><S sid = NA ssid = NA>?he, he?</S> | Discourse Facet:  NA | Annotator: Automatic


