Citance Number: 1 | Reference Article:  W00-0717.txt | Citing Article:  W01-0713.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I am currently applying this approach to the induction of phrase structure rules, and preliminary experiments have shown encouraging results.</S><S sid = NA ssid = NA>At this point we have a preliminary clustering â€” no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W00-0717.txt | Citing Article:  W12-0706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The work of Chater and Finch can be seen as similar to the work presented here given an independence assumption.</S><S sid = NA ssid = NA>Finch and Chater (1992), (1995) and Schiitze (1993), (1997) use a set of features derived from the co-occurrence statistics of common words together with standard clustering and information extraction techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W00-0717.txt | Citing Article:  E09-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Inducing Syntactic Categories By Context Distribution Clustering</S><S sid = NA ssid = NA>Appendix A shows the five most frequent words in a clustering with 77 clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W00-0717.txt | Citing Article:  P08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).</S><S sid = NA ssid = NA>We can then measure the similarity of words by the similarity of their context distributions, using the Kullback-Leibler (KL) divergence as a distance function.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our work builds on two older part-of-speech inducers word clustering algorithms of Clark (2000) and Brown et al (1992) that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.</S><S sid = NA ssid = NA>A minimum of this function can be found using the EM algorithm(Dempster et al., 1977).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.</S><S sid = NA ssid = NA>If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words: the word before and the word after.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can then measure the similarity of words by the similarity of their context distributions, using the Kullback-Leibler (KL) divergence as a distance function.</S><S sid = NA ssid = NA>Unfortunately it is not possible to cluster based directly on the context distributions for two reasons: first the data is too sparse to estimate the context distributions adequately for any but the most frequent words, and secondly some words which intuitively are very similar (Schi_itze's example is 'a' and 'an') have radically different context distributions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.</S><S sid = NA ssid = NA>If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words: the word before and the word after.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.</S><S sid = NA ssid = NA>The new algorithm currently does not use information about the orthography of the word, an important source of information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.</S><S sid = NA ssid = NA>If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words: the word before and the word after.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W00-0717.txt | Citing Article:  D11-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.</S><S sid = NA ssid = NA>In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W00-0717.txt | Citing Article:  N09-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).</S><S sid = NA ssid = NA>For two CLAWS tags, AJO (adjective) and NN1(singular common noun) that occur frequently among rare words in the corpus, I selected all of the words that occurred n times in the corpus, and at least half the time had that CLAWS tag.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W00-0717.txt | Citing Article:  W08-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At this point we have a preliminary clustering â€” no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties.</S><S sid = NA ssid = NA>Previous techniques give good results, but fail to cope well with ambiguity or rare words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W00-0717.txt | Citing Article:  W04-2602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark (2000) reports results on a corpus containing 12 million terms, Schutze (1993) on one containing 25 million terms, and Brown, et al (1992) on one containing 365 million terms.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).</S><S sid = NA ssid = NA>In absolute terms the perplexities are rather high; I deliberately chose a rather crude model without backing off and only the minimum amount of smoothing, which I felt might sharpen the contrast.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W00-0717.txt | Citing Article:  W04-2602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Clark (2000) presentsa framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ambiguity can be handled naturally within this framework.</S><S sid = NA ssid = NA>There are often several local minima â€” in practice this does not seem to be a major problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W00-0717.txt | Citing Article:  W11-0303.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters (Clark, 2000) computed by Finkel and Manning (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Both of these problems can be overcome in the normal way by using clusters: approximate the context distribution as being a probability distribution over ordered pairs of clusters multiplied by the conditional distributions of the words given the clusters : I use an iterative algorithm, starting with a trivial clustering, with each of the K clusters filled with the kth most frequent word in the corpus.</S><S sid = NA ssid = NA>Comparison against existing tag-sets is not meaningful â€” one set of tags chosen by linguists would score very badly against another without this implying any fault as there is no 'gold standard'.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W00-0717.txt | Citing Article:  W10-2922.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I therefore chose to use an objective statistical measure, the perplexity of a very simple finite state model, to compare the tags generated with this clustering technique against the BNC tags, which uses the CLAWS-4 tag set (Leech et al., 1994) which had 76 tags.</S><S sid = NA ssid = NA>As can be seen, the perplexity is lower with the model trained on data tagged with the new algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W00-0717.txt | Citing Article:  D07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each cluster is represented by the most frequent member of the cluster.</S><S sid = NA ssid = NA>I then sort the words by the divergence from the cluster that is closest to them, and select the best as being the members of the cluster for the next iteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W00-0717.txt | Citing Article:  D07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schutze (1995) and Clark (2000)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>I am currently applying this approach to the induction of phrase structure rules, and preliminary experiments have shown encouraging results.</S><S sid = NA ssid = NA>This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W00-0717.txt | Citing Article:  D07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In fact, Jardino and Adda (1994), Schutze (1997) and Clark (2000) have attempted to address the ambiguity problem to a certain extent.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are often several local minima â€” in practice this does not seem to be a major problem.</S><S sid = NA ssid = NA>Ambiguity can be handled naturally within this framework.</S> | Discourse Facet:  NA | Annotator: Automatic


