Citance Number: 1 | Reference Article:  P95-1026.txt | Citing Article:  W96-0104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Remarkably good performance may be achieved by identifying a single defining collocate for each class (e.g. bird and machine for the word crane), and using for seeds only those contexts containing one of these words.</S><S sid = NA ssid = NA>Comparative performance: Column 5 shows the relative performance of supervised training using the decision list algorithm, applied to the same data and not using any discourse information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P95-1026.txt | Citing Article:  W96-0104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The details of this process are discussed in Section 7.</S><S sid = NA ssid = NA>This provides a mechanism for bootstrapping a sense tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P95-1026.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The training sets (e.g.</S><S sid = NA ssid = NA>This helps improve the purity of the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P95-1026.txt | Citing Article:  W09-2404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).</S><S sid = NA ssid = NA>In general, the decision-list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P95-1026.txt | Citing Article:  P13-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>They have been displaced by more broadly applicable collocations that better partition the newly learned classes.</S><S sid = NA ssid = NA>For these words, the table below measures the claim's accuracy (when the word occurs more than once in a discourse, how often it takes on the majority sense for the discourse) and applicability (how often the word does occur more than once in a discourse).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P95-1026.txt | Citing Article:  P13-1120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The strong tendency for words to exhibit only one sense in a given collocation was observed and quantified in (Yarowsky, 1993).</S><S sid = NA ssid = NA>The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P95-1026.txt | Citing Article:  W04-0846.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach is least successful for senses with a complex concept space, which cannot be adequately represented by single words.</S><S sid = NA ssid = NA>While not fully automatic, this approach yields rich and highly reliable seed sets with minimal work.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P95-1026.txt | Citing Article:  W03-0417.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similarly, the one-sense-per-discourse constraint may also be used to correct erroneously labeled examples.</S><S sid = NA ssid = NA>This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P95-1026.txt | Citing Article:  W04-2808.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example:</S><S sid = NA ssid = NA>This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P95-1026.txt | Citing Article:  W04-2808.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similarly, the one-sense-per-discourse constraint may also be used to correct erroneously labeled examples.</S><S sid = NA ssid = NA>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P95-1026.txt | Citing Article:  P11-2045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus the noise introduced by a few irrelevant or misleading seed words is not fatal.</S><S sid = NA ssid = NA>In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P95-1026.txt | Citing Article:  N07-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation.</S><S sid = NA ssid = NA>In a large corpus, identify all examples of the given polysemous word, storing their contexts as lines in an initially untagged training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P95-1026.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See Yarowsky (1995) for details.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Details are provided in (Yarowsky, to appear). encountered in other frameworks.</S><S sid = NA ssid = NA>The details of this process are discussed in Section 7.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P95-1026.txt | Citing Article:  N10-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).</S><S sid = NA ssid = NA>In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P95-1026.txt | Citing Article:  N10-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For each possible sense of the word, identify a relatively small number of training examples representative of that sense.'</S><S sid = NA ssid = NA>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P95-1026.txt | Citing Article:  P10-3016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>She trained her fully supervised algorithm on hand-labelled sentences, applied the result to new data and added the most confidently tagged examples to the training set.</S><S sid = NA ssid = NA>A supervised algorithm based on this property is given in (Yarowsky, 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P95-1026.txt | Citing Article:  P10-3016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm uses these properties to incrementally identify collocations for target senses of a word, given a few seed collocations 'Note that the problem here is sense disambiguation: assigning each instance of a word to established sense definitions (such as in a dictionary).</S><S sid = NA ssid = NA>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P95-1026.txt | Citing Article:  P14-2087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When this decision list is applied to a new test sentence, ... the loss of animal and plant species through extinction ... , the highest ranking collocation found in the target context (species) is used to classify the example as SENSE-A (a living plant).</S><S sid = NA ssid = NA>In general, the decision-list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P95-1026.txt | Citing Article:  P04-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A supervised algorithm based on this property is given in (Yarowsky, 1994).</S><S sid = NA ssid = NA>IIf their classification begins to waver because new examples have discredited the crucial collocate, they are returned to the residual and may later be classified differently.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P95-1026.txt | Citing Article:  P09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Self-training (Yarowsky, 1995) is a form of semi-supervised learning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A supervised algorithm based on this property is given in (Yarowsky, 1994).</S><S sid = NA ssid = NA>This augmentation of the training data can often form a bridge to new collocations that may not otherwise co-occur in the same nearby context with previously identified collocations.</S> | Discourse Facet:  NA | Annotator: Automatic


