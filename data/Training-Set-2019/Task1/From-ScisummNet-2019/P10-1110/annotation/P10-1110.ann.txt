Citance Number: 1 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dynamic Programming for Linear-Time Incremental Parsing</S><S sid = NA ssid = NA>So we apply the same beam search idea from Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).</S><S sid = NA ssid = NA>To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Feature templates are functions that draw information from the feature window (see Tab.</S><S sid = NA ssid = NA>A new state has the form where [i..j] is the span of the top tree s0, and sd..s1 are merely “left-contexts”.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the following, we take this second approach, which is also the approach of Huang and Sagae (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.</S><S sid = NA ssid = NA>Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our dynamic programming algorithm also runs on top of beam search in practice.</S><S sid = NA ssid = NA>We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context).</S><S sid = NA ssid = NA>Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec.</S><S sid = NA ssid = NA>So we apply the same beam search idea from Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989).</S><S sid = NA ssid = NA>To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P10-1110.txt | Citing Article:  P11-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our dynamic programming algorithm also runs on top of beam search in practice.</S><S sid = NA ssid = NA>5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P10-1110.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dynamic Programming for Linear-Time Incremental Parsing</S><S sid = NA ssid = NA>We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P10-1110.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dynamic Programming for Linear-Time Incremental Parsing</S><S sid = NA ssid = NA>As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P10-1110.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1(a) for the list of feature templates used in the full model.</S><S sid = NA ssid = NA>Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P10-1110.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 10011136), development (secs 886-931 and 11481151), and test (secs 816-885 and 1137-1147) sets, assume gold-standard POS-tags for the input, and use the head rules of Zhang and Clark (2008).</S><S sid = NA ssid = NA>To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P10-1110.txt | Citing Article:  P12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.</S><S sid = NA ssid = NA>Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P10-1110.txt | Citing Article:  D11-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 10011136), development (secs 886-931 and 11481151), and test (secs 816-885 and 1137-1147) sets, assume gold-standard POS-tags for the input, and use the head rules of Zhang and Clark (2008).</S><S sid = NA ssid = NA>For Secs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P10-1110.txt | Citing Article:  D11-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>H&S10 refers to the results of Huang and Sagae (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The higher frequency of early updates results in faster iterations of perceptron training.</S><S sid = NA ssid = NA>Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P10-1110.txt | Citing Article:  P14-2128.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>9,10</S><S sid = NA ssid = NA>Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P10-1110.txt | Citing Article:  W11-0906.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.</S><S sid = NA ssid = NA>Our dynamic programming algorithm also runs on top of beam search in practice.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P10-1110.txt | Citing Article:  S12-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the queue head position (current word q0 is wj).</S><S sid = NA ssid = NA>5b shows a similar comparison for dependency accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P10-1110.txt | Citing Article:  D12-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1, which is a subset of those in McDonald et al. (2005b).</S><S sid = NA ssid = NA>Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


