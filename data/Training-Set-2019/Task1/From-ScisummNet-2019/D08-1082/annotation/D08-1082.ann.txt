Citance Number: 1 | Reference Article:  D08-1082.txt | Citing Article:  E12-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are also interested in investigating ways to apply the generative model to the inverse task: generation of a NL sentence that explains a given MR structure.</S><S sid = NA ssid = NA>The last row contains hybrid patterns that reflect reordering of one productionâ€™s child semantic categories during the generation process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1082.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We describe in this section our proposed generative model, which simultaneously generates a NL sentence and an MR structure.</S><S sid = NA ssid = NA>WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1082.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The hybrid tree model (Lu et al, 2008) takes a transformative perspective that is in some ways more similar to our model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that our generative model is different from the synchronous context free grammars (SCFG) in a number of ways.</S><S sid = NA ssid = NA>This model is also referred to as Bigram Model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1082.txt | Citing Article:  P12-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Examples are given based on the hybrid tree in Figure 3.</S><S sid = NA ssid = NA>We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1082.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1082.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Of particular interest is our prior work Lu et al (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>This process produces a hybrid tree T, whose nodes are either NL words or MR productions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1082.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hybrid Tree in Lu et al (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free tree structured form.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1082.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this section, we present a novel hybrid tree model that provides the following extensions over the model of Lu et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model III We make the following assumption: We can view this model, called the Mixgram Model, as an interpolation between Model I and II.</S><S sid = NA ssid = NA>Figure 3 gives a partial hybrid tree for the training example from Section 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1082.txt | Citing Article:  D11-1149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since we allow a packed meaning forest representation rather than a fixed tree structure, the MR model parameters in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Therefore model parameters can be directly estimated from the training corpus by counting.</S><S sid = NA ssid = NA>The MR model parameters can be estimated independently from the other two.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Motivated by the limitations of these previous methods, we propose a new generative alignment model that includes a full semantic parsing model proposed by Lu et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>We describe in this section our proposed generative model, which simultaneously generates a NL sentence and an MR structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>This IBM model is a word-toword alignment model that does not model word order, so we do not have to linearize the hierarchical MR structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our model is built on top of the generative semantic parsing model developed by Lu et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>These factors combine to present a challenging problem for parsing with the generative model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lu et al (2008) introduced a generative semantic parsing model using a hybrid-tree framework.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>The hybrid tree is constructed using hidden variables and estimated from the training set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use Lu et al (2008)'s generative model for this step, in which: P (w|e)=?? T over (w, m) P (T ,w|m) (2) where m is the MR logical form defined by event e and T is a hybrid tree defined over the NL? MR pair (w, m).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree.</S><S sid = NA ssid = NA>The correct reference hybrid tree is determined by running the Viterbi algorithm on each training NL-MR pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Lu et al (2008) propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model III is simply an interpolation of the above two models.</S><S sid = NA ssid = NA>The algorithm supports both unigram and bigram context assumptions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our model is built on top of Lu et al (2008)'s generative semantic parsing model, which is also trained in several steps in its best-performing version.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>In fact, if we look at the recall scores alone, our best-performing model achieves a 6.7% and 9.8% absolute improvement over two other state-of-the-art models WASP and KRISP respectively.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The bigram model of Lu et al (2008), which is the one used in this paper, must be trained using parameters previously learned for the IBM Model 1 and unigram model in order to exhibit the best performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are three categories of parameters used in the model.</S><S sid = NA ssid = NA>This model is also referred to as Bigram Model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1082.txt | Citing Article:  C10-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A Generative Model for Parsing Natural Language to Meaning Representations</S><S sid = NA ssid = NA>We describe in this section our proposed generative model, which simultaneously generates a NL sentence and an MR structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1082.txt | Citing Article:  W11-0105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are also interested in investigating ways to apply the generative model to the inverse task: generation of a NL sentence that explains a given MR structure.</S><S sid = NA ssid = NA>The last row contains hybrid patterns that reflect reordering of one productionâ€™s child semantic categories during the generation process.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1082.txt | Citing Article:  W11-0105.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006).</S><S sid = NA ssid = NA>Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


