Citance Number: 1 | Reference Article:  P90-1034.txt | Citing Article:  C04-1165.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the subject-verb-object table, the root form of the head of phrases is recorded, and the deep subject and object are used when available.</S><S sid = NA ssid = NA>This demonstration has depended on: 1) the availability of relatively large text corpora; 2) the existence of parsing technology that, despite a large error rate, allows us to find the relevant syntactic relations in unrestricted text; and 3) (most important) the fact that the lexical relations involved in the distribution of words in syntactic structures are an extremely strong linguistic constraint.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P90-1034.txt | Citing Article:  P07-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.</S><S sid = NA ssid = NA>We propose the following metric of similarity, based on the mutual information of verbs and arguments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P90-1034.txt | Citing Article:  C04-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But it is unlikely that the class is well-defined.</S><S sid = NA ssid = NA>The second column in each table shows the number of instances that the noun appears in a predicate-argument pair (including verb environments not in the list in the fifth column).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P90-1034.txt | Citing Article:  P06-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first column lists the noun which is similar to boat.</S><S sid = NA ssid = NA>This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P90-1034.txt | Citing Article:  W02-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Empty words.</S><S sid = NA ssid = NA>Some are not synonyms but are nevertheless closely related: economist - analyst, 2 - 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P90-1034.txt | Citing Article:  C04-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.</S><S sid = NA ssid = NA>The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P90-1034.txt | Citing Article:  C04-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.</S><S sid = NA ssid = NA>The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P90-1034.txt | Citing Article:  C04-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Of course, not all nouns fall into such neat clusters: Table 6 shows a quite heterogeneous group of nouns similar to table, though even here the most similar word (floor) is plausible.</S><S sid = NA ssid = NA>A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P90-1034.txt | Citing Article:  C00-2104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.</S><S sid = NA ssid = NA>We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P90-1034.txt | Citing Article:  W04-1216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.</S><S sid = NA ssid = NA>The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P90-1034.txt | Citing Article:  N09-3007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.</S><S sid = NA ssid = NA>This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P90-1034.txt | Citing Article:  P08-2008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Nouns may then be grouped according to the extent to which they appear in similar environments.</S><S sid = NA ssid = NA>The first column lists the noun which is similar to boat.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P90-1034.txt | Citing Article:  P06-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(It may also skew the sample to the extent that the parsing errors are consistent.)</S><S sid = NA ssid = NA>The work that has been done based on Harris' distributional hypothesis (most notably, the work of the associates of the Linguistic String Project (see for example, Hirschman, Grishrnan, and Sager 1975)) unfortunately does not provide a direct answer, since the corpora used have been small (tens of thousands of words rather than millions) and the analysis has typically involved considerable intervention by the researchers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P90-1034.txt | Citing Article:  P06-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The distributional hypothesis is that nouns are similar to the extent that they share contexts.</S><S sid = NA ssid = NA>Thus, of the ten nouns most similar to boat (Table 4), nine are words for vehicles; the most similar noun is the near-synonym ship.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P90-1034.txt | Citing Article:  E99-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The similarity metric proposed here, based on subject-verb-object relations, represents a considerable reduction in the information available in the subjec-verbobject table.</S><S sid = NA ssid = NA>We propose the following metric of similarity, based on the mutual information of verbs and arguments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P90-1034.txt | Citing Article:  P98-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.</S><S sid = NA ssid = NA>The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P90-1034.txt | Citing Article:  P98-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Hindle, 1990), a small set of sample results are presented.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It would be possible to predetermine a set of empty words in advance of analysis, and thus avoid some of the problem presented by empty words.</S><S sid = NA ssid = NA>The current sample is too small; many words occur too infrequently to be adequately sampled, and it is easy to think of usages that are not represented in the sample.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P90-1034.txt | Citing Article:  W12-3201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.</S><S sid = NA ssid = NA>We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P90-1034.txt | Citing Article:  P06-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We propose the following metric of similarity, based on the mutual information of verbs and arguments.</S><S sid = NA ssid = NA>Each noun has a set of verbs that it occurs with (either as subject or object), and for each such relationship, there is a mutual information value.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P90-1034.txt | Citing Article:  P06-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first column lists the noun which is similar to boat.</S><S sid = NA ssid = NA>Now define the overall similarity of two nouns as the sum across all verbs of the object similarity and the subject similarity, as in (3).</S> | Discourse Facet:  NA | Annotator: Automatic


