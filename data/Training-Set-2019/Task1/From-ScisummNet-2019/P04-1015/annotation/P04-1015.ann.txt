Citance Number: 1 | Reference Article:  P04-1015.txt | Citing Article:  D10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>0347631.</S><S sid = NA ssid = NA>Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-1015.txt | Citing Article:  D10-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>0347631.</S><S sid = NA ssid = NA>Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-1015.txt | Citing Article:  P12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As a final note, following Collins (2002), we used the averaged parameters from the training algorithm in decoding test examples in our experiments.</S><S sid = NA ssid = NA>This sort of scenario was used in Roark et al. (2004) for training an n-gram language model using the perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-1015.txt | Citing Article:  W09-0508.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is possible to prove that, provided the training set (xi ,zi) is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We can then state the following theorem (see (Collins, 2002) for a proof): This theorem implies that if there is a parameter vector U which makes zero errors on the training set, then after a finite number of iterations the training algorithm will converge to parameter values with zero training error.</S><S sid = NA ssid = NA>We will say that a training sequence (xi, yi) for i = 1... n is separable with margin S > 0 if there exists some vector U with ||U ||= 1 such that Next, define Ne to be the number of times an error is made by the algorithm in figure 1 – that is, the number of times that zi =6 yi for some (t, i) pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-1015.txt | Citing Article:  D10-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although we have not discussed it to this point, (Collins and Roark, 2004) present a perceptron algorithm for use with the Roark architecture.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model.</S><S sid = NA ssid = NA>This sort of scenario was used in Roark et al. (2004) for training an n-gram language model using the perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-1015.txt | Citing Article:  P07-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model.</S><S sid = NA ssid = NA>A beam-search algorithm is used during both training and decoding phases of the method.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-1015.txt | Citing Article:  E12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Shen et al (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The learning task is to set the parameter values α¯ using the training examples as evidence.</S><S sid = NA ssid = NA>This sort of scenario was used in Roark et al. (2004) for training an n-gram language model using the perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-1015.txt | Citing Article:  D12-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Early update was introduced by Collins and Roark (2004) for incremental parsing and adopted to forest re-ranking by Wang and Zong (2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Incremental Parsing With The Perceptron Algorithm</S><S sid = NA ssid = NA>This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-1015.txt | Citing Article:  P14-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also show, in Section 3.3, how perceptron training with early update (Collins and Roark, 2004) can be used in this setting.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This sort of scenario was used in Roark et al. (2004) for training an n-gram language model using the perceptron algorithm.</S><S sid = NA ssid = NA>Figure 5 shows the convergence of the training algorithm with neither of the two refinements presented; with just early update; and with both.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-1015.txt | Citing Article:  P14-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model.</S><S sid = NA ssid = NA>We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-1015.txt | Citing Article:  D09-1127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>0347631.</S><S sid = NA ssid = NA>Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-1015.txt | Citing Article:  W08-2129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of Collins and Roark (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.</S><S sid = NA ssid = NA>We then built a generative model with this feature set and the same tree transform, for use with the beam-search parser from Roark (2004) to compare against our baseline perceptron model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-1015.txt | Citing Article:  P13-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the gold standard state item falls off the agenda.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As before, define yi to be the gold standard parse for the i’th sentence, and also define yji to be the partial analysis under the gold-standard parse for the first j words of the i’th sentence.</S><S sid = NA ssid = NA>This is likely to lead to less noisy input to the parameter estimation algorithm; and early update will also improve efficiency, as at the early stages of training the parser will frequently give up after a small proportion of each sentence is processed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-1015.txt | Citing Article:  D08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>0347631.</S><S sid = NA ssid = NA>Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-1015.txt | Citing Article:  D08-1059.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, the model is updated as usual with the current example, which is then added to a cache of examples.</S><S sid = NA ssid = NA>Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-1015.txt | Citing Article:  P13-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Collins and Roark (2004) proposed the early-update idea, and Huang et al (2012) later proved its convergence and formalized a general framework which includes it as a special case.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, Johnson et al. (1999) and Riezler et al.</S><S sid = NA ssid = NA>Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P04-1015.txt | Citing Article:  P09-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This section gives a description of Collins and Roark's incremental parser (Collins and Roark, 2004) and discusses its problem.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This section gives a description of the basic incremental parsing approach.</S><S sid = NA ssid = NA>Finally, we give an abstract description of an incremental parser, and describe how it can be used with the perceptron algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P04-1015.txt | Citing Article:  P09-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>0347631.</S><S sid = NA ssid = NA>Then the averaged parameters ¯αAV G are defined as ¯αAV G = Ei,t ¯αti/NT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P04-1015.txt | Citing Article:  P09-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best results of Collins and Roark (2004) (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We included some punctuation-oriented features, which included (i) a Boolean feature indicating whether the final punctuation is a question mark or not; (ii) the POS label of the word after the current look-ahead, if the current lookahead is punctuation or a coordinating conjunction; and (iii) a Boolean feature indicating whether the look-ahead is punctuation or not, that fires when the category immediately to the left of the current position is immediately preceded by punctuation.</S><S sid = NA ssid = NA>Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P04-1015.txt | Citing Article:  D12-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is likely to lead to less noisy input to the parameter estimation algorithm; and early update will also improve efficiency, as at the early stages of training the parser will frequently give up after a small proportion of each sentence is processed.</S><S sid = NA ssid = NA>Figure 5 shows the convergence of the training algorithm with neither of the two refinements presented; with just early update; and with both.</S> | Discourse Facet:  NA | Annotator: Automatic


