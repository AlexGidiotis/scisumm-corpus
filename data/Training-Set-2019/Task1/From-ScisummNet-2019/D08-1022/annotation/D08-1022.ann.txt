Citance Number: 1 | Reference Article:  D08-1022.txt | Citing Article:  P09-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S><S sid = NA ssid = NA>Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1022.txt | Citing Article:  P13-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.</S><S sid = NA ssid = NA>Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1022.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Inspired by the parsing literature on pruning (Charniak and Johnson, 2005; Huang, 2008) we penalize a rule r by the posterior probability of its tree fragment frag = lhs(r).</S><S sid = NA ssid = NA>The final BLEU score results are shown in Table 4.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1022.txt | Citing Article:  P12-3022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.</S><S sid = NA ssid = NA>Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.</S><S sid = NA ssid = NA>We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S><S sid = NA ssid = NA>We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes.</S><S sid = NA ssid = NA>In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.</S><S sid = NA ssid = NA>To test the effect of forest-based rule extraction, we parse the training set into parse forests and use three levels of pruning thresholds: pe = 2, 5, 8.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.</S><S sid = NA ssid = NA>Our experiments use composed rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1022.txt | Citing Article:  C10-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S><S sid = NA ssid = NA>We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S><S sid = NA ssid = NA>The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S><S sid = NA ssid = NA>Nodes with non-empty, contiguous, and faithful spans form the admissible set (shaded nodes in the figure), which serve as potential cut-points for rule extraction.3 With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we “cut” the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).</S><S sid = NA ssid = NA>We refer readers to Mi et al. (2008) for details of the decoding algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1022.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With pruning threshold pe = 8, forest-based extraction achieves a (case insensitive) BLEU score of 0.2533, which is an absolute improvement of 1.0% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al. (2005) (p < 0.01).</S><S sid = NA ssid = NA>In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.</S><S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.</S><S sid = NA ssid = NA>With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p < 0.01).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mi and Huang (2008) extend the tree-based rule extraction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S><S sid = NA ssid = NA>We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S><S sid = NA ssid = NA>Like in tree-based extraction, we extract rules from a packed forest F in two steps: It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Typically, these models extract rules using parse trees from both or either side(s) of the bitext.</S><S sid = NA ssid = NA>So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1022.txt | Citing Article:  P09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The conditional probability P(d  |T) decomposes into the product of rule probabilities: where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities.</S><S sid = NA ssid = NA>Forest-based Translation Rule Extraction</S> | Discourse Facet:  NA | Annotator: Automatic


