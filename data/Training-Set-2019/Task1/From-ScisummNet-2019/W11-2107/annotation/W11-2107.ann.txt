Citance Number: 1 | Reference Article:  W11-2107.txt | Citing Article:  W12-0514.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.</S><S sid = NA ssid = NA>Table 1 lists the number of phrase pairs found in each paraphrase table before and after filtering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W11-2107.txt | Citing Article:  P14-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), TER (Snover et al, 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Commonly used metrics such as BLEU and earlier versions of Meteor make no distinction between content and function words.</S><S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W11-2107.txt | Citing Article:  W12-3901.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S><S sid = NA ssid = NA>The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W11-2107.txt | Citing Article:  W12-3119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores.</S><S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W11-2107.txt | Citing Article:  E12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We report two translation measures: BLEU (Papineni et al 2002) and METEOR 1.3 (Denkowski and Lavie, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.</S><S sid = NA ssid = NA>The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W11-2107.txt | Citing Article:  W12-3131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Test set translations are evaluated using BLEU, TER, and Meteor 1.2.</S><S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W11-2107.txt | Citing Article:  W12-4202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6.</S><S sid = NA ssid = NA>Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W11-2107.txt | Citing Article:  N12-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data (Przybocki, 2009) plus 900M words of monolingual data from the English Gigaword corpus (Parker et al., 2009) to build a standard Moses system (Hoang et al., 2007) as follows.</S><S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W11-2107.txt | Citing Article:  S12-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.</S><S sid = NA ssid = NA>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W11-2107.txt | Citing Article:  P12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S><S sid = NA ssid = NA>Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W11-2107.txt | Citing Article:  W12-3102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The source code and all resources for Meteor 1.3 and the version of Z-MERT with Meteor integration will be available for download from the Meteor website.</S><S sid = NA ssid = NA>Stem: Stem words using a language-appropriate Snowball Stemmer (Porter, 2001) and match if the stems are identical.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W11-2107.txt | Citing Article:  P13-1138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.</S><S sid = NA ssid = NA>While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.</S> | Discourse Facet:  NA | Annotator: Automatic


