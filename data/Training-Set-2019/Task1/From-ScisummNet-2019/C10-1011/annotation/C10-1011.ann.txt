Citance Number: 1 | Reference Article:  C10-1011.txt | Citing Article:  D11-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>et al, 2009).</S><S sid = NA ssid = NA>We combined thisparsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al, 2003; McDon ald et al, 2005; Crammer et al, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  C10-1011.txt | Citing Article:  P14-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, the parser would have ranked second for these languages.</S><S sid = NA ssid = NA>used cores.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  C10-1011.txt | Citing Article:  P13-1152.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>96</S><S sid = NA ssid = NA>It updates ??v as well, whereby the algorithm additionally weights the updates by ?.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  C10-1011.txt | Citing Article:  P13-1152.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as ?factors?.</S><S sid = NA ssid = NA>To determine why, we analyzed thetime usage of a dependency parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  C10-1011.txt | Citing Article:  P14-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm 3 shows the training algorithm forthe Hash Kernel in pseudo code.</S><S sid = NA ssid = NA>Algorithm 4 shows the paral lel feature extraction in pseudo code.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  C10-1011.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>{} // thread-save data list for w1 ? 1 to |xi| for w2 ? 1 to |xi| data-list?</S><S sid = NA ssid = NA>(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  C10-1011.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.</S><S sid = NA ssid = NA>et al, 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  C10-1011.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.</S><S sid = NA ssid = NA>To gain even faster parsing times, it may be possible to trade accuracy against speed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  C10-1011.txt | Citing Article:  P13-2046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>et al, 2009).</S><S sid = NA ssid = NA>We combined thisparsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al, 2003; McDon ald et al, 2005; Crammer et al, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  C10-1011.txt | Citing Article:  P13-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>4 4We provide the Parser and Hash Kernel as open source for download from http://code.google.com/p/mate-tools.</S><S sid = NA ssid = NA>To determine why, we analyzed thetime usage of a dependency parser.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  C10-1011.txt | Citing Article:  E12-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild.</S><S sid = NA ssid = NA>The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  C10-1011.txt | Citing Article:  P12-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>??u return ??w , ??v al., 2006).</S><S sid = NA ssid = NA>The baseline parser resembles the architec ture of McDonald and Pereira (2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  C10-1011.txt | Citing Article:  P14-2026.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.</S><S sid = NA ssid = NA>For a transition based parser, Gesmundo et al (2009) reported run times between 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se mantic dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  C10-1011.txt | Citing Article:  P12-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is 3.5 times faster than the baseline parser.</S><S sid = NA ssid = NA>However, the parser would have ranked second for these languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  C10-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>is the set of training examples where an example is a pair (xi, yi) of a sentence and the corresponding dependency structure.</S><S sid = NA ssid = NA>We will call the features built due to the training examples positive features and the rest negative features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  C10-1011.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Please refer to Table 4 of Bohnet (2010) for the complete feature list.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 6 shows.</S><S sid = NA ssid = NA>This procedure is repeated until the list is empty.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  C10-1011.txt | Citing Article:  P13-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>l represents the label, h the head, d the dependent, s a sibling, and g a grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.</S><S sid = NA ssid = NA>(n? 1) ? I + i ? = E ? I ? k + 2 // passive-aggressive weight tsr,k; A = read-features-and-calc-arrays(i,??w ) ; ter,k tsp,k; yp = predicte-projective-parse-tree(A);tep,k tsa,k; ya = non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average dren ?h,d,g where h, d, g, and s are the indexes of the words included in xi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  C10-1011.txt | Citing Article:  W12-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>96</S><S sid = NA ssid = NA>It updates ??v as well, whereby the algorithm additionally weights the updates by ?.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  C10-1011.txt | Citing Article:  P14-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.</S><S sid = NA ssid = NA>Parsing and training times can be improved by methods that maintain the accuracy level, or methods that trade accuracy against better parsing times.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  C10-1011.txt | Citing Article:  D12-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The Hash Kernel shares values when collisions occur that can be considered as an approximation of the kernel function, because a weight might be adapted due to more than one feature.</S><S sid = NA ssid = NA>Algorithm 3: Training ? Hash Kernel for n?</S> | Discourse Facet:  NA | Annotator: Automatic


