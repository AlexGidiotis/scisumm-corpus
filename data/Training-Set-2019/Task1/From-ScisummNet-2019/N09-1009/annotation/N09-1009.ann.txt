Citance Number: 1 | Reference Article:  N09-1009.txt | Citing Article:  D12-1121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our extension to the model in Cohen et al. (2008) follows naturally after we have defined the shared LN distribution.</S><S sid = NA ssid = NA>Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N09-1009.txt | Citing Article:  P10-2034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.</S><S sid = NA ssid = NA>Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N09-1009.txt | Citing Article:  N10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights.</S><S sid = NA ssid = NA>We provide a variational EM algorithm for inference.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N09-1009.txt | Citing Article:  D10-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S><S sid = NA ssid = NA>2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.</S><S sid = NA ssid = NA>We define a shared logistic normal distribution with N “experts” over a collection of K multinomial distributions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S><S sid = NA ssid = NA>2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N09-1009.txt | Citing Article:  P11-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S><S sid = NA ssid = NA>2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In §3, we present our model and a variational inference algorithm for it.</S><S sid = NA ssid = NA>We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y).</S><S sid = NA ssid = NA>Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parts of speech are matched through the single coarse tagset (footnote 4).</S><S sid = NA ssid = NA>We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.</S><S sid = NA ssid = NA>Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p < 0.05).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N09-1009.txt | Citing Article:  P10-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A tree y is defined by a pair of functions yleft and yright (both 10, 1, 2,..., n} __+ 2{1,2,...,n}) that map each word to its sets of left and right dependents, respectively.</S><S sid = NA ssid = NA>Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These are the same training, development, and test sets used by Cohen and Smith (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.</S><S sid = NA ssid = NA>Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.</S><S sid = NA ssid = NA>This performance measure is also known as attachment accuracy.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N09-1009.txt | Citing Article:  N10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.</S><S sid = NA ssid = NA>2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N09-1009.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This has been done successfully in multilingual settings (Cohen and Smith, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al. ): bilities corresponding to a verbal parent (any parent, using the coarse tags of Cohen et al., 2008).</S><S sid = NA ssid = NA>In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N09-1009.txt | Citing Article:  D10-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our experiments found that these improvements do not hold on longer sentences.</S><S sid = NA ssid = NA>Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N09-1009.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our variational inference algorithm is derived similarly to that of Cohen et al. (2008).</S><S sid = NA ssid = NA>For the covariance matrices, we follow the setting in Cohen et al. (2008) in our experiments also described in §3.1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N09-1009.txt | Citing Article:  N10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing.</S><S sid = NA ssid = NA>The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&N.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N09-1009.txt | Citing Article:  D11-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The baselines include right attachment (where each word is attached to the word to its right), MLE via EM (Klein and Manning, 2004), and empirical Bayes with Dirichlet and LN priors (Cohen et al., 2008).</S><S sid = NA ssid = NA>There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


