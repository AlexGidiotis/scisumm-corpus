Citance Number: 1 | Reference Article:  J05-4003.txt | Citing Article:  W06-0605.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We work in the context of Arabic-English and Chinese-English statistical machine translation systems.</S><S sid = NA ssid = NA>This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J05-4003.txt | Citing Article:  N09-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The filter verifies that the ratio of the lengths of the two sentences is no greater than two.</S><S sid = NA ssid = NA>For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J05-4003.txt | Citing Article:  N09-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is due to the low coverage of the dictionary learned from that corpus.</S><S sid = NA ssid = NA>Tables 3 and 4 present the coverage of our extracted corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J05-4003.txt | Citing Article:  N09-2024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The most important feature of our parallel sentence selection approach is its robustness.</S><S sid = NA ssid = NA>The others are passed on to the parallel sentence selection stage.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs.</S><S sid = NA ssid = NA>However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.</S><S sid = NA ssid = NA>They start by attempting to identify similar article pairs from the two corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.</S><S sid = NA ssid = NA>The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.</S><S sid = NA ssid = NA>We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One such model is the IBM Model 1 (Brown et al. 1993).</S><S sid = NA ssid = NA>We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J05-4003.txt | Citing Article:  N10-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.</S><S sid = NA ssid = NA>The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J05-4003.txt | Citing Article:  D10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method.</S><S sid = NA ssid = NA>In this article, we have shown how they can be efficiently mined for parallel sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J05-4003.txt | Citing Article:  D07-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.</S><S sid = NA ssid = NA>We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J05-4003.txt | Citing Article:  W11-1209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The approach of Fung and Cheung (2004) is a simpler version of ours.</S><S sid = NA ssid = NA>Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J05-4003.txt | Citing Article:  W11-1209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1).</S><S sid = NA ssid = NA>This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J05-4003.txt | Citing Article:  W11-1209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One such model is the IBM Model 1 (Brown et al. 1993).</S><S sid = NA ssid = NA>We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J05-4003.txt | Citing Article:  W11-1209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones.</S><S sid = NA ssid = NA>Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J05-4003.txt | Citing Article:  W11-1205.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The approach of Fung and Cheung (2004) is a simpler version of ours.</S><S sid = NA ssid = NA>We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J05-4003.txt | Citing Article:  P09-2057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S><S sid = NA ssid = NA>However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J05-4003.txt | Citing Article:  P09-2057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.</S><S sid = NA ssid = NA>Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J05-4003.txt | Citing Article:  P09-2057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model.</S><S sid = NA ssid = NA>The filter verifies that the ratio of the lengths of the two sentences is no greater than two.</S> | Discourse Facet:  NA | Annotator: Automatic


