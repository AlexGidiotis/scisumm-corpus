<PAPER>
	<S sid="0">A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm</S><ABSTRACT>
	<SECTION title="Introduct ion. " number="1">
			<S sid="1" ssid="1">In recent years, we have seen a fair number of l)al)ers re- porting accuracies ofmore than 95% for English part of speech tagging with statistical language modeling tech- niques \[2-4, 10, 11\].</S>
			<S sid="2" ssid="2">On the other hand, there are few works on stochastic Japanese morphological nalysis \[9, 12, 14\], and they don't seem to have convinced the Japanese NLP community that the statistically-based teclmiques are superior to conventional rule-based tech- niques uch as \[16, 17\].</S>
			<S sid="3" ssid="3">We show in this paper that we can buihl a stochastic Japanese morphological nalyzer that offers approxi- mately 95% accuracy on a statistical language model- ing technique and an efficient two-pass N-best search strategy.</S>
			<S sid="4" ssid="4">We used tile simple tri-POS model as the tagging model for Japanese.</S>
			<S sid="5" ssid="5">Probability estimates were ob- tained after training on the ATI{ l)ialogue Database \[5\], whose word segmentation a d part of speech tag assignment were laboriously performed by hand.</S>
			<S sid="6" ssid="6">We propose a novel search strategy for getting the N best morphological nalysis hypotheses for the in- put sentence.</S>
			<S sid="7" ssid="7">It consists of the forward dynamic pro- gramming search and the backward A* search.</S>
			<S sid="8" ssid="8">The proposed algorithm amalgamates and extends three well-known algorithms in different fields: the Minimum Connective-Cost Method \[7\] for Japanese morphologi- cal analysis, Extended Viterbi Algorithm for charac- ter recognition \[6\], and "l~'ee-Trellis N-Best Search for speech recognition \[15\].</S>
			<S sid="9" ssid="9">We also propose a novel method for handling un- known words uniformly within the statistical pproach.</S>
			<S sid="10" ssid="10">Using character trigrams ms tim word model, it gener- ates the N-best word hypotheses that match the left- most substrings starting at a given position in the input senten ce.</S>
			<S sid="11" ssid="11">Moreover, we propose a novel method for evaluat- ing the performance of morphological analyzers.</S>
			<S sid="12" ssid="12">Un- like English, Japanese does not place spaces between words.</S>
			<S sid="13" ssid="13">It is difficult, even for native Japanese, to place word boundaries consistently because of the aggluti- native nature of the language.</S>
			<S sid="14" ssid="14">Thus, there were no standard performance metrics.</S>
			<S sid="15" ssid="15">We applied bracketing accuracy measures \[1\], which is originally used for En- glish parsers, to Japanese morphological nalyzers.</S>
			<S sid="16" ssid="16">We also slightly extended the original definition to describe the accuracy of tile N-best candidates.</S>
			<S sid="17" ssid="17">In the following sections, we first describe the tech- niques used in the proposed morphological nalyzer, we then explain the cwduation metrics and show the system's performance by experimental results.</S>
	</SECTION>
	<SECTION title="Tagging Model. " number="2">
			<S sid="18" ssid="1">2.1 Tr i -POS Mode l and Re la t ive F re -.</S>
			<S sid="19" ssid="2">quency Tra in ing We used the tri-POS (or triclass, tri-tag, tri-Ggram etc.) model ~Ls tile tagging model for Japanese.</S>
			<S sid="20" ssid="3">Con- sider a word segmentation f the input sentence W = wl w2.</S>
			<S sid="21" ssid="4">w,~ and a sequence of tags T = t i ts . . , t,, of the same length.</S>
			<S sid="22" ssid="5">The morphological analysis tmsk cau I)e formally defined ,~ finding a set of word segmen- tat.ion and parts of speech ~ssignment that maximize the joint probability of word sequence arm tag sequence P(W, 7').</S>
			<S sid="23" ssid="6">In the tri-POS model, the joint probability is approximated bythe product of parts of speech trigram probabilities P(t i l t i_2,t i_ l ) and word output probabil- ities for given part of speech P(wl\]ll): r(w,:r) = \]~ r(tdt,_o.,t,_x)r'(w, lt4 (1) i=1 201 In practice, we consider sentence boundaries ~s special symbols as follows.</S>
			<S sid="24" ssid="7">P(W,T) = P(ql#)P(wtltt)P(t,.</S>
			<S sid="25" ssid="8">l#, tl)P(w21t~) ~I P(tilti_2,ti_l)P(willi)P(#\[t,,_l,?,,) (2) i=3 where "#" indicates the sentence boundary marker.</S>
			<S sid="26" ssid="9">If we have some tagged text available, we can estimate the probabilities P(tdti_2,ti_l ) and P(wiltl) by comput- ing the relative frequencies of the corresponding events on this data:.</S>
			<S sid="27" ssid="10">N(ti_2, ti-1, tl) P(tifti-2'ti-t) = f(qltl-2'ti-x) - iV ( t i _ . . , , t i _ , ) (3) P(wilti) = f(wilt,) -- N(w,t) ('1) N(t) where f indicates the relative frequency, N(w, t) is t!,e number of times a given word w appears with tag l, aid N(li_2,ti-l,tl) is the number of times that sequer~ce l~ i _2t i _ l l i appears in the text.</S>
			<S sid="28" ssid="11">It is inevitable to s~ irer from sparse-data problem in the part of speech tag tri- gram probability I . To handle open text, trigram Frol:- ability is smoothed by interpolated estimation, wi~ich simply interpolates trlgram, bigram, unigram, and ze- rogram relative frequencies\[8\], P ( t i lq_ , ,q_ l ) = qaf(tilt,_2,q_, ) A-q2f(tdti_l) + qtf(ti) + qoV (5) where f indicates the relative.frequency and V is a uniform probability that each tag will occur.</S>
			<S sid="29" ssid="12">The non- negative weights qi satisfy q3 + q~ + q1 + q0 = 1, and they are adjusted so as to make the observed ata most probable after the adjustment by using EM algorithm ~-.</S>
			<S sid="30" ssid="13">2.2 Order Reduct ion and Recursive.</S>
			<S sid="31" ssid="14">Tracing In order to understand the search algorithm described in the next section, we will introduce the second order HMM and extended Viterbi algorithm \[6\].</S>
			<S sid="32" ssid="15">Considering the combined state sequence U = ltl'tt2.., ttn, where ul = tl and ui = ti-tli, we have P(uilui_l) = P(tilti_=,ti_l) (6) Substituting Equation (6) into Equation (l), we have lWe used 120 part of speedl tags.</S>
			<S sid="33" ssid="16">In the ATR Corpus, 26 parts of speech, 13 conjugation types, and 7 conjugation forms are defined.</S>
			<S sid="34" ssid="17">Out of 26, 5 parts of speech ave conjugation.</S>
			<S sid="35" ssid="18">Since we used a list of part of speech, conjugation type, and conjuga- tion form as a tag, there are 119 tags in the A'IT?</S>
			<S sid="36" ssid="19">Corpus.</S>
			<S sid="37" ssid="20">We added the sentence boundary marker to them.</S>
			<S sid="38" ssid="21">aTo handle open text, word output probahility P( lo i l t i ) must also be smoothed.</S>
			<S sid="39" ssid="22">Tiffs problem is discussed in a later section *Ls the unknown word problem.</S>
			<S sid="40" ssid="23">Equation model.</S>
			<S sid="41" ssid="24">'ill I . . .</S>
			<S sid="42" ssid="25">Wi we have P(W,r) = 1\].</S>
			<S sid="43" ssid="26">P(mlui-a)P(wilti) (7) i=1 (7) have the same form as the first order Conshler the partial word sequence HI/ = and the partial tag sequence Ti = t l . . .</S>
			<S sid="44" ssid="27">t l , F(w~,~) = ?)(w,_, ,~-, )P(~d,.</S>
			<S sid="45" ssid="28">-~)p(wdtO (8) Equation (8) suggests that, to find the maxlmmn P(I,Vi,7\]) for each ul, we need only to: remember the maximum P(W?_I, 7\]_1), extend each of these prob- abilities to every ul by computing Eqnation (8), and select the m;uxinmm P(~/Vi,Ti) for each ui.</S>
			<S sid="46" ssid="29">'thus, by increasing i by 1 to n, selecting the u. ttlat maximize P(W.,7\]~), and backtracing the sequence leading to the nmxinmm probability, we can get the optimal tag seqnence.</S>
	</SECTION>
	<SECTION title="Search  S t ra tegy. " number="3">
			<S sid="47" ssid="1">The search algorithm consists of a forward dynamic programming search and a backward A* search.</S>
			<S sid="48" ssid="2">First, a linear time dynamic programming is used for record- ing the scores of all partial paths in a table 3.</S>
			<S sid="49" ssid="3">A back- ward A* algorithm based tree search is then used to extend the partial paths.</S>
			<S sid="50" ssid="4">Partial paths extended in the backward tree search are ranked by their correspond- ing fill path scores, which are cmnputed by adding the scores of backward partial path scores to the cot responding best possihle scores of the remaining paths which are prerecorded in the forward search.</S>
			<S sid="51" ssid="5">Since the score of the incomplete portion of a path is exactly known, the backward search is admissible.</S>
			<S sid="52" ssid="6">That is, the top-N candidates are exact.</S>
			<S sid="53" ssid="7">3.1 The Forward DP Search.</S>
			<S sid="54" ssid="8">Table 1 shows the two data structures used in our al- gorithm.</S>
			<S sid="55" ssid="9">The st,'t, cture parse stores tile information of a word and the best partial path up to the word.</S>
			<S sid="56" ssid="10">Parse .s ta r t and parse.end are the indices of tile start and end positions of the word in the sentence.</S>
			<S sid="57" ssid="11">Parse.pos is tile part of speech tag, which is a list of part of speech, conjugation type, and conjugation form in our system for Japanese.</S>
			<S sid="58" ssid="12">Parse .n th -order -~tate is a list of the last two parts of speech tags includ- ing that of the current word.</S>
			<S sid="59" ssid="13">This slot corresponds to the combined state in the second order IIMM.</S>
			<S sid="60" ssid="14">Parse .prob-so - fa r is the score of the best partial path from the beginning of the sentence to the word.</S>
			<S sid="61" ssid="15">Parse.prev?ous i the pointer to the (best) previous parse structure as in conventional Viterbi decoding, which is not necessary if we use the backward N best search.</S>
			<S sid="62" ssid="16">~ln fact, we use two tables, pa~se-\].</S>
			<S sid="63" ssid="17">ist and path-~ap.</S>
			<S sid="64" ssid="18">The reason is described later.</S>
			<S sid="65" ssid="19">202 The structure word represents the word information in the dictionary including its lexical form, part of speech tag, and word output probability given tt,e part of speech.</S>
			<S sid="66" ssid="20">Table h Data structures for the N best algorithm start end pea nth-order-state prob-ao-far previous parse strttctul'e "tim beginning pasition of the word the end position of the word part of speech tag of the word a list of the la-'~t two parts (,f speech the b,~t partial path score from the start a pointer to previous parse strllettll'e word structure form \] lexical f,.'-n{ of the word l)Oa \[ part of speech tag of the word prob _ word outlmt probability Before explaining tim forward search, we will de- fine some flmctions and tables used in the algo- rithm.</S>
			<S sid="67" ssid="21">In the forward search, we use a table called parse-list, whose key is the end position of the parse structure, and wlm,se value is a list of parse structures that have the best partial path scores for each combined state at the end position.</S>
			<S sid="68" ssid="22">Function reg is ter - to -parse - l i s t registers a parse structure against the parse-list and maintains the best par- tim parses.</S>
			<S sid="69" ssid="23">Function get -parse - l i s t returns a list of parse structnres at the specified position.</S>
			<S sid="70" ssid="24">We also use the fimetion l e l tmost -subst r ings which returns a list of word structures in the dictionary whose lexical form matches the substrings tarting at the.</S>
			<S sid="71" ssid="25">specified position in the input sentence.</S>
			<S sid="72" ssid="26">funct ion ~orward-paae (string) begin i n i t ia l -a tepO ; It Pods spec ia l symbols at both ends.</S>
			<S sid="73" ssid="27">for iffil to length(s t r ing) do foreach parse in get -parse - l i s t ( i ) do foreach word ill l e f tmost -subat r ings(a t r ing , i ) ,I(7 poa-ngrma : - append(parse.nth-order-stato , l i s t (word.poa)) if (traneprob(poe-ngrtm) &gt; O) then new-parse :'.</S>
			<S sid="74" ssid="28">make-parseO ; new-parse.mtart :~ i ; new-parse.end : - i + length(word.form); hey-pares,poe :- word.pea; new-parae.nth-order-mtate :~ rest(pos-ngram) ; naw-paree.preb-ee- far :- parae.prob-so- far * transprob(pos-ngram) * word.prob; new-parse.previous := paras; reg ie ter -parse - to -parae- l t s t (new-parse) ; reg is ter -paree- to -path -ma p (new-parse) ; endif elld end end f inn l -e tQp( ) ; i/ Randlan t r tmai t ion to tho e~d symbol.</S>
			<S sid="75" ssid="29">end Figure h The forward DP search algorithm Figure 1 shows the central part of the forward dy- namic programming search algorithm.</S>
			<S sid="76" ssid="30">It starts from the beg,string of tim inlmt sentence, and proceeds char- attar by character.</S>
			<S sid="77" ssid="31">At each point in tim sentence, it looks up the combination of the best partial parses ending at the point and word hypotheses tarting at that point.</S>
			<S sid="78" ssid="32">If tim connection of a partial parse and a word llypothesis is allowed by the tagging model, a new continuation parse is made and registered in the parse - l i s t . The partial path score for the new con- titular,on parse is the product of the best partial path score up to the poi,g, the trigram probability of the last three parts of speech tags and the word output probability for LIfe part of speech 4.</S>
			<S sid="79" ssid="33">3 .2 The Backward A* Search The backward search uses a table called path-map, whose key is the end position of tile parse structure, and whose value is a list of parse structures that have the best partial path scores for each distinct combin~ ties of the start position and the combined state.</S>
			<S sid="80" ssid="34">The dilference 1)etween parse - l i s t and path-map is that path-map is classi/ied by tim start position of the last word in addition to tim combined state.</S>
			<S sid="81" ssid="35">This distinction is crucial for the proposed N best algorithm, l"or tim tbrward search to tind a parse that maximizes Equation (1), it is the parts of speech se- quence that matters.</S>
			<S sid="82" ssid="36">For the backward N-best search, how(wet, we want N most likely word segmentation and part of speech sequence.</S>
			<S sid="83" ssid="37">Parse-list may shadow less probable candidates that have the same part of speech sc:qnence for the best scoring candidate, but differ in tim segmentaL,on of the last word.</S>
			<S sid="84" ssid="38">As shown in Figure 1, path-map is made during the forward search by the function reg is ter -parse - to -path -map, which regis- ters a parse structure to path-map and maintains the best partial parses in the table's criteria.</S>
			<S sid="85" ssid="39">Now we describe the central part of tim backward A* search algorithm.</S>
			<S sid="86" ssid="40">But we assume that the readers know the A* algorithm, and exphtin only the way we applied the algorithm to the problem.</S>
			<S sid="87" ssid="41">We consider a parse structure ,~q a state in A* search.</S>
			<S sid="88" ssid="42">Two slates are e(plat if their parse structures have the same start position, end position, and com- bined state.</S>
			<S sid="89" ssid="43">The backward search starts at the end of the input, sentence, and backtracks to the beginning of the sentence using tim path-map.</S>
			<S sid="90" ssid="44">Initial states are obtained by looking up the entries of tim sentence nd position of the path-map.</S>
			<S sid="91" ssid="45">The suc- cessor states are obtained by first, looking u 1) tim en- tries of the path-map at the start position of the cur- rent parse, then cbecldng whether they satisfy the con- straint of the combined state transition in the second order IIMM, aim whether the transition is allowed by the tagging model.</S>
			<S sid="92" ssid="46">The combined state transition con- straint means that tim part of speech sequence in the parse .n th -order -s ta te of the current parse, ignor- given trlgraln.</S>
			<S sid="93" ssid="47">Functions i n i t ia l - s tep and f ina l - s tep treat \ [be t l 'a l iS l t \ [ons I%L sltl i l l~llce \] ,Ol l l |dl l l ' ieg, 203 ing the last element, equals that of tile previous parse, ignoring the first element.</S>
			<S sid="94" ssid="48">The state transition cost of the backward search is the product of the part of speech trigram probability and the word output probability.</S>
			<S sid="95" ssid="49">Tile score estimate of the remaining portion of a path is obtained from the parse .prob-so -~ar slot in the parse structure.</S>
			<S sid="96" ssid="50">The backward search generates the N best hypothe- ses sequentially and there is no need to preset N. The complexity of the backward search is significantly less than that of the forward search.</S>
	</SECTION>
	<SECTION title="Word  Mode l. " number="4">
			<S sid="97" ssid="1">To handle open text, we have to cope with unknown words.</S>
			<S sid="98" ssid="2">Since Japanese do not put spaces between words, we have to identify unknown words at first.</S>
			<S sid="99" ssid="3">To do this, we can look at the spelling (character sequence) that may constitute a word, or look at the context o identify words that are acceptable in this context.</S>
			<S sid="100" ssid="4">Once word hypotheses for unknown words are gener- ated, the proposed N-best algorithm will find tile most likely word segmentation a d part of speech assignment taking into account he entire sentence.</S>
			<S sid="101" ssid="5">Therefore, we can formalize the unknown word problem as (letermin- ing the span of an unknown word, assigning its part of speech, and estimating its probability given its part of speech.</S>
			<S sid="102" ssid="6">Let us call a computational model that determines the probability of any word hypothesis given its lexi- cal form and its part of speech the "word model".</S>
			<S sid="103" ssid="7">The word model must account for morphology and word for- marion to estimate the part of speech and tile probabil- ity of a word hypothesis.</S>
			<S sid="104" ssid="8">For tile first approxinmtion, we used the character trigram of each part of sl)eech as the word model.</S>
			<S sid="105" ssid="9">Let C = cic~.., c,~ denote the sequence of n charac- ters that constitute word zv whose part of speech is t. We approximate the probability of the word given part of speech P(wlt ) by tile trigram probabilities, p(,,,Iz) = P,(C) - - f',(~,l#, #)~',(~1#, &lt;) u IX P,(~,lc+-=, ~ -~)r,(#1c.._l, ..,) i=3 (9) where special symbol "#" indicates ttle word boundary marker.</S>
			<S sid="106" ssid="10">Character trigram probabilities are estimated from the training corpus by computing relative fre- quency of character bigram and trigram that appeared in words tagged as t. Pt(cilci-2, q - i ) = f,(c~l~-=, ?,-~) = Nt(ci_2, Ci_l, ci) N,(c~_.~, i) (lO) where Nt(ci_2,ci_~,ci) is tile total number of times character trigram ci_2ci_~el appears in words tagged as t in the training corpus.</S>
			<S sid="107" ssid="11">Note that the character trigram probabilities reflect the frequency of word to- kens in tile training corpus.</S>
			<S sid="108" ssid="12">Since there are more than 3,000 characters in Japanese, trigram probabilities are smoothed by interpolated estimation to cope with the sparse-data problem.</S>
			<S sid="109" ssid="13">It is ideal to make this character trigram model for all open clmss categories, llowever, the amount of train- ing data is too small for low frequency categories if we divide it by part of speech tags.</S>
			<S sid="110" ssid="14">Therefore, we made trigram models only for tile 4 most frequent parts of speech that are open categories and have no conju~ gation.</S>
			<S sid="111" ssid="15">They are common noun, proper noun, sahen no/ln5~ and nun lera l . &gt; (est imate-paxt-of-spoech ~ ) ; Hiyako Hotel ( C ~ \ ] 2.7621915641723623E-7) (~f~/~ 6.3406095003694205E-9) (~ l~\ ] 5,840424519473811E-19) (~(~ 5.7364195413101E-29)) &gt; (est imate-part-of-speech ~ I 9 9 4 ) ((~()~ 1,8053860295767367E-6) (~M o.s1224s6sls404~zE-17) (~-Jdrl:~"~%~ 2.288684007246524E-17) (~,~ 7.sos~s3~aso211e-20)) ; p roper noun ; common noun ; sa_han noun ; numeral ; numeral ; proper noun ; common noun ; sahen noun Figure 2: N-best Tags for Unknown Words Figure 2 show two examples of part of speech estima- tion for unknown words.</S>
			<S sid="112" ssid="16">Each trigram model returns a probability if the input string is a word belonging to the category.</S>
			<S sid="113" ssid="17">In both examples, the correct category has the largest probability.</S>
			<S sid="114" ssid="18">&gt; (get -lef tmo st-subst riags-uit h-word-model ( ( i~ 4) -~M 2.519457'597358691E-7) (~ ~;~f:~"~j~ 2.3449215070189967E-8) (~ ~tlfj/~l~i\] 7.02439907471337451{-9) (~\]i,~, '1 2.375650975098567E-9) (',l~J~ "4.)'~;'~$il 5.706S'(4990251415E-IO) (t~.~ '~j~cj~,\] 4.735628004876359E-13) (~,~ ~$~1 8.9289423481071831{-14) ( i~b ~)'~,~i l 7.266613344265452E-14) (~1~,~ \[~i~ 6.866d9949613207E-16) ( , l~b 'RlJlIiG,~I 2.45302390s251351sE-17)) Figure 3: N-Best Word lIylmtheses Figure 3 shows the N-best word hypotheses gener- ated by using tile character trigram models.</S>
			<S sid="115" ssid="19">A word hypothesis is a list of word boundary, part of speech assignment, and word probability that matches tile left- most substrings starting at a given position in tile input sentence.</S>
			<S sid="116" ssid="20">In the forward search, to handle unknown words, word hypotheses are generated at every posi- tion in addition to the ones generated by the function leftmost-subs~;r ings, which are the words found ill tile dictionary, llowever, ill our system, we limited the ntunl)er of word hyl)otheses generated at each position to 10, for efficiency reasons.</S>
			<S sid="117" ssid="21">aA noun tlmt can be used a~s a verb when it is followed by a forlna,\] verb "s~tr~t", 204</S>
	</SECTION>
	<SECTION title="Eva luat ion  Measures. " number="5">
			<S sid="118" ssid="1">We applied the performance measures for English parsers \[1\] to Japanese morphological analyzers.</S>
			<S sid="119" ssid="2">The basic idea is that morphological nalysis for a sentence can be thought of as a set of labeled brackets, where a bracket corresponds to word segmentation and its la-.</S>
			<S sid="120" ssid="3">bel corresponds to part of speech.</S>
			<S sid="121" ssid="4">We then compare the brackets contained in the system's output to the brackets contained in the standard analysis.</S>
			<S sid="122" ssid="5">For the N-best candidate, we will make the union of t\],e brack- ets contained in each candidate, and compare thenr to the brackets in the standard.</S>
			<S sid="123" ssid="6">For comparison, we court{, the number of I)rackcts in the standard data (Std), the number of brackets in the system output (Sys), and the nunlber of match- ing brackets (M).</S>
			<S sid="124" ssid="7">We then calculate the nleasurcs of recall (= M/Std) and precision (= M/Sys).</S>
			<S sid="125" ssid="8">We also connt the number of crossings, which is tile mmtber of c,'mes where a bracketed sequence from the standard data overlaps a bracketed sequence from tile system output, but neither sequence is completely coutained in the other.</S>
			<S sid="126" ssid="9">We defined two equaiity criteria of brackets for counting tim number of matching brackets.</S>
			<S sid="127" ssid="10">Two brack- ets are unlabeled-bracket-equal if the boundaries of the two brackets are tile same.</S>
			<S sid="128" ssid="11">Two brackets are labeled- bracket.equal if the labels of the brackets ark the same in addition to unlabeled-I)racket-equal.</S>
			<S sid="129" ssid="12">In comparing the consistency of the word segmentations of two brack- clings, wllich we call structure-consistency, we count the measures (recall, precision, crossings) by unlabeled- bracket-equal.</S>
			<S sid="130" ssid="13">In comparing the consistency of part of speech assignment in addition to word segmenta- tion, which we call label-consistency, we couut them by labeled-bracket-equal.</S>
			<S sid="131" ssid="14">-31.90894138309038 -38 . S9433~3fi658235fi ~b/tRllDll~iil-ill!lll,Til~l t-J-/lJ/ltlDil, l . l~k o I i fd~) -43, I0367483fi46801 Figure 4: N-Best Morphological Analysis hypotheses For example, Figure 4 shows a sample of N-hest anal- ysls hypotheses, where the first candidate is the correct analysis a. For the second candhlate, since there are !) })rackets in tim correct data (Std=9), 11 brackets in the second candidate (Sys=l l ) , and 8 nlatciiing brackets (M=8), tile recall and precision with respect to label consistency are 8/9 and 8/11, respectively.</S>
			<S sid="132" ssid="15">For the top 6Probabilities m'e in liiltura\] log b~se . two candidates, since tliere ;ire 12 distinct brackets in tile systems otll.litlt and 9 Inatehing brackets, tile re- call and precision with respect o hal)el consistency are 9/9 aud 9/12, respeetiwqy.</S>
			<S sid="133" ssid="16">For the third candidate, since the correct data and the third candidate differ in just one part of Sl)eech tag, the recall and precision wittl respect o structure consistency are 9/9 and 9/9, respectiw&gt;ly.</S>
	</SECTION>
	<SECTION title="Exper iment. " number="6">
			<S sid="134" ssid="1">Table 2: The aillount of training and test data ~_~ trahling texts closed test open 10 o 0 Sentences / -1~5 " 10{i0 13899 Words ' 149059 13176 \[ Characters _ 267,122 \[ 9422~ 98997 We used the NI'I~ Dialogue Databaae\[5\] to train and test the proposed morphological nalysis method.</S>
			<S sid="135" ssid="2">It is a corpus of approxiumtely 800,000 words whose word segmentatio,l and part of speech tag assigmnent were laboriously performed by hand.</S>
			<S sid="136" ssid="3">In tiffs experilneut, we only used one fourth of the A'Ft~.</S>
			<S sid="137" ssid="4">Corl)us , a portion of the keyl)oard dialogues in the conference registration domain.</S>
			<S sid="138" ssid="5">First, we selected 1,000 test sentences for all open test, arid used I.he others for training.</S>
			<S sid="139" ssid="6">Tile corpus was divided into 90% R)r training and 10% for test- ing.</S>
			<S sid="140" ssid="7">We then selected 1,000 sentences from tile traiu- ing set and used them for a closed test.</S>
			<S sid="141" ssid="8">The number of sentences, words, and characters for each test set and training texts are shown iu 'Pable 2.</S>
			<S sid="142" ssid="9">The training texts contained 6580 word types and 6945 tag trigram types.</S>
			<S sid="143" ssid="10">There were 247 unknown word types and 213 unknown tag trigram types in tim open test senteuces.</S>
			<S sid="144" ssid="11">Thus, both part of speech tri- gralrl l)robabilities alld word output probabilities must be snioothed to handle open texts.</S>
			<S sid="145" ssid="12">Table 3: Perccld.;ige of words correctly segmented and tagged: raw part o\[' speech bigram aud trigrmn I 2 I 98'{l% I 8 9 . 7 ' ~ ~ \[90.7% \[ 0.007 \[ I :~\[ os.,~:~ \[ 8 a . ~ . s ~ \] 84.a% \[ o.m2 \] I '~ 9a.'2% I 7 s . ~ / o I 79.6% I o.o15 I k 5 I &lt;~l''i''i?</S>
			<S sid="146" ssid="13">I &gt; i n ' / ~ % I r(~.o~ I o.o~s_l First, as a I)reliminary experiment, we compared tile perforn)ances of part of speech bigram and trigram.</S>
			<S sid="147" ssid="14">Table 3 shows the percentages of words correctly seg- mented and tagged, tested on the closed test sentences.</S>
			<S sid="148" ssid="15">The trigram model achiew;d 97.5% recall and 97.8% precision flu" the top candidate, while tile bigram model achiew.'d 96.2% recall and 96.6% precision.</S>
			<S sid="149" ssid="16">Although both tagging models sllow very high l)erformanee, tile 20,5 trigram model outperformed tile bigram model in every metric.</S>
			<S sid="150" ssid="17">We then tested the proposed system, which uses smoothed part of speech trigram with word model, on the open test sentences.</S>
			<S sid="151" ssid="18">Table 4 shows tile percentages of words correctly segmented and tagged.</S>
			<S sid="152" ssid="19">In Table 4, label consistency 2 represents the accuracy of segmen- tation and tagging ignoring the difference in conjuga- tion form.</S>
			<S sid="153" ssid="20">For open texts, tile morphological nalyzer achieved 95.1% recall and 94.6% precision for the top candidate, and 97.8% recall and 73.2% precision for the 5 best candidates.</S>
			<S sid="154" ssid="21">This performance is very encouraging, and is comparable to the state-of-the-art stochastic tagger for English \[2-4, 10, 11\].</S>
			<S sid="155" ssid="22">Since the segmentation accuracy of the proposed sys- tem is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphologi- cal analysis accuracy, it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model.</S>
			<S sid="156" ssid="23">We find a fair num- ber of tagging errors happened in conjugation forms.</S>
			<S sid="157" ssid="24">We assume that this is caused by the fact that the Japanese tag set used in tile ATR.</S>
			<S sid="158" ssid="25">Corpus is not de- tailed enough to capture the complicated Japanese verb morphology.</S>
			<S sid="159" ssid="26">100 95 90 $5 80q 75 90 65 60 Hogpho loq lca l Ana ly l t s Accuracy fo r N - I les t Sentences r iw t r tq ram (c losed ce :~t) -a-.-- raw b i t / tam , a ~ t , ? ~ a~othed t r t~ram wi th Iopen text ) -o , . . Imoothed t r lq ram wi word a lo t le l l opes te~t ) -~ .... raw m wi th word moOel l apen text} ~.</S>
			<S sid="160" ssid="27">- iraw t r r l ra w i thout word nloc~el lopes text ) -.12-::-.</S>
			<S sid="161" ssid="28">I I , I 2 3 4 R ink Figure 5: Tile percentage of sentences correctly seg- mented and tagged.</S>
			<S sid="162" ssid="29">Figure 5 shows tile percentage of sentences (not words) correctly segmented and tagged.</S>
			<S sid="163" ssid="30">For open texts, the sentence accuracy of the raw part of speech trigram without word model is 62.7% for the top candidate and 70.4% for the top-5, while that of smoothed trigram with word model is 66.9% for the top and 80.3% for the top-5.</S>
			<S sid="164" ssid="31">We can see that, by smoothing tile part ofsllecch trigram and by adding word model to handle unknown words, the accuracy and robustness of the morpholog- ical analyzer is significantly improved.</S>
			<S sid="165" ssid="32">Ilowever, tile sentence accuracy for closed texts is still significantly better that that for ol)en texts.</S>
			<S sid="166" ssid="33">It is clear that more research as to be done on the smoothing problem.</S>
	</SECTION>
	<SECTION title="Discuss ion. " number="7">
			<S sid="167" ssid="1">Morphological analysis is an important practical prob- lem with potential apl)lication in many areas including kana-to-kanji conversion 7, speech recognition, charac- ter recognition, speech synthesis, text revision support, information retrieval, and machine translation.</S>
			<S sid="168" ssid="2">Most conventional Japanese morphological nalyzers use rule-based heuristic searches.</S>
			<S sid="169" ssid="3">They usually use a connectivity rnatrix (part-of-sl)eech-pair grammar) ,as the language model.</S>
			<S sid="170" ssid="4">To rank the morphological nal- ysis hypotheses, they usually use heuristics uch as Longest Match Method or Least Bunsetsu's Number Method \[16\].</S>
			<S sid="171" ssid="5">There are some statistically-based approaches to Japanese morphological nalysis.</S>
			<S sid="172" ssid="6">The tagging models previously used are either part of speech I)igram \[9, 14\] or Character-based IIMM \[12\].</S>
			<S sid="173" ssid="7">Both hem'istic-based and statistically-based ap- proaches use t.he Minimum Connective-Cost Method \[7\], which is a linear time dynamic programming algo- rithm that finds the morphological hypothesis that has tile minimal connective cost (i.e. bigram-ba~sed cost) as derived by certain criteria.</S>
			<S sid="174" ssid="8">q'o handle unknown words, most Japanese morpho- logical analyzers u,~e character type heuristics \[17\], which is "a string of the same character type is likely to constitute a word".</S>
			<S sid="175" ssid="9">There is one stochastic approach that uses bigram of word formation unit \[13\].</S>
			<S sid="176" ssid="10">tlowever, it does not learn probabilities from training texts, but learns them fi'om machine readable dictionaries, and the model is not incorporated in working morphologi- cal analyzers, as fitr as the author knows.</S>
			<S sid="177" ssid="11">The unique features of the proposed Japanese mor- phological analyzer is that it can find tile exact N most likely hyl)otheses using part of speech trigram, and it can handle unlmown words using character trigram.</S>
			<S sid="178" ssid="12">The algoril.hm can naturally be extended to handle any higher order Markov models.</S>
			<S sid="179" ssid="13">Moreover, it can nat- nrally be extended to handle lattice-style input that is often used as t.he output of speech recognition and character ecognition systems, by extending the func- tion ( le f tmost -subat r inga) so as to return a list of words in the dictionary that matches the substrings in tile input lattice stm'ting at the specified p(xqition.</S>
			<S sid="180" ssid="14">For future wot'k, we have to study the most effective way of generating word hypotheses that can handh.</S>
			<S sid="181" ssid="15">un: known words.</S>
			<S sid="182" ssid="16">Currently, we are limiting the number of word hypotheses to reduce ambiguity at tile cost of ac- curacy.</S>
			<S sid="183" ssid="17">We have also to study tile word model for open categories thai, have conjugation, because the training 7Kana- to -kan j i convers ion i s a pop~alar J I Lpanese input method on computer using ASCII keyboard.</S>
			<S sid="184" ssid="18">Phonetic tranHerip- tion by Roams (ASCII) characters are input and converted fir, st to the Japanese syllabary hiragana which is then converted to orthographic trm~scrlption ncluding Chinese character kanjl.</S>
			<S sid="185" ssid="19">206 Table 4: The percentage of words correctly segmented and tagged: smoothed trigram with word model smoothed trigram with word model (open text) lal)el consistency recall precision crossings 95.1% 94.6% 0.013 96.5% 88.0% 0.023 97.3% 82.1% 0.031 97.6% 77.4% 0.0'16 97.8% 73.2% 0.061 label consistency 2 recall precision crossings 95.9% 95.4% (J.013 97.0% 90.3% 0.023 97.6% 85.1% 0.031 97.9% 80.7% 0.046 98.1% 77.1% 0.060 structure consistency recall precision 97.7% 97.2% 98.2% 94.4% 98.5% 91.7% 98.7% 89.6% 98.8% 87.9% crossings 0.013 0.022 0.029 0.044 0.056 data gets too small to make trigrams if we divide it by tags.</S>
			<S sid="186" ssid="20">We will probably have to tie some parameters to solve the insufficient data problem.</S>
			<S sid="187" ssid="21">Moreover, we have to study the method to adapt he system to a new domain.</S>
			<S sid="188" ssid="22">To develop an m~supervised learning method, like the forward-backward algorithm for IIMM, is an urgent goal, since we can't always ex- pect the availability of manually segmented and tagged data.</S>
			<S sid="189" ssid="23">We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm, but we don't know how to handle unknown words in this algorithm.</S>
	</SECTION>
	<SECTION title="Conc lus ion. " number="8">
</SECTION>
</ABSTRACT></PAPER>
