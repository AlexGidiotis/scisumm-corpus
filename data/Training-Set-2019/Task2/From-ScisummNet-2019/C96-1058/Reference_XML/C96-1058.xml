<PAPER>
	<S sid="0">Three New Probabilistic Models For Dependency Parsing: An Exploration</S><ABSTRACT>
	<SECTION title="In t roduct ion. " number="1">
			<S sid="1" ssid="1">In recent years, the statistical parsing community has begun to reach out; for syntactic formalisms that recognize the individuality of words, l,ink grammars (Sleator and 'Pemperley, 1991) and lex- icalized tree-adjoining ranunars (Schabes, 1992) have now received stochastic treatments.</S>
			<S sid="2" ssid="2">Other researchers, not wishing to abandon context-flee grammar (CI"G) but disillusioned with its lexica\] blind spot, have tried to re-parameterize stochas- tic CI"G in context-sensitive ways (Black et al, 1992) or have augmented the formalism with lex- ical headwords (Magerman, 1995; Collins, 11996).</S>
			<S sid="3" ssid="3">In this paper, we 1)resent a \[lexible l)robat)ilistic parser that simultaneously assigns both part-of- sl)eech tags and a bare-bones dependency struc- ture (illustrate.d in l!'igure 1).</S>
			<S sid="4" ssid="4">The choice o t 'a simple syntactic structure is deliberate: we would like to ask some basic questions about where h'x- ical relationships al)pear and how best, to exploit *This materia.l is based upon work supported un- der a National Science I%undation Graduate Fellow- ship, and has benefited greatly from discussions with Mike Collins, Dan M(:lame(l, Mitch Marcus and Ad- wait Ratnaparkhi.</S>
			<S sid="5" ssid="5">(a) Tile man in the coiner taught his dachsht , ld IO play gol f I';OS DT NN IN DT NN VBD PP.P$ NN TO VH NN /?</S>
			<S sid="6" ssid="6">man N ~..</S>
			<S sid="7" ssid="7">phty~ j J - y , .% (b) The ill __ ~ / .~dachshund It) gol f . ) f COfllel his file Figure 1: (a) A bare-l&gt;ones dependen(-y parse.</S>
			<S sid="8" ssid="8">\]'\]a&lt;:h word points to a single t)arent, the word it modities; the head of the sentence points to the EOS (end-of: sentence) ma.rk.</S>
			<S sid="9" ssid="9">Crossing links and cycles arc not al- lowed.</S>
			<S sid="10" ssid="10">(b) Constituent structure and sub(:ategoriza- tion may be highlighted by displaying the same de- pendencies as a lexical tree.</S>
			<S sid="11" ssid="11">them.</S>
			<S sid="12" ssid="12">It is uscflfl to look into thes0 basic ques- tions before trying to tine-tmm the performance of systems whose behavior is harder to understand.</S>
			<S sid="13" ssid="13">1 The main contribution of' the work is to I)ro- pose three distin('t, lexiealist hyl)otheses abou(.</S>
			<S sid="14" ssid="14">(,he probability space underlying seHl\]ence structure.</S>
			<S sid="15" ssid="15">We il\]ustrate how each hypothesis is (:xl)ressed in a depemteney framework, and how each can be used to guide our parser toward its favored so- lution.</S>
			<S sid="16" ssid="16">Finally, we point to experimental resul(;s that compare the three hypotheses' parsing per- formance on sentences fi:om the Wall ,b'treel dour- hal.</S>
			<S sid="17" ssid="17">\ ] 'he parser is trained on an annol,ated corpus; no hand-written grammar is required.</S>
	</SECTION>
	<SECTION title="Probabilistic Dependencies. " number="2">
</SECTION>
</ABSTRACT></PAPER>
