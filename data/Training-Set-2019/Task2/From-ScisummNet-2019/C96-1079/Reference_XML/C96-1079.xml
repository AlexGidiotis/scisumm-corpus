<PAPER>
	<S sid="0">Message Understanding Conference-6: A Brief History</S><ABSTRACT>
	<SECTION title="The MUC Evaluations. " number="1">
			<S sid="1" ssid="1">We have just completed the sixth in a series of Message Understanding Conferences, which have been organized by NRAD, the RDT&amp;E division of the Naval Command, Control and Ocean Surveil- lance Center (formerly NOSC, the Naval Ocean Systems Center) with the support of DARPA, the Defense Advanced Research Projects Agency.</S>
			<S sid="2" ssid="2">This paper looks briefly at the history of these Conferences and then examines the considerations which led to the structure of MUC-6} The Message Understanding Conferences were initiated by NOSC to assess and to foster research on the automated analysis of military messages containing textual information.</S>
			<S sid="3" ssid="3">Although called "conferences", the distinguishing characteristic of the MUCs are not the conferences themselves, but the evaluations to which participants must submit in order to be permitted to attend the conference.</S>
			<S sid="4" ssid="4">For each MUC, participating roups have been given sample messages and instructions on the type of information to be extracted, and have developed a system to process uch messages.</S>
			<S sid="5" ssid="5">Then, shortly before the conference, participants are given a set of test messages to be run through their system (without making any changes to the system); the output of each participant's system 1The full proceedings of the conference are to be distributed by Morgan Kaufmann Publishers, San Ma- teo, California; earlier MUC proeeedings~ for MUC-3, 4, and 5, are also available from Morgan Kaufmann.</S>
			<S sid="6" ssid="6">Beth Sundheim Naval Command, Control and Ocean Surveillance Center Research, Development, Test and Evaluation Division (NRaD) Code 44208 53140 Gatchell Road San Diego, CMifornia 92152-7420 sundhe im@poj ke . nosc . mi l is then evaluated against a manually-prepared an- swer key.</S>
			<S sid="7" ssid="7">The MUCs are remarkable in part because of the degree to which these evaluations have defined a prograin of research and development.</S>
			<S sid="8" ssid="8">DARPA has a number of information science and technol- ogy programs which are driven in large part, by regular evaluations.</S>
			<S sid="9" ssid="9">The MUCs are notable, how- ever, in that they in large part have shaped the research program in information extraction and brought it to its current state}</S>
	</SECTION>
	<SECTION title="Early History. " number="2">
			<S sid="10" ssid="1">MUC-1 (1987) was basically exploratory; each group designed its own format for recording the information in the document, and there was no formal evaluation.</S>
			<S sid="11" ssid="2">By MUC-2 (1989), the task had crystalized as one of template filling.</S>
			<S sid="12" ssid="3">One re- ceives a description of a class of events to be iden- tiffed in the text; for each of these events one must fill a template with information about the event.</S>
			<S sid="13" ssid="4">The template has slots for information about the event, such as the type of event, the agent, the time and place, the effect, etc. For MUC-2, the template had 10 slots.</S>
			<S sid="14" ssid="5">Both MUC-1 and MUC- 2 involved sanitized forms of military messages about naval sightings and engagements.</S>
			<S sid="15" ssid="6">The second MUC also worked out the details of the primary evaluation measures, recall and pre- cision.</S>
			<S sid="16" ssid="7">To present it in simplest terms, suppose the answer key has Nke~ filled slots; and that a system fills Neor,.~t slots correctly and Nin~or,,~t incorrectly (with some other slots possibly left un- filled).</S>
			<S sid="17" ssid="8">Then Ncorrect recall - Nkey 2There were, however, a number of individual re- scm'eh efforts in information extraction underway be- \[bre the first MUC, including the work on information formatting of medieM narrative by Sager at New York University; the formatting of naval equipment failure reports by Marsh at the Naval Research Laboratory; and the DBG work by Logieon for RADC.</S>
			<S sid="18" ssid="9">466 Nco,,;,.ect prec is ion = Ncorrect + Nincorrect For MUC-3 (1991), tile task shifted to reports of terrorist events ill Central and South Amer- ica, as reported in articles provided by the For- eign Broadcast Information Service, and the tem- plate becmne somewhat more complex (18 slots).</S>
			<S sid="19" ssid="10">This same task was used for MUC-4 (1992), with a further small increase in template complexity (24 slots).</S>
			<S sid="20" ssid="11">MUC-5 (1993), which was conducted as part of the Tipster program, a represented a substantial fllrther jump in task complexity.</S>
			<S sid="21" ssid="12">Two tasks were involved, international joint ventures and elec- tronic circuit fabrication, in two hmgnages, En- glish and Japanese.</S>
			<S sid="22" ssid="13">The joint venture task re- quired 11 templates with a total of 47 slots for the output double tile number of slots defined for MUC-4 and the task documentation was over 40 pages long.</S>
			<S sid="23" ssid="14">One innovation of MUC-5 was the use of a nested template structure.</S>
			<S sid="24" ssid="15">In earlier MUCs, each event had been represented as a single temi)late ? in effect, a single record in a data l)ase, with a large nuinber of attributes.</S>
			<S sid="25" ssid="16">This format proved awkward when an event had several participmlts (e.g., several victims of a terrorist attack) and one wanted to record a set of facts about each partic- ipant.</S>
			<S sid="26" ssid="17">This sort of information (:ould be ranch more easily recorded in the hierarchical structure introduced for MUC-5, in which there was a single template for an event, which pointed to a list of temI(lates, one for each particii)mlt in tile event;.</S>
			<S sid="27" ssid="18">4</S>
	</SECTION>
	<SECTION title="MUC-6 :  initial goals. " number="3">
			<S sid="28" ssid="1">1)ARI)A convened a meeting of Tipster partici- pants and government representatives in Decca&gt; bet' 1993 to define goals and tasks tot MUC-6) Among the goals which were identified were ? demonstrating taskqndependent component technologies of information extraction which would be immediately useflfl ? encouraging work to make information ex- tractioil systems in&lt;)re portable ? encouraging work on "deeper understanding" aTipster is a U.S. Govermnent program of research and development in the areas of inibrmation retrieval and information extraction.</S>
			<S sid="29" ssid="2">4In fact the MUC-5 structure wa~s much (nor(; com- plex, because there were separate temt)lates for prod- ucts, time, activities of organizations, etc. '~The representatives of the resear(:h community were Jim Cowie, lS(.alph Grishman (commit;tee chair), Jerry Hobbs, Paul Jacobs, Lea Schubert, Carl Weir, and Ralph Weischedel.</S>
			<S sid="30" ssid="3">The government people at- tending wcre George Doddington, Donna Harman, Boyan Onyshkevych, John Prangc, Bill Schultheis, and Beth Sundheim.</S>
			<S sid="31" ssid="4">Each of these can been seen in part as a reaction to the trends in the prior MUCs.</S>
			<S sid="32" ssid="5">The MUC-5 tasks, in particular, had been quite complex and a great effort had been invested by the government in preparing the training and test data and by the participants in adapting their systems for these tasks.</S>
			<S sid="33" ssid="6">Most participants worked on the tasks for 6 months; a few (the Tipster contractors) had been at work on the tasks tbr consi(lerably longer.</S>
			<S sid="34" ssid="7">While the performance of solne systems was quite impressive (the best got 57% recall, 64% precision overall, with 73% recall and 74% t)recision on the 4 "(:or(;" template types), tile question naturally arose as to whether there were many apl)lieations tbr which art investment of one or several develop- ers over half-&gt;year (or more) could be justified.</S>
			<S sid="35" ssid="8">Furthermore, while so much effort had been ex- pended, a large portion was specific to tire partic- ular tasks.</S>
			<S sid="36" ssid="9">It wasn't clear whether much progress was being made on the underlying technologies which would be needed for hetter understanding.</S>
			<S sid="37" ssid="10">To address these goals, the meeting formulated an ambitious menu of tasks for MUC-6, with the idea that individual participants could choose a subset of these tasks.</S>
			<S sid="38" ssid="11">We consider the three goals in the three sections below, and describe the tasks which were developed to address each goal.</S>
	</SECTION>
	<SECTION title="Short-term subtasks. " number="4">
			<S sid="39" ssid="1">The first goal was to identit~y, from the compo- nent technologies being developed for information extraction, flmctions which would be of 1)ractical use, would be largely domain indet)endent, and couhl in the near term be performed automatically with high ac('uracy.</S>
			<S sid="40" ssid="2">To meet this goal the con&gt; mittce developed the "named entity" task, which t(asically involves identifying the names of all the people, organizations, and geographic locations in a text.</S>
			<S sid="41" ssid="3">The final task specification, which also involved time, currency, and percentage xpressions, used SGML markup to identify the names in a text.</S>
			<S sid="42" ssid="4">Figure 1 shows a sample sentence with named en- tity annotations.</S>
			<S sid="43" ssid="5">The tag ENAMEX ("entity name expression") is used for both people and organiza- tion names; the tag NUNEX ( "numer ic expression") is used for currency and I)ercentages.</S>
	</SECTION>
	<SECTION title="Portability. " number="5">
</SECTION>
</ABSTRACT></PAPER>
