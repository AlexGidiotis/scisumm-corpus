<PAPER>
	<S sid="0">A Computational Model Of Language Performance: Data Oriented Parsing</S><ABSTRACT>
	<SECTION title="I I " number="1">
			<S sid="1" ssid="1">opened in July ACRES DE COLING-92?</S>
			<S sid="2" ssid="2">NANTES, 23-28 AOL'r 1992 8 5 6 I)ROC.</S>
			<S sid="3" ssid="3">(71: COLING-92, NANTES.</S>
			<S sid="4" ssid="4">AUG. 23-28, 1992 The Model In order to come to fomml definitions of p,'u'se and prefettedparse we tirst specify some basic notions.</S>
			<S sid="5" ssid="5">Labels We distinguish between file set of lexical l,lbels L and the ~t of non-lexical labels IV.</S>
			<S sid="6" ssid="6">Lexical labels represent words.</S>
			<S sid="7" ssid="7">Non-lexical l',fl~els represent syi~tactic and/or semantic mid/or i)ragnlalie infonnatiou, depending on file kind of corpns being used.</S>
			<S sid="8" ssid="8">We write J~ for l , u l~ SUing Given a set of hlbcls ~, a string is all u-tuple of elements of ~: (LI,...,L n) ~ ~u. All input string is ml nquple of elements of L: (l,t,._,Ln) ~ I, n. A Collckttellatio\[l ~ Gill big defined OI( sllil(gS US usual: (;l,...,b),~(c,...,ll) = (a,...,b,c,...,d).</S>
			<S sid="9" ssid="9">Tree Given a set of labels J~, the set of trees is defined as tile snmllest set Tree sucl~ that (1) i f I ,~ , then (l,,O)~Tree (2) i f L6"~, tl,..,,tneTi'ce., then (l,,(ll,...,tn))eT~ee For a set of trees 77cc over a ~t of labels ~, we define a function root.</S>
			<S sid="10" ssid="10">~/i-ee-9~ mid a tuuction le;tves: ~l?ee~L n by for n_&gt;O, root((L,(tl,...,tn))) = I, rot n&gt;O, le,~ves((L,(tt,...,l~t))) ~ l?,'lves(tl)*...</S>
			<S sid="11" ssid="11">~le~lves(tn) torn--O, leaves((L,O)) = (L) Corpus A corpus C is a multiset of trees, ill file ~nse that ally tree can occur zero, nile or more times.</S>
			<S sid="12" ssid="12">'File lt~tves of every tree in a corpus is ml element of Ln: it consfimtes the string of wo(ds of which that tree is the amdysis that seemed most appropriate for understanding tile striug ill the context in which it was uttered.</S>
			<S sid="13" ssid="13">Construction8 Ill order to define the Constowtions of a tree, we need two additional notions: Subffees and l~tttems, Snbtrees((L,(tl,...,t~))) = n \[(L,(tl,..,t~))} u (~ Snhtrees?ll)) i=~ Pattems((L,(t 1,..., In))) = {(L,O) 1 ty {(l,(ul,...,no.) ) / Vi~11,,l: nid~attenls(ti)l Constructions(T) = {t / 3beSubtrecs(1): teP,'tttenls(u)} We Slulll use tile lbllowing notation for a constnlction of a tree in a corpus: tee =tier ~nc()" tc(.imstmctionsO0.</S>
			<S sid="14" ssid="14">Example: consider tree T. qhe trees T 1 and T 2 m~ conslnletions of T, while '\['3 is not.</S>
			<S sid="15" ssid="15">T S TI " T T VP PP VP PP I v , I / x op~wwwi N ~ Ju~/ vi a po T 3~N,~ vp pp / p Compos i t ion If t and u are trees, such Ilmt tile le\[tmost non-lcxic;ll leMof t is equal to the mot of n, then tou is the tree that results from substituting this leaf in t by tree u. The i)mtial function o:'l~eexTree-47ivc is called ~mlposJtion.</S>
			<S sid="16" ssid="16">We will write (toU)ov ;Ls touov, and ill general (..((tloQ)o(~)o..)otn as tl~t2o(~o...otn.</S>
			<S sid="17" ssid="17">Exmuple: v t~ vp Np vp T VP pp t~a:l VP PP N Pr tr0 he I Palp~e Tree 7' is a par~ of input slring s with respect o C, iff leaves(7) = s and there m'e constructions tl,-.,tn e (~, such that 1' = tto...ot n. A tuple (fl,...,t n) of such constructions i said to generate par~ To f s. Note that different tuples of constructions Gm generate the .,vante parse.</S>
			<S sid="18" ssid="18">The set of par~s of s will( respect to C, P,'use(s,C), is given by I','use(S,C) = (1 eTive / lcaves(T)=s A 3tl,...,t . e C" ?-t lo.</S>
			<S sid="19" ssid="19">:tn\] "File set of tuples of C(nlstructions that generate a parse 7; "lbples(F,C), is given by luplcs('L(O = \[(tl,...,t~p / tl,...,t n ~" C A tlo...otn=T } Probability All input string can have several parses and every such parse can be generated by ~veral different c()mbinations ()f COllstruclious lrOlll tile corpus.</S>
			<S sid="20" ssid="20">What we are interested in, is, given an input string s, tile probability that arbiffury conlbinations of COllSIxuctions fro((I tile colpus generate a celtain prose 25 of s. Thus we are interested ill tile colldJtkmal prolXlbility of a pm'se 1)given s, with as probability space tile set of constructions of O'ees in the corpus.</S>
			<S sid="21" ssid="21">l,et '/~ be a parse of iupet string s, and supl~)se timt 15 can exhaustively be generated by k tuples of constructions: 1iqges(15,C) = ((tl l,..,thn), (t21,..,12n2) . . .</S>
			<S sid="22" ssid="22">(tkh..,tknt)}.</S>
			<S sid="23" ssid="23">Thell 7) occurs ill" (tll,...,tlnl) or (t21,...,ten 2) or .... or (Ikl,,.,tknk) occur, aud (thl,...,tlmt) (~culs iff thl and th2 and ....</S>
			<S sid="24" ssid="24">ACrl!s ol.: COLING-92.</S>
			<S sid="25" ssid="25">NAN rEs.</S>
			<S sid="26" ssid="26">23-28 AOt~:f 1992 8 5 7 l)mlc.</S>
			<S sid="27" ssid="27">OF COI,ING-92, NANTES, AU?}.</S>
			<S sid="28" ssid="28">23-28.</S>
			<S sid="29" ssid="29">I992 and t/mh Occur (hall,k\]).</S>
			<S sid="30" ssid="30">Thus the probability of Ti is given by P(T i) = P( (t l l r%.r3t lm) u ....</S>
			<S sid="31" ssid="31">~ (tglc3...r~tknvJ k ~p In shortened form: P(Ti) = P (u (el tlxl) p=l q=l Tile events tpq are no__L mutually exclusive, since conslructions can overlap, and can include other constructions.</S>
			<S sid="32" ssid="32">The formula for tim joint probability of events E i is given by: n n P(,'3 E i) = 11 l'OSilEi_l.,.h'l) i=l i=l Tile formula for the probability of combination of events E i (that are not independent) is given by (see e.g. \[Harris 1966\]): k P(L/ Ei) = X P(Ei) - X l'(ldi1~L'i2) + X P(h'it,'~Ei2~Ei3) i=l i i1&lt;i2 i1&lt;i2.~i3 - ....</S>
			<S sid="33" ssid="33">+/- P (E I~E2~ ... c~lS k) We will use Bayes' decomposition formula to derive the conditional probability of "1) given s. Let 7/~ and Tj be parses of s; the conditional probability of T i given s, is illen given by: P(Ti)P(sFI" i) P(r)P(srl~) V(7)ts) . . .</S>
			<S sid="34" ssid="34">P(s) z~j P(Tj)P(slTj) Since P(slTj) is 1 for all j, we may write P(T) P(Tils ) . . .</S>
			<S sid="35" ssid="35">~ p&lt;rj) A parse 1)of s with nmxinml conditional probability P(Tils) is called a preferred parse of s. Implementation Several different implementations of DOP are possible.</S>
			<S sid="36" ssid="36">In \[Scholtes 1992\] a neural net implementation f DOP is proposed, ltere we will show that conventional rule- based parsing strategies can be applied tn DOP, by converting constructions into rules.</S>
			<S sid="37" ssid="37">A construction Can be seen as a production rule, where the lefthand-side of the rule is constituted by the root of rite construction and the righthand-side is constituted by the leaves of the construction.</S>
			<S sid="38" ssid="38">The only exmt condition is that of every such rule its corresponding construction should be remembered in order to generate a parse-tree for the input string (by composing the constructions that correspond to the rules ilmt are applied).</S>
			<S sid="39" ssid="39">For a construction t, the corresponding production rule is given by root(t) ~ leaves(O In order to calculate the pteterredparse of an input string by maximizing the conditional probability, all parses with all possible tuples of constructions must be generated, which becomes highly inefficient.</S>
			<S sid="40" ssid="40">Often we are not interested in all parses of an alnbiguous input string, neither in their exact probabilities, but only in which parse is the preferred parse.</S>
			<S sid="41" ssid="41">Thus we would like to have a strategy fllat estimates file top of the probability hierarchy of parses.</S>
			<S sid="42" ssid="42">"llais call be achieved by using Monte Carlo techniques (see e.g. \[Hammersley 1964\]): we estimate the preferred parse by taking random samples frotn the space of possibilities.</S>
			<S sid="43" ssid="43">This will give us a more effective approach dian exhaustively calculating the probabilities.</S>
			<S sid="44" ssid="44">Discussion Although DOP has not yet been tested thoroughly 2, we call already predict sonic of its capabilities.</S>
			<S sid="45" ssid="45">In DOP, the probability of a parse depends on all tuples of coustructious that generate that parse.</S>
			<S sid="46" ssid="46">~lhe more different ways in which a parse can be generated, the lligher its probability.</S>
			<S sid="47" ssid="47">This implies that a parse which can (also) be generated by relatively large constructions is favoured over a parse which can only be generated by relatively small constructions.</S>
			<S sid="48" ssid="48">This means that prepositiotml plu'ase attxichments arid figures of speech can be processed adequately by I)OP.</S>
			<S sid="49" ssid="49">As 1o the problem of hmguage acquisition, this ntight seem problematic for DOP: with all "already analyzed corpus, only adult language behaviour can be simulated.</S>
			<S sid="50" ssid="50">The problem of language acquisition is itt our perspective the problem of the acquisition of an initial corpus, in which non-linguistic input and pragmatics should play na important role.</S>
			<S sid="51" ssid="51">An additional remark should be devoted here to formal granlmars and disambiguation.</S>
			<S sid="52" ssid="52">Much work has been done to extend rule-based granunars with selectional restrictions such that the explosion of ambiguities is constrabled considerably, llowever, to represent semantic and pragmatic onstraints i a very expensive task.</S>
			<S sid="53" ssid="53">No one has ever succeeded in doing so except in relatively small grammars.</S>
			<S sid="54" ssid="54">Furthermore, a basic question renmins as to whether it is possible to formally etlcode all of die syntactic, semantic alld pragmatic infomlation needed for disambiguation.</S>
			<S sid="55" ssid="55">In DOP, the additional infornmtion that one can draw from a corpus of hand-marked structural annotations i that one can by-pass the necessity for modelling world knowledge, since this will autonmtically enter into the disarnbiguation of structures by Imnd.</S>
			<S sid="56" ssid="56">Extracting constructions from these structures, and combining them in the most probable way, taking into account all possible statistical dependencies between them, preserves this world knowledge in the best possible way.</S>
			<S sid="57" ssid="57">In conclusion, it may be interesting to note that our idea of using past lallguage xperiences instead of rules, has much in cormnon with Stich's ideas about language (\[Stich 1971\]).</S>
			<S sid="58" ssid="58">lu Stich's view, judgements of gralnmaticality are not determined by applying a precompiled set of gratmuar rules, but rather have the character of a perceptual judgement on the question to what extent rite judged sentonce 'lotiks like' the sentences the language user has in his head as examples of granlmaticality.</S>
			<S sid="59" ssid="59">The cot)crete language xperiences of file past of a language user determine how a new utterance is processed; there is no evidence for file assumption that past language experiences are generalized into a consistent heory that defines the Corpus, built at the University of Nijmugen, and possibly the Penn Trcebm~k, built at the Umversity of Pennsylvania.</S>
			<S sid="60" ssid="60">AcrEs Dr: COLING-92.</S>
			<S sid="61" ssid="61">NANTES.</S>
			<S sid="62" ssid="62">23-28 AOt',q" 1992 8 S 8 PROC.</S>
			<S sid="63" ssid="63">OF COLING-92, NANTES, Aunt.</S>
			<S sid="64" ssid="64">23-28.</S>
			<S sid="65" ssid="65">1992 grammaticality and the structure of new utterances univocally.</S>
	</SECTION>
</ABSTRACT>
</PAPER>
