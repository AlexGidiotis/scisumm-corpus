<PAPER>
	<S sid="0">A Maximum Entropy Approach to Chinese Word Segmentation</S><ABSTRACT>
	<SECTION title="Chinese Word Segmenter. " number="1">
			<S sid="1" ssid="1">The Chinese word segmenter we built is similarto the maximum entropy word segmenter we em ployed in our previous work (Ng and Low, 2004).</S>
			<S sid="2" ssid="2">Our word segmenter uses a maximum entropy framework (Ratnaparkhi, 1998; Xue and Shen,2003) and is trained on manually segmented sen tences.</S>
			<S sid="3" ssid="3">It classifies each Chinese character given the features derived from its surrounding context.</S>
			<S sid="4" ssid="4">Each Chinese character can be assigned one of four possible boundary tags: s for a character thatoccurs as a single-character word, b for a charac ter that begins a multi-character (i.e., two or more characters) word, e for a character that ends a multi-character word, and m for a character that is neither the first nor last in a multi-character word.</S>
			<S sid="5" ssid="5">Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features.</S>
			<S sid="6" ssid="6">The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.</S>
			<S sid="7" ssid="7">Templates (a) ?</S>
			<S sid="8" ssid="8">(c) refer to a context of five characters (the current character and two characters to its left and right).</S>
			<S sid="9" ssid="9">C0 denotes the current character, Cn(C?n ) denotes the character n positions to the right (left) of thecurrent character.</S>
			<S sid="10" ssid="10">For example, given the charac ter sequence ?c?????, when considering the character C0 ???, C?2 denotes ?c?, C1C2 denotes ????, etc. The punctuation feature, Pu(C0), checks whether C0 is a punctuationsymbol (such as ???, ???, ?,?).</S>
			<S sid="11" ssid="11">For the type fea ture (e), four type classes are defined: numbers represent class 1, dates (???, ???, ?#?, the Chinese characters for ?day?, ?month?, ?year?, respectively) represent class 2, English letters represent class 3, and other characters represent class 4.</S>
			<S sid="12" ssid="12">For example, when considering the character ?#?</S>
			<S sid="13" ssid="13">in the character sequence ?? #SR?, the feature T (C?2) . . .</S>
			<S sid="14" ssid="14">T (C2) = 11243 1http://maxent.sourceforge.net/ 161 will be set to 1 (???</S>
			<S sid="15" ssid="15">is the Chinese character for ?9?</S>
			<S sid="16" ssid="16">and ??</S>
			<S sid="17" ssid="17">is the Chinese character for ?0?).</S>
			<S sid="18" ssid="18">Besides these basic features, we also made useof character normalization.</S>
			<S sid="19" ssid="19">We note that characters like punctuation symbols and Arabic dig its have different character codes in the ASCII, GB, and BIG5 encoding standard, although they mean the same thing.</S>
			<S sid="20" ssid="20">For example, comma ?,?</S>
			<S sid="21" ssid="21">is represented as the hexadecimal value 0x2c in ASCII, but as the hexadecimal value 0xa3ac in GB.</S>
			<S sid="22" ssid="22">In our segmenter, these different charactercodes are normalized and replaced by the corresponding character code in ASCII.</S>
			<S sid="23" ssid="23">Also, all Ara bic digits are replaced by the ASCII digit ?0?</S>
			<S sid="24" ssid="24">todenote any digit.</S>
			<S sid="25" ssid="25">Incorporating character normal ization enables our segmenter to be more robust against the use of different encodings to represent the same character.</S>
			<S sid="26" ssid="26">For all the experiments that we conducted, training was done with a feature cutoff of 2 and 100 iterations, except for the AS corpus which had a feature cutoff of 3.</S>
			<S sid="27" ssid="27">A major difficulty faced by a Chinese word segmenter is the presence of out-of-vocabulary (OOV) words.</S>
			<S sid="28" ssid="28">Segmenting a text with many OOVwords tends to result in lower accuracy.</S>
			<S sid="29" ssid="29">We ad dress the problem of OOV words in two ways: using an external dictionary containing a list of predefined words, and using additional training corpora which are not segmented according to the same segmentation standard.</S>
			<S sid="30" ssid="30">1.2 External Dictionary.</S>
			<S sid="31" ssid="31">If a sequence of characters in a sentence matches a word in an existing dictionary, it may be a clue that the sequence of characters should be segmented as one word.</S>
			<S sid="32" ssid="32">We used an online dictionary from Peking University downloadable from the Internet2, consisting of about 108,000 words of length one to four characters.</S>
			<S sid="33" ssid="33">If there is some sequence of neighboring characters around C0 in the sentence that matches a word in this dictionary, then we greedily choose the longest such matching word W in the dictionary.</S>
			<S sid="34" ssid="34">Let t0 be the boundary tag of C0 in W , L the number of characters in W , and C1(C?1) be the character 2http://ccl.pku.edu.cn/doubtfire/Course/ Chinese%20Information%20Processing/Source Code/ Chapter 8/Lexicon full 2000.zip immediately following (preceding) C0 in the sentence.</S>
			<S sid="35" ssid="35">We then add the following features derived from the dictionary: (f) Lt0 (g) Cnt0(n = ?1, 0, 1) For example, consider the sentence ?c??</S>
			<S sid="36" ssid="36">When processing the current character C0 ???, we will attempt to match the following candidate sequences ???, ?c??, ????, ?c ???, ?????, ?c????, and ??????</S>
			<S sid="37" ssid="37">against existing words in our dictionary.</S>
			<S sid="38" ssid="38">Suppose both ????</S>
			<S sid="39" ssid="39">and ?c???</S>
			<S sid="40" ssid="40">are found in the dictionary.</S>
			<S sid="41" ssid="41">Then the longest matching word W chosen is ?c???, t0 is m, L is 3, C?1 is ?c?, and C1 is ???.</S>
			<S sid="42" ssid="42">1.3 Additional Training Corpora.</S>
			<S sid="43" ssid="43">The presence of different standards in Chinese word segmentation limits the amount of trainingcorpora available for the community, due to dif ferent organizations preparing training corpora intheir own standards.</S>
			<S sid="44" ssid="44">Indeed, if one uniform seg mentation standard were adopted, more training data would have been available, and the OOV problem could be significantly reduced.We observed that although different segmenta tion standards exist, the differences are limited, and many words are still segmented in the same way across two different segmentation standards.</S>
			<S sid="45" ssid="45">As such, in our work, we attempt to incorporatecorpora from other segmentation standards as ad ditional training data, to help reduce the OOV problem.</S>
			<S sid="46" ssid="46">Specifically, the steps taken are: 1.</S>
			<S sid="47" ssid="47">Perform training with maximum entropy.</S>
			<S sid="48" ssid="48">modeling using the original training corpusD0 annotated in a given segmentation stan dard.</S>
			<S sid="49" ssid="49">another corpus D annotated in a different segmentation standard.</S>
			<S sid="50" ssid="50">3.</S>
			<S sid="51" ssid="51">Suppose a Chinese character C in D is as-.</S>
			<S sid="52" ssid="52">signed a boundary tag t by the word seg menter with probability p. If t is identical to the boundary tag of C in the gold-standard 162 annotated corpus D, and p is less than some threshold ?, then C (with its surrounding context in D) is used as additional training data.</S>
			<S sid="53" ssid="53">4.</S>
			<S sid="54" ssid="54">Add all such characters C as additional train-.</S>
			<S sid="55" ssid="55">ing data to the original training corpus D0,and train a new word segmenter using the en larged training data.</S>
			<S sid="56" ssid="56">5.</S>
			<S sid="57" ssid="57">Evaluate the accuracy of the new word seg-.</S>
			<S sid="58" ssid="58">menter on the same test data annotated in the original segmentation standard of D0.</S>
			<S sid="59" ssid="59">For the current bakeoff, when training a wordsegmenter on a particular training corpus, the ad ditional training corpora are all the three corpora in the other segmentation standards.</S>
			<S sid="60" ssid="60">For example,when training a word segmenter for the AS cor pus, the additional training corpora are CITYU,MSR, and PKU.</S>
			<S sid="61" ssid="61">The necessary character encoding conversion between GB and BIG5 is per formed, and the probability threshold ? is set to 0.8.</S>
			<S sid="62" ssid="62">We found from our experiments that setting.</S>
			<S sid="63" ssid="63">to a higher value did not further improve seg mentation accuracy, but would instead increase the training set size and incur longer training time.</S>
	</SECTION>
	<SECTION title="Testing. " number="2">
			<S sid="64" ssid="1">During testing, the probability of a boundary tag sequence assignment t1 . . .</S>
			<S sid="65" ssid="2">tn given a character sequence C1 . . .</S>
			<S sid="66" ssid="3">Cn is determined by using themaximum entropy classifier to compute the prob ability that a boundary tag ti is assigned to eachindividual character Ci.</S>
			<S sid="67" ssid="4">If we were to just as sign each character the boundary tag with thehighest probability, it is possible that the clas sifier produces a sequence of invalid tags (e.g.,m followed by s).</S>
			<S sid="68" ssid="5">To eliminate such possibil ities, we implemented a dynamic programming algorithm which considers only valid boundary tag sequences given an input character sequence.At each character position i, the algorithm considers each last word candidate ending at posi tion i and consisting of K characters in length(K = 1, . . .</S>
			<S sid="69" ssid="6">, 20 in our experiments).</S>
			<S sid="70" ssid="7">To deter mine the boundary tag assignment to the last word W with K characters, the first character of W is assigned boundary tag b, the last character of W is assigned tag e, and the intervening characters Corpus R P F ROOV RIV AS 0.962 0.950 0.956 0.684 0.975 CITYU 0.967 0.956 0.962 0.806 0.980 MSR 0.969 0.968 0.968 0.736 0.975 PKU 0.968 0.969 0.969 0.838 0.976 Table 1: Our official SIGHAN bakeoff results are assigned tag m.</S>
			<S sid="71" ssid="8">(If W is a single-character word, then the single character is assigned tag s).</S>
			<S sid="72" ssid="9">In this way, the dynamic programming algorithm only considers valid tag sequences.After word segmentation is done by the maxi mum entropy classifier, a post-processing step is applied to correct inconsistently segmented words made up of 3 or more characters.</S>
			<S sid="73" ssid="10">A word W isdefined to be inconsistently segmented if the con catenation of 2 to 6 consecutive words elsewhere in the segmented output document matches W . In the post-processing step, the segmentation of the characters of these consecutive words is changed so that they are segmented as a single word.</S>
			<S sid="74" ssid="11">To illustrate, if the concatenation of 2 consecutivewords ???</S>
			<S sid="75" ssid="12">and ??n? in the segmented out put document matches another word ???n?, then the 2 consecutive words ???</S>
			<S sid="76" ssid="13">and ??n? will be re-segmented as a single word ???</S>
			<S sid="77" ssid="14">n?.</S>
	</SECTION>
	<SECTION title="Evaluation Results. " number="3">
			<S sid="78" ssid="1">We evaluated our Chinese word segmenter in the open track, on all 4 corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), andPeking University (PKU).</S>
			<S sid="79" ssid="2">Table 1 shows our of ficial SIGHAN bakeoff results.</S>
			<S sid="80" ssid="3">The columns R,P, and F show the recall, precision, and F mea sure, respectively.</S>
			<S sid="81" ssid="4">The columns ROOV and RIV show the recall on out-of-vocabulary words and in-vocabulary words, respectively.</S>
			<S sid="82" ssid="5">Our Chinese word segmenter which participated in the bakeoff was trained with the basic features (Section 1.1),and made use of the external dictionary (Section 1.2) and additional training corpora (Section 1.3).</S>
			<S sid="83" ssid="6">Our word segmenter achieved the high est F measure for AS, CITYU, and PKU, and the second highest for MSR.</S>
			<S sid="84" ssid="7">After the release of the official bakeoff results, 163 Corpus V1 V2 V3 V4 AS 0.953 0.955 0.956 0.956 CITYU 0.950 0.960 0.961 0.962 MSR 0.960 0.968 0.963 0.968 PKU 0.948 0.965 0.956 0.969Table 2: Word segmentation accuracy (F mea sure) of different versions of our word segmenter we ran a series of experiments to determine thecontribution of each component of our word seg menter, using the official scorer and test sets with gold-standard segmentations.</S>
			<S sid="85" ssid="8">Version V1 used only the basic features (Section 1.1); Version V2used the basic features and additional features de rived from our external dictionary (Section 1.2);Version V3 used the basic features but with additional training corpora (Section 1.3); and Version V4 is our official submitted version combining basic features, external dictionary, and addi tional training corpora.</S>
			<S sid="86" ssid="9">Table 2 shows the wordsegmentation accuracy (F measure) of the differ ent versions of our word segmenter, when tested on the official test sets of the four corpora.</S>
			<S sid="87" ssid="10">The results indicate that the use of external dictionary increases segmentation accuracy.</S>
			<S sid="88" ssid="11">Similarly, theuse of additional training corpora of different seg mentation standards also increases segmentation accuracy.</S>
	</SECTION>
	<SECTION title="Conclusion. " number="4">
			<S sid="89" ssid="1">Using a maximum entropy approach, our Chinese word segmenter achieves state-of-the-art ac curacy, when evaluated on all four corpora in the open track of the Second International ChineseWord Segmentation Bakeoff.</S>
			<S sid="90" ssid="2">The use of an exter nal dictionary and additional training corpora of different segmentation standards helps to further improve segmentation accuracy.</S>
			<S sid="91" ssid="3">Acknowledgements This research is partially supported by a researchgrant R252-000-125-112 from National Univer sity of Singapore Academic Research Fund, as well as the Singapore-MIT Alliance.</S>
	</SECTION>
</ABSTRACT>
</PAPER>
