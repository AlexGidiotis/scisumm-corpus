"Citance Number","Reference Article","Citing Article","Citation Marker Offset","Citation Marker","Citation Offset","Citation Text","Citation Text Clean","Reference Offset","Reference Text","Discourse Facet"
1,"J01-2004","W05-0104",0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",,"<S sid=""15"" ssid=""3"">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>",
2,"J01-2004","P08-1013",0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)","Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition",,"<S sid=""248"" ssid=""4"">In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.</S>",
4,"J01-2004","P04-1015",0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank",,"<S sid=""25"" ssid=""13"">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>",
5,"J01-2004","P04-1015",0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",,"<S sid=""367"" ssid=""123"">Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40M words, with a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.</S>",
6,"J01-2004","P04-1015",0,"2001a",0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",,"<S sid=""294"" ssid=""50"">Figure 6 shows the observed time at our standard base beam of 10-11 with the full conditioning regimen, alongside an approximation of the reported observed (linear) time in Ratnaparkhi (1997).</S>",
7,"J01-2004","P04-1015",0,"2001a",0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",,"<S sid=""22"" ssid=""10"">A left-toright parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures.</S>",
9,"J01-2004","P04-1015",0,"2001a",0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",,"<S sid=""145"" ssid=""49"">Examples of this are bilexical grammars&#8212;such as Eisner and Satta (1999), Charniak (1997), Collins (1997)&#8212;where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item.</S>",
10,"J01-2004","P05-1022",0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",,"<S sid=""77"" ssid=""35"">See that paper for more discussion of the benefits of Two parse trees: (a) a complete left-factored parse tree with epsilon productions and an explicit stop symbol; and (b) a partial left-factored parse tree. factorization for top-down and left-corner parsing.</S>",
11,"J01-2004","P05-1022",0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)",,"<S sid=""260"" ssid=""16"">We will call the manual parse GOLD and the parse that the parser returns TEST.</S>",
12,"J01-2004","P05-1022",0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",,"<S sid=""155"" ssid=""59"">This perspective on conditioning production probabilities makes it easy to see that, in essence, by conditioning these probabilities, we are growing the state space.</S>",
13,"J01-2004","P04-1006",0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)",,"<S sid=""160"" ssid=""64"">Each of these functions is an algorithm for walking the provided tree and returning a value.</S>",
14,"J01-2004","P05-1063",0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",,"<S sid=""106"" ssid=""10"">String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language models.</S>",
15,"J01-2004","W10-2009",0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)",,"<S sid=""2"" ssid=""2"">The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>",
17,"J01-2004","D09-1034",0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",,"<S sid=""5"" ssid=""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>",
18,"J01-2004","D09-1034",0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",,"<S sid=""310"" ssid=""66"">This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;\', the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT\'\', that is, the total probability mass represented by the priority queues is monotonically decreasing.</S>",
19,"J01-2004","D09-1034",0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",,"<S sid=""140"" ssid=""44"">Next we will outline our conditional probability model over rules in the PCFG, followed by a presentation of the top-down parsing algorithm.</S>",
20,"J01-2004","D09-1034",0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures",,"<S sid=""128"" ssid=""32"">More precisely, for a beam of derivations D, where hod and hid are the lexical heads of the top two entries on the stack of d. Figure 3 gives a partial tree representation of a potential derivation state for the string &amp;quot;the dog chased the cat with spots&amp;quot;, at the point when the word &amp;quot;with&amp;quot; is to be predicted.</S>",
