"Citance Number","Reference Article","Citing Article","Citation Marker Offset","Citation Marker","Citation Offset","Citation Text","Citation Text Clean","Reference Offset","Reference Text","Discourse Facet"
1,"W99-0613","N01-1023",0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",,"<S sid=""119"" ssid=""52"">Each vertex within a connected component must have the same label &#8212; in the binary classification case, we need a single labeled example to identify which component should get which label.</S>",
2,"W99-0613","N01-1023",0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)",,"<S sid=""171"" ssid=""38"">We define the following function: If Zco is small, then it follows that the two classifiers must have a low error rate on the labeled examples, and that they also must give the same label on a large number of unlabeled instances.</S>",
3,"W99-0613","W03-1509",0,"Collins and Singer 1999",0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",,"<S sid=""7"" ssid=""1"">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>",
4,"W99-0613","C02-1154",0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus",,"<S sid=""105"" ssid=""38"">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S>",
5,"W99-0613","C02-1154",0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify",,"<S sid=""236"" ssid=""3"">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>",
6,"W99-0613","W06-2204",0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",,"<S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>",
8,"W99-0613","W03-1022",0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",,"<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>",
9,"W99-0613","E09-1018",0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",,"<S sid=""61"" ssid=""15"">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>",
11,"W99-0613","W07-1712",0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",,"<S sid=""146"" ssid=""13"">The distribution specifies the relative weight, or importance, of each example &#8212; typically, the weak learner will attempt to minimize the weighted error on the training set, where the distribution specifies the weights.</S>",
12,"W99-0613","W09-2208",0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",,"<S sid=""6"" ssid=""6"">The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).</S>\n  '",
13,"W99-0613","W06-2207",0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",,"<S sid=""254"" ssid=""5"">Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.</S>",
15,"W99-0613","W06-2207",0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",,"<S sid=""85"" ssid=""18"">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>",
16,"W99-0613","P12-1065",0,1999,0,"We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ ?|? f |+ L? (1) where ? is a smoothing constant","We use Collins and Singer (1999) for our exact specification of Yarowsky",,"<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S>\n  '",
