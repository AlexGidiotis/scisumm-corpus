Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Text,Discourse Facet
2,W06-3114,D07-1092,0.0,0.0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",['6'],"<S sid =""6"" ssid = ""4"">English was again paired with German, French, and Spanish.</S>",Method_Citation
10,W06-3114,D07-1030,0.0,0.0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test",['49'],"<S sid =""49"" ssid = ""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>",Method_Citation
11,W06-3114,D07-1030,0.0,0.0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",['68'],"<S sid =""68"" ssid = ""7"">We asked participants to each judge 200â€“300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>",Results_Citation
11,W06-3114,D07-1030,0.0,0.0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",['123'],"<S sid =""123"" ssid = ""16"">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>",Method_Citation
13,W06-3114,W11-1002,0.0,0.0,"Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality",['39'],"<S sid =""39"" ssid = ""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>",Results_Citation
14,W06-3114,D07-1091,0.0,0.0,"The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",['9'],"<S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S>",Method_Citation
14,W06-3114,D07-1091,0.0,0.0,"The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",['16'],"<S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>",Method_Citation
14,W06-3114,D07-1091,0.0,0.0,"The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",['69'],"<S sid =""69"" ssid = ""8"">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S>",Method_Citation
14,W06-3114,D07-1091,0.0,0.0,"The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",['126'],"<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",Method_Citation
18,W06-3114,E12-3010,0.0,0.0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",['175'],"<S sid =""175"" ssid = ""6"">Replacing this with an ranked evaluation seems to be more suitable.</S>",Impact_Citation
19,W06-3114,W09-0402,0.0,0.0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",['6'],"<S sid =""6"" ssid = ""4"">English was again paired with German, French, and Spanish.</S>",Method_Citation
