Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0.0,0.0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)",['10'],"<S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S>",Method_Citation
2,D10-1044,P12-1048,0.0,0.0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",['65'],"<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>",Results_Citation
2,D10-1044,P12-1048,0.0,0.0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)",['132'],"<S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>",Method_Citation
4,D10-1044,P14-2093,0.0,0.0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",['24'],"<S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>",Method_Citation
4,D10-1044,P14-2093,0.0,0.0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models",['62'],"<S sid =""62"" ssid = ""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>",Method_Citation
6,D10-1044,E12-1055,0.0,0.0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs",['26'],"<S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S>",Method_Citation
6,D10-1044,E12-1055,0.0,0.0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs",['65'],"<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>",Results_Citation
6,D10-1044,E12-1055,0.0,0.0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs",['67'],"<S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S>",Method_Citation
6,D10-1044,E12-1055,0.0,0.0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs",['68'],"<S sid =""68"" ssid = ""5"">First, we learn weights on individual phrase pairs rather than sentences.</S>",Method_Citation
6,D10-1044,E12-1055,0.0,0.0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs",['152'],"<S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>",Method_Citation
10,D10-1044,P12-1099,0.0,0.0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT",['28'],"<S sid =""28"" ssid = ""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>",Method_Citation
11,D10-1044,P12-1099,0.0,0.0,Our technique for setting ? m is similar to that outlined in Foster et al (2010),['30'],"<S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only.</S>",Results_Citation
14,D10-1044,P12-1099,0.0,0.0,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,['22'],"<S sid =""22"" ssid = ""19"">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>",Method_Citation
14,D10-1044,P12-1099,0.0,0.0,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,['89'],"<S sid =""89"" ssid = ""26"">We used 22 features for the logistic weighting model, divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language, and one intended to capture similarity to the IN domain.</S>",Method_Citation
14,D10-1044,P12-1099,0.0,0.0,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,['153'],"<S sid =""153"" ssid = ""10"">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>",Method_Citation
15,D10-1044,P13-1126,0.0,0.0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs",['30'],"<S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only.</S>",Results_Citation
18,D10-1044,D11-1033,0.0,0.0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",['65'],"<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>",Results_Citation
18,D10-1044,D11-1033,0.0,0.0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences",['68'],"<S sid =""68"" ssid = ""5"">First, we learn weights on individual phrase pairs rather than sentences.</S>",Method_Citation
