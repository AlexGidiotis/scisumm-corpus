Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Text,Discourse Facet
2,W11-2123,P14-2022,0.0,0.0,"The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",['223'],"<S sid =""223"" ssid = ""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>",Method_Citation
10,W11-2123,W12-3134,0.0,0.0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,['92'],"<S sid =""92"" ssid = ""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S>",Method_Citation
10,W11-2123,W12-3134,0.0,0.0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,['283'],"<S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>",Method_Citation
15,W11-2123,E12-1083,0.0,0.0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",['103'],"<S sid =""103"" ssid = ""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>",Method_Citation
15,W11-2123,E12-1083,0.0,0.0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",['283'],"<S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>",Method_Citation
17,W11-2123,D12-1108,0.0,0.0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",['52'],"<S sid =""52"" ssid = ""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>",Results_Citation
