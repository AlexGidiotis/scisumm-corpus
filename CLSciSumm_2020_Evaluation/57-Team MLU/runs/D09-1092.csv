Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Text,Discourse Facet
3,D09-1092,P11-2084,0.0,0.0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",['137'],"<S sid =""137"" ssid = ""86"">For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt, respectively.</S>",Method_Citation
3,D09-1092,P11-2084,0.0,0.0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",['138'],"<S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>",Method_Citation
5,D09-1092,D11-1086,0.0,0.0,"For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)",['148'],"<S sid =""148"" ssid = ""97"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>",Method_Citation
5,D09-1092,D11-1086,0.0,0.0,"For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)",['182'],"<S sid =""182"" ssid = ""16"">As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S>",Method_Citation
7,D09-1092,N12-1007,0.0,0.0,"Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",['79'],"<S sid =""79"" ssid = ""28"">For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['3'],"<S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['28'],"<S sid =""28"" ssid = ""4"">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['35'],"<S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['88'],"<S sid =""88"" ssid = ""37"">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['105'],"<S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>",Method_Citation
8,D09-1092,D10-1025,0.0,0.0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",['192'],"<S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>",Method_Citation
11,D09-1092,D10-1025,0.0,0.0,"The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",['155'],"<S sid =""155"" ssid = ""104"">Finally, for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>",Method_Citation
13,D09-1092,D10-1025,0.0,0.0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",['156'],"<S sid =""156"" ssid = ""105"">We use both Jensen-Shannon divergence and cosine distance.</S>",Method_Citation
16,D09-1092,W12-3117,0.0,0.0,"Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",['35'],"<S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>",Method_Citation
17,D09-1092,W11-2133,0.0,0.0,"Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)",['35'],"<S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>",Method_Citation
