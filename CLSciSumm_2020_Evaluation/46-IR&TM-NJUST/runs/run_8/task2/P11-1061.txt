To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).The number of latent HMM states for each language in our experiments was set to the number of fine tags in the languageâ€™s treebank.While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.