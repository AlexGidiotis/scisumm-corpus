Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y.There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting.Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).Our learning algorithm alternates between (i) using the current parameters θ to generate the K-best set ˜ZL,θ(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.