The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.As noted in [5], that system is based upon a "tree-bank grammar" - a grammar read directly off the training corpus.The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].