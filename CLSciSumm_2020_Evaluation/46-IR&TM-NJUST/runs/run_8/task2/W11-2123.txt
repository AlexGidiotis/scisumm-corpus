We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.Given counts cn1 where e.g. c1 is the vocabulary size, total memory consumption, in bits, is Our PROBING data structure places all n-grams of the same order into a single giant hash table.We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.Then we ran binary search to determine the least amount of memory with which it would run.For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.