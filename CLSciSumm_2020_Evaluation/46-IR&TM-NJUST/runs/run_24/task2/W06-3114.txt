This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.The sign test checks, how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.The out-of-domain test set differs from the Europarl data in various ways.However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate â€” due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).