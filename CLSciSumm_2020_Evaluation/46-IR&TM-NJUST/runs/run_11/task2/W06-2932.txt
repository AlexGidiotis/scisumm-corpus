First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.This system is primarily based on the parsing models described by McDonald and Pereira (2006).In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.However, if we only allow projective parses, do not use morphological features and label edges with a simple atomic classifier, the overall drop in performance becomes significant (row 5 versus row 1).We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.