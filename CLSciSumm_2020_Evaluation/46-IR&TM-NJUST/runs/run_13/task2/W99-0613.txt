This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating â€” this deserves more theoretical investigation.(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples.The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.AdaBoost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances.In the next section we present an alternative approach that builds two classifiers while attempting to satisfy the above constraints as much as possible.This allow the learners to "bootstrap" each other by filling the labels of the instances on which the other side has abstained so far.