Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.This system is primarily based on the parsing models described by McDonald and Pereira (2006).In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.