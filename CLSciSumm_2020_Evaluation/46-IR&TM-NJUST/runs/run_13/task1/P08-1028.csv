Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'47','63','78','145'","<S sid=""47"" ssid=""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).</S><S sid=""63"" ssid=""11"">This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.</S><S sid=""78"" ssid=""26"">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid=""145"" ssid=""58"">The latter were the most common context words (excluding a stop list of function words).</S>",'Method_Citation'
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'87','93','174','184'","<S sid=""87"" ssid=""35"">Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = αui +βvi +γuivi (11) where α, β, and γ are weighting constants.</S><S sid=""93"" ssid=""6"">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid=""184"" ssid=""18"">Given that the basis of Kintsch’s model is the summation of the verb, a neighbor close to the verb and the noun, it is not surprising that it produces results similar to a summation which weights the verb more heavily than the noun.</S>",'Method_Citation'
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'21','43','60','145'","<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""60"" ssid=""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S><S sid=""145"" ssid=""58"">The latter were the most common context words (excluding a stop list of function words).</S>",'Method_Citation'
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",'72',"<S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S>",'Method_Citation'
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'34','44','190','193'","<S sid=""34"" ssid=""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid=""44"" ssid=""17"">For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid=""193"" ssid=""5"">Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>",'Method_Citation'
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'48','49','145'","<S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""145"" ssid=""58"">The latter were the most common context words (excluding a stop list of function words).</S>",'Method_Citation'
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'35','36','38','69'","<S sid=""35"" ssid=""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""69"" ssid=""17"">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>",'Method_Citation'
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'57','60','61','201'","<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""60"" ssid=""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S><S sid=""61"" ssid=""9"">One particularly useful constraint is to hold R fixed by focusing on a single well defined linguistic structure, for example the verb-subject relation.</S><S sid=""201"" ssid=""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994).</S>",'Method_Citation'
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'43','47','53','58'","<S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""47"" ssid=""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""58"" ssid=""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v, as is the case for tensor products.</S>",'Method_Citation'
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'10','51','93','174'","<S sid=""10"" ssid=""6"">Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004).</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""93"" ssid=""6"">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S>",'Method_Citation'
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'34','38','49','168'","<S sid=""34"" ssid=""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'19','88','161','168'","<S sid=""19"" ssid=""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994).</S><S sid=""88"" ssid=""1"">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'63','155','161','175'","<S sid=""63"" ssid=""11"">This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.</S><S sid=""155"" ssid=""68"">Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S><S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S><S sid=""175"" ssid=""9"">We observe a similar pattern for the non compositional baseline model, the weighted additive model and Kintsch (2001).</S>",'Method_Citation'
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'92','128','134','172'","<S sid=""92"" ssid=""5"">To quantify this shift, Kintsch proposes measuring similarity relative to other verbs acting as landmarks, for example gallop and dissolve.</S><S sid=""128"" ssid=""41"">First, we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones.</S><S sid=""134"" ssid=""47"">We used Spearman’s ρ, a non parametric correlation coefficient, to avoid making any assumptions about the distribution of the similarity ratings.</S><S sid=""172"" ssid=""6"">Here, we are interested in relative differences, since the two types of ratings correspond to different scales.</S>",'Method_Citation'
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'28','49','155','200'","<S sid=""28"" ssid=""1"">The problem of vector composition has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Fodor and Pylyshyn, 1988).</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""155"" ssid=""68"">Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S><S sid=""200"" ssid=""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S>",'Method_Citation'
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'38','43','168','193'","<S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""193"" ssid=""5"">Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>",'Method_Citation'
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'35','38','44','193'","<S sid=""35"" ssid=""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""44"" ssid=""17"">For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997).</S><S sid=""193"" ssid=""5"">Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>",'Method_Citation'
