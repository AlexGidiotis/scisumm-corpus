Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","'7','9','157','163'","<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""157"" ssid=""42"">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S><S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>",'Method_Citation'
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","'9','132','163'","<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>",'Method_Citation'
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","'7','9','19','134'","<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""19"" ssid=""15"">However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space.</S><S sid=""134"" ssid=""19"">In fact, although neither DCS nor SEMRESP uses logical forms, DCS uses even less supervision than SEMRESP.</S>",'Method_Citation'
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","'33','39','87','89'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""39"" ssid=""15"">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid=""87"" ssid=""63"">For example, in Figure 4(a), before execution, the denotation of the DCS tree is hh{[(CA, OR), (OR)],... }; ø; (E, Qhstatei�w, ø)ii; after applying X1, we have hh{[(OR)], ... }; øii.</S><S sid=""89"" ssid=""65"">For example, In a DCS tree, the quantifier appears as the child of a Q relation, and the restrictor is the parent (see Figure 4(b) for an example).</S>",'Method_Citation'
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","'33','34','68','87'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""34"" ssid=""10"">Figure 2(a) shows an example of a DCS tree.</S><S sid=""68"" ssid=""44"">The denotation of a DCS tree can now be defined recursively: The base case is defined in (3): if z is a single node with predicate p, then the denotation of z has one column with the tuples w(p) and an empty store.</S><S sid=""87"" ssid=""63"">For example, in Figure 4(a), before execution, the denotation of the DCS tree is hh{[(CA, OR), (OR)],... }; ø; (E, Qhstatei�w, ø)ii; after applying X1, we have hh{[(OR)], ... }; øii.</S>",'Method_Citation'
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","'51','129','165'","<S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid=""129"" ssid=""14"">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g., (large, size)), which cancels the predicates triggered by x’s POS tag.</S><S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>",'Method_Citation'
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","'33','53','64','74'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""53"" ssid=""29"">Having instantiated s as a value, everything above this node is an ordinary CSP: s constrains the count node, which in turns constrains the root node to |s|.</S><S sid=""64"" ssid=""40"">Column 1 always corresponds to the root node.</S><S sid=""74"" ssid=""50"">Extending this notation to denotations, let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>",'Method_Citation'
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","'9','13','14','159'","<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""13"" ssid=""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S><S sid=""14"" ssid=""10"">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid=""159"" ssid=""44"">The closest work to ours is Clarke on compositional semantics.</S>",'Method_Citation'
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","'10','33','35','163'","<S sid=""10"" ssid=""6"">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""35"" ssid=""11"">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S><S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>",'Method_Citation'
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"'9','14','132','163'","<S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""14"" ssid=""10"">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S><S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>",'Method_Citation'
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)",'150',"<S sid=""150"" ssid=""35"">For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc.</S>",'Method_Citation'
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","'33','34','87','89'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""34"" ssid=""10"">Figure 2(a) shows an example of a DCS tree.</S><S sid=""87"" ssid=""63"">For example, in Figure 4(a), before execution, the denotation of the DCS tree is hh{[(CA, OR), (OR)],... }; ø; (E, Qhstatei�w, ø)ii; after applying X1, we have hh{[(OR)], ... }; øii.</S><S sid=""89"" ssid=""65"">For example, In a DCS tree, the quantifier appears as the child of a Q relation, and the restrictor is the parent (see Figure 4(b) for an example).</S>",'Method_Citation'
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","'33','46','51','167'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""46"" ssid=""22"">Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z ∈ Z consists of (i) a predicate for each child i, the ji-th component of v must equal the j'i-th component of some t in the child’s denotation (t ∈ JciKw).</S><S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid=""167"" ssid=""52"">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>",'Method_Citation'
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","'35','45','118','137'","<S sid=""35"" ssid=""11"">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S><S sid=""45"" ssid=""21"">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S><S sid=""118"" ssid=""3"">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid=""137"" ssid=""22"">If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%).</S>",'Method_Citation'
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"'1','9','163'","<S sid=""1"" ssid=""1"">Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>",'Method_Citation'
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","'7','24','165','172'","<S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S><S sid=""24"" ssid=""20"">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S><S sid=""172"" ssid=""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>",'Method_Citation'
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","'33','100','156','165'","<S sid=""33"" ssid=""9"">As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid=""100"" ssid=""76"">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid=""156"" ssid=""41"">There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>",'Method_Citation'
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)",'165',"<S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>",'Method_Citation'
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","'25','134','150','159'","<S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S><S sid=""134"" ssid=""19"">In fact, although neither DCS nor SEMRESP uses logical forms, DCS uses even less supervision than SEMRESP.</S><S sid=""150"" ssid=""35"">For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc.</S><S sid=""159"" ssid=""44"">The closest work to ours is Clarke on compositional semantics.</S>",'Method_Citation'
