Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","'7','57','81','175'","<S sid=""7"" ssid=""5"">We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid=""57"" ssid=""23"">We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.</S><S sid=""81"" ssid=""20"">This is less than the 694 judgements 2004 DARPA/NIST evaluation, or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid=""175"" ssid=""6"">Replacing this with an ranked evaluation seems to be more suitable.</S>",'Method_Citation'
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","'38','132'","<S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""132"" ssid=""25"">It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.</S>",'Method_Citation'
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","'16','38','126'","<S sid=""16"" ssid=""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S><S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",'Method_Citation'
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","'26','38','49'","<S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>",'Method_Citation'
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)",'28',"<S sid=""28"" ssid=""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>",'Method_Citation'
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","'7','38','57','65'","<S sid=""7"" ssid=""5"">We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""57"" ssid=""23"">We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.</S><S sid=""65"" ssid=""4"">The main disadvantage of manual evaluation is that it is time-consuming and thus too expensive to do frequently.</S>",'Method_Citation'
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","'38','39','140','145'","<S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""145"" ssid=""38"">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>",'Method_Citation'
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"'39','152'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""152"" ssid=""45"">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S>",'Method_Citation'
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"'39','140'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",'Method_Citation'
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","'49','57','122','167'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""57"" ssid=""23"">We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.</S><S sid=""122"" ssid=""15"">While the Bootstrap method is slightly more sensitive, it is very much in line with the sign test on text blocks.</S><S sid=""167"" ssid=""60"">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>",'Method_Citation'
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","'49','92','98','119'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""92"" ssid=""8"">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S><S sid=""98"" ssid=""14"">Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.</S><S sid=""119"" ssid=""12"">There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them.</S>",'Method_Citation'
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","'36','119','145','150'","<S sid=""36"" ssid=""2"">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S><S sid=""119"" ssid=""12"">There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them.</S><S sid=""145"" ssid=""38"">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S><S sid=""150"" ssid=""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French), at least as measured by BLEU.</S>",'Method_Citation'
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","'38','39','140','145'","<S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""145"" ssid=""38"">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>",'Method_Citation'
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","'16','58','79','126'","<S sid=""16"" ssid=""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S><S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S><S sid=""79"" ssid=""18"">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",'Method_Citation'
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","'20','58','107','167'","<S sid=""20"" ssid=""13"">For statistics on this test set, refer to Figure 1.</S><S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S><S sid=""107"" ssid=""23"">Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2.</S><S sid=""167"" ssid=""60"">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>",'Method_Citation'
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","'33','49'","<S sid=""33"" ssid=""26"">While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>",'Method_Citation'
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","'7','63','86','91'","<S sid=""7"" ssid=""5"">We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid=""63"" ssid=""2"">Many human evaluation metrics have been proposed.</S><S sid=""86"" ssid=""2"">The average fluency judgement per judge ranged from 2.33 to 3.67, the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid=""91"" ssid=""7"">In fact, it is very difficult to maintain consistent standards, on what (say) an adequacy judgement of 3 means even for a specific language pair.</S>",'Method_Citation'
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","'38','39','140'","<S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",'Method_Citation'
