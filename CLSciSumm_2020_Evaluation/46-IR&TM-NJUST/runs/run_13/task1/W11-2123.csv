Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'199','218','219','244'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""219"" ssid=""38"">Based on revision 4041, we modified Moses to print process statistics before terminating.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'96','214','216','244'","<S sid=""96"" ssid=""74"">The size of TRIE is particularly sensitive to F1092 c11, so vocabulary filtering is quite effective at reducing model size.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'0','11','39','259'","<S sid=""0"" ssid=""1"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""11"" ssid=""6"">Many packages perform language model queries.</S><S sid=""39"" ssid=""17"">As the name implies, space is O(m) and linear in the number of entries.</S><S sid=""259"" ssid=""1"">There any many techniques for improving language model speed and reducing memory consumption.</S>",'Method_Citation'
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'91','102','201','240'","<S sid=""91"" ssid=""69"">We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17−20 bits).</S><S sid=""102"" ssid=""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S sid=""201"" ssid=""20"">Unlike Germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'47','79','210','244'","<S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""79"" ssid=""57"">Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'47','84','101','240'","<S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""101"" ssid=""5"">With some minor API changes, namely returning the length of the n-gram matched, it could also be faster—though this would be at the expense of an optimization we explain in Section 4.1.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),'152',"<S sid=""152"" ssid=""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.</S>",'Method_Citation'
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'91','103','240','244'","<S sid=""91"" ssid=""69"">We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17−20 bits).</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'172','214','240','278'","<S sid=""172"" ssid=""44"">This is especially effective at reducing load time, since raw bytes are read directly to memory—or, as happens with repeatedly used models, are already in the disk cache.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S><S sid=""278"" ssid=""5"">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>",'Method_Citation'
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'16','38','53','92'","<S sid=""16"" ssid=""11"">BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid=""38"" ssid=""16"">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid=""53"" ssid=""31"">Sorted arrays store key-value pairs in an array sorted by key, incurring no space overhead.</S><S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S>",'Method_Citation'
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","'98','142','242','272'","<S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S><S sid=""142"" ssid=""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore, when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf , it may return state s(wn1) = wnf since no longer context will be found.</S><S sid=""242"" ssid=""61"">The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE, the default.</S><S sid=""272"" ssid=""14"">Generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension.</S>",'Method_Citation'
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)",'118',"<S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S>",'Method_Citation'
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","'10','121','221','245'","<S sid=""10"" ssid=""5"">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid=""121"" ssid=""25"">A direct-mapped cache makes BerkeleyLM faster on repeated queries, but their fastest (scrolling) cached version is still slower than uncached PROBING, even on cache-friendly queries.</S><S sid=""221"" ssid=""40"">SRILM’s compact variant has an incredibly expensive destructor, dwarfing the time it takes to perform translation, and so we also modified Moses to avoiding the destructor by calling exit instead of returning normally.</S><S sid=""245"" ssid=""64"">Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. aLossy compression with the same weights. bLossy compression with retuned weights. the non-lossy options.</S>","'Implication_Citation','Method_Citation'"
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'91','102','200','244'","<S sid=""91"" ssid=""69"">We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17−20 bits).</S><S sid=""102"" ssid=""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","'24','214','240','252'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'47','131','199','223'","<S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>",'Method_Citation'
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'84','165','214','240'","<S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""165"" ssid=""37"">The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'60','107','165','201'","<S sid=""60"" ssid=""38"">Otherwise, the scope of the search problem shrinks recursively: if A[pivot] < k then this becomes the new lower bound: l +— pivot; if A[pivot] > k then u +— pivot.</S><S sid=""107"" ssid=""11"">Each trie node contains a sorted array of entries and they use binary search.</S><S sid=""165"" ssid=""37"">The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.</S><S sid=""201"" ssid=""20"">Unlike Germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.</S>",'Method_Citation'
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'7','47','52','199'","<S sid=""7"" ssid=""2"">This paper presents methods to query N-gram language models, minimizing time and space costs.</S><S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>",'Method_Citation'
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","'47','84','214','240'","<S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
