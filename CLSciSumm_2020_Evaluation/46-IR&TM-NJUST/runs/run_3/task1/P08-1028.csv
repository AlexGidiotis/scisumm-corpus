Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'60','62','83','190'","<S sid=""60"" ssid=""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S><S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S sid=""83"" ssid=""31"">Although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'87','157','170','184'","<S sid=""87"" ssid=""35"">Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = αui +βvi +γuivi (11) where α, β, and γ are weighting constants.</S><S sid=""157"" ssid=""70"">Finally, Kintsch’s (2001) additive model has two extra parameters.</S><S sid=""170"" ssid=""4"">Table 2 shows the average model ratings for High and Low similarity items.</S><S sid=""184"" ssid=""18"">Given that the basis of Kintsch’s model is the summation of the verb, a neighbor close to the verb and the noun, it is not surprising that it produces results similar to a summation which weights the verb more heavily than the noun.</S>",'Method_Citation'
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'28','51','78','142'","<S sid=""28"" ssid=""1"">The problem of vector composition has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Fodor and Pylyshyn, 1988).</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""78"" ssid=""26"">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid=""142"" ssid=""55"">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>",'Method_Citation'
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",'72',"<S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S>",'Method_Citation'
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'26','49','168','173'","<S sid=""26"" ssid=""22"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""173"" ssid=""7"">Model similarities have been estimated using cosine which ranges from 0 to 1, whereas our subjects rated the sentences on a scale from 1 to 7.</S>",'Method_Citation'
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'40','48','56','142'","<S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid=""56"" ssid=""4"">Here, the space has only five dimensions, and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal, stable, and so on.</S><S sid=""142"" ssid=""55"">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>",'Method_Citation'
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'12','40'","<S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>",'Method_Citation'
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'2','49','62','96'","<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S sid=""96"" ssid=""9"">Any adequate model of composition must be able to represent argument-verb meaning.</S>",'Method_Citation'
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'2','49','149','168'","<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""149"" ssid=""62"">Our composition models have no additional parameters beyond the semantic space just described, with three exceptions.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'150','175','178','183'","<S sid=""150"" ssid=""63"">First, the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid=""175"" ssid=""9"">We observe a similar pattern for the non compositional baseline model, the weighted additive model and Kintsch (2001).</S><S sid=""178"" ssid=""12"">Figure 3 shows the distribution of estimated similarities under the multiplicative model.</S><S sid=""183"" ssid=""17"">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S>",'Method_Citation'
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'65','66','164','168'","<S sid=""65"" ssid=""13"">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S><S sid=""66"" ssid=""14"">To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid=""164"" ssid=""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'26','83','156','168'","<S sid=""26"" ssid=""22"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid=""83"" ssid=""31"">Although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.</S><S sid=""156"" ssid=""69"">This yielded a weighted sum consisting of 95% verb, 0% noun and 5% of their multiplicative combination.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'140','155','157','188'","<S sid=""140"" ssid=""53"">Following previous work (Bullinaria and Levy, 2007), we optimized its parameters on a word-based semantic similarity task.</S><S sid=""155"" ssid=""68"">Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S><S sid=""157"" ssid=""70"">Finally, Kintsch’s (2001) additive model has two extra parameters.</S><S sid=""188"" ssid=""22"">Also note that in contrast to the combined model, the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.</S>",'Method_Citation'
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'118','128','168','171'","<S sid=""118"" ssid=""31"">Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.</S><S sid=""128"" ssid=""41"">First, we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""171"" ssid=""5"">For comparison, we also show the human ratings for these items (UpperBound).</S>",'Method_Citation'
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'12','121','129','152'","<S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""121"" ssid=""34"">Here, burn is a high similarity landmark (High) for the reference The fire glowed, whereas beam is a low similarity landmark (Low).</S><S sid=""129"" ssid=""42"">Figure 2 presents a box-and-whisker plot of the distribution of the ratings.</S><S sid=""152"" ssid=""65"">To this end, we optimized the weights on a small held-out set.</S>",'Method_Citation'
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'43','51','66','168'","<S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""66"" ssid=""14"">To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'2','12','40','77'","<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid=""77"" ssid=""25"">For additive models, a natural way to achieve this is to include further vectors into the summation.</S>",'Method_Citation'
