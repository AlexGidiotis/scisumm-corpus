Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","'66','115','306','408'","<S sid=""66"" ssid=""24"">Since the beginning of string symbol is not predicted by language models, but rather is axiomatic in the same way that St is for a parser, we can safely omit it from the current discussion, and simply assume that it is there.</S><S sid=""115"" ssid=""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S><S sid=""306"" ssid=""62"">Recall the discussion of the grammar models above, and our definition of the set of partial derivations Dw, with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid=""408"" ssid=""21"">The improvement that we derived from interpolating the different models above indicates that using multiple models may be the most fruitful path in the future.</S>",'Method_Citation'
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,'14',"<S sid=""14"" ssid=""2"">Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S>",'Method_Citation'
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","'314','346','367','375'","<S sid=""314"" ssid=""70"">It is not part of the probability model itself.</S><S sid=""346"" ssid=""102"">These results are particularly remarkable, given that we did not build our model as a language model per se, but rather as a parsing model.</S><S sid=""367"" ssid=""123"">Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40M words, with a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.</S><S sid=""375"" ssid=""131"">Interestingly, as mentioned above, interpolating two models together gave no improvement over the better of the two, whether our model was interpolated with the lattice or the Treebank trigram.</S>",'Method_Citation'
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","'93','314','371','375'","<S sid=""93"" ssid=""51"">The standard language model used in many speech recognition systems is the trigram model, i.e., a Markov model of order 2, which can be characterized by the following equation: To smooth the trigram models that are used in this paper, we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).</S><S sid=""314"" ssid=""70"">It is not part of the probability model itself.</S><S sid=""371"" ssid=""127"">For our model and the Treebank trigram model, the LM weight that resulted in the lowest error rates is given.</S><S sid=""375"" ssid=""131"">Interestingly, as mentioned above, interpolating two models together gave no improvement over the better of the two, whether our model was interpolated with the lattice or the Treebank trigram.</S>",'Method_Citation'
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","'115','116','126','267'","<S sid=""115"" ssid=""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S><S sid=""116"" ssid=""20"">The SLM is like a trigram, except that the conditioning words are taken from the tops of the stacks of candidate parses in the beam, rather than from the linear order of the string.</S><S sid=""126"" ssid=""30"">When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","'76','110','134','223'","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""110"" ssid=""14"">The parser performs two basic operations: (i) shifting, which involves pushing the POS label of the next word onto the stack and moving the pointer to the following word in the input string; and (ii) reducing, which takes the top k stack entries and replaces them with a single new entry, the nonterminal label of which is the left-hand side of a rule in the grammar that has the k top stack entry labels on the right-hand side.</S><S sid=""134"" ssid=""38"">The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically!'</S><S sid=""223"" ssid=""127"">We implement this as a beam search.</S>",'Method_Citation'
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","'32','118','126','184'","<S sid=""32"" ssid=""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S><S sid=""118"" ssid=""22"">The first stage assigns a probability to the word given the left context (represented by the stack state).</S><S sid=""126"" ssid=""30"">When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid=""184"" ssid=""88"">If the POS is leftmost within its constituent, then very often the lexical item is sensitive to the governing category to which it is attaching.</S>",'Method_Citation'
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","'76','119','126','331'","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""119"" ssid=""23"">The second stage predicts the POS given the word and the left context.</S><S sid=""126"" ssid=""30"">When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid=""331"" ssid=""87"">We can see that removing the punctuation causes (unsurprisingly) a dramatic drop in the accuracy and efficiency of the parser.</S>",'Method_Citation'
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","'112','116','124','197'","<S sid=""112"" ssid=""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state, and conditioning on those entries in a way similar to an n-gram.</S><S sid=""116"" ssid=""20"">The SLM is like a trigram, except that the conditioning words are taken from the tops of the stacks of candidate parses in the beam, rather than from the linear order of the string.</S><S sid=""124"" ssid=""28"">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid=""197"" ssid=""101"">We will refer to the amount of conditioning by specifying the deepest level from which a value is returned for each branching path in the decision tree, from left to right in Figure 4: the first number is for left contexts where the left branch of the decision tree is always followed (non-POS nonterminals on the left-hand side); the second number is for a left branch followed by a right branch (POS nodes that are leftmost within their constituent); and the third number is for the contexts where the right branch is always followed (POS nodes that are not leftmost within their constituent).</S>",'Method_Citation'
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","'75','76','112','397'","<S sid=""75"" ssid=""33"">In effect, this is an underspecification of some of the predictions that our top-down parser is making about the rest of the string.</S><S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""112"" ssid=""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state, and conditioning on those entries in a way similar to an n-gram.</S><S sid=""397"" ssid=""10"">A top-down parser, in contrast to a standard bottom-up chart parser, has enough information to predict empty categories only where they are likely to occur.</S>",'Method_Citation'
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)",'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","'76','108','115','400'","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""108"" ssid=""12"">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to ""surface"" c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S><S sid=""115"" ssid=""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S><S sid=""400"" ssid=""13"">Other parsing approaches might also be used in the way that we have used a topdown parser.</S>",'Method_Citation'
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","'227','238','259','295'","<S sid=""227"" ssid=""131"">The parse on H,i+i with the highest probability is returned for evaluation.</S><S sid=""238"" ssid=""142"">Thus, if 100 analyses have already been pushed onto then a candidate analysis must have a probability above 10-5/3 to avoid being pruned.</S><S sid=""259"" ssid=""15"">Evaluation is carried out on a hand-parsed test corpus, and the manual parses are treated as correct.</S><S sid=""295"" ssid=""51"">Our observed times look polynomial, which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis, the more time will be spent working on these competitors; and the farther along in the sentence, the more chance for ambiguities that can lead to such a situation.</S>",'Method_Citation'
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","'76','112','123','209'","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""112"" ssid=""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state, and conditioning on those entries in a way similar to an n-gram.</S><S sid=""123"" ssid=""27"">Each different POS assignment or parser operation is a step in a derivation.</S><S sid=""209"" ssid=""113"">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>",'Method_Citation'
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","'199','265','289','294'","<S sid=""199"" ssid=""103"">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials, with a mnemonic label that will be used when presenting results.</S><S sid=""265"" ssid=""21"">LR and LP are part of the standard set of PARSEVAL measures of parser quality (Black et al. 1991).</S><S sid=""289"" ssid=""45"">These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid=""294"" ssid=""50"">Figure 6 shows the observed time at our standard base beam of 10-11 with the full conditioning regimen, alongside an approximation of the reported observed (linear) time in Ratnaparkhi (1997).</S>",'Method_Citation'
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","'76','113','126','349'","<S sid=""76"" ssid=""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid=""113"" ssid=""17"">In empirical trials, Goddeau used the top two stack entries to condition the word probability.</S><S sid=""126"" ssid=""30"">When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid=""349"" ssid=""105"">One way to test this is the following: at each point in the sentence, calculate the conditional probability of each word in the vocabulary given the previous words, and sum them.'</S>",'Method_Citation'
