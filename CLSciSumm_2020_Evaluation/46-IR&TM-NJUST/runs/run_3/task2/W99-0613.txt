(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples.The key to the methods we describe is redundancy in the unlabeled data.Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.The core of Yarowsky's algorithm is as follows: where h is defined by the formula in equation 2, with counts restricted to training data examples that have been labeled in step 2.Formally, let el (62) be the number of classification errors of the first (second) learner on the training data, and let Eco be the number of unlabeled examples on which the two classifiers disagree.In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.Our first algorithm is similar to Yarowsky's, but with some important modifications motivated by (Blum and Mitchell 98).