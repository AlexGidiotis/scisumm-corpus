We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.It is worth noting that, although nonprojective constructions are less frequent in DDT than in PDT, they seem to be more deeply nested, since only about 80% can be projectivized with a single lift, while almost 95% of the non-projective arcs in PDT only require a single lift.The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.However, since we want to preserve as much of the original structure as possible, we are interested in finding a transformation that involves a minimal number of lifts.For each token, three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token â€“ the arc from its head and the arcs to its leftmost and rightmost dependent, respectively.