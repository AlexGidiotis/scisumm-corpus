Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"'24','44','71','116'","<S sid=""24"" ssid=""10"">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid=""44"" ssid=""30"">Each of the constituents must have received at least 1 votes from the k parsers, so a > I1 and 2 — 2k±-1 b > ri-5-111.</S><S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S><S sid=""116"" ssid=""45"">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>",'Method_Citation'
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","'23','33','38','125'","<S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""33"" ssid=""19"">C is the union of the sets of constituents suggested by the parsers. r(c) is a binary function returning t (for true) precisely when the constituent c E C should be included in the hypothesis.</S><S sid=""38"" ssid=""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid=""125"" ssid=""54"">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>",'Method_Citation'
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","'44','66','71','117'","<S sid=""44"" ssid=""30"">Each of the constituents must have received at least 1 votes from the k parsers, so a > I1 and 2 — 2k±-1 b > ri-5-111.</S><S sid=""66"" ssid=""52"">Each decision determines the inclusion or exclusion of a candidate constituent.</S><S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S><S sid=""117"" ssid=""46"">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>",'Method_Citation'
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","'23','27','48'","<S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""48"" ssid=""34"">• Similarly, when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>",'Method_Citation'
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","'16','27','32','125'","<S sid=""16"" ssid=""2"">We call this approach parse hybridization.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""32"" ssid=""18"">In Equations 1 through 3 we develop the model for constructing our parse using naïve Bayes classification.</S><S sid=""125"" ssid=""54"">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>",'Method_Citation'
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,'23',"<S sid=""23"" ssid=""9"">We call this technique constituent voting.</S>",'Method_Citation'
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","'28','72','130','131'","<S sid=""28"" ssid=""14"">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid=""130"" ssid=""59"">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid=""131"" ssid=""60"">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S>",'Method_Citation'
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","'23','27','94'","<S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""94"" ssid=""23"">This is the only important case, because otherwise the simple majority combining technique would pick the correct constituent.</S>",'Method_Citation'
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","'51','71','95','97'","<S sid=""51"" ssid=""37"">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S><S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S><S sid=""95"" ssid=""24"">One side of the decision making process is when we choose to believe a constituent should be in the parse, even though only one parser suggests it.</S><S sid=""97"" ssid=""26"">If we were working with more than three parsers we could investigate minority constituents, those constituents that are suggested by at least one parser, but which the majority of the parsers do not suggest.</S>",'Method_Citation'
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","'44','89','99','133'","<S sid=""44"" ssid=""30"">Each of the constituents must have received at least 1 votes from the k parsers, so a > I1 and 2 — 2k±-1 b > ri-5-111.</S><S sid=""89"" ssid=""18"">None of the models we have presented utilize features associated with a particular constituent (i.e. the label, span, parent label, etc.) to influence parser preference.</S><S sid=""99"" ssid=""28"">Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.</S><S sid=""133"" ssid=""62"">The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.</S>",'Method_Citation'
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","'36','102','108'","<S sid=""36"" ssid=""22"">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid=""102"" ssid=""31"">In Table 1 we see with very few exceptions that the isolated constituent precision is less than 0.5 when we use the constituent label as a feature.</S><S sid=""108"" ssid=""37"">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>",'Method_Citation'
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","'51','71','95','131'","<S sid=""51"" ssid=""37"">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S><S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S><S sid=""95"" ssid=""24"">One side of the decision making process is when we choose to believe a constituent should be in the parse, even though only one parser suggests it.</S><S sid=""131"" ssid=""60"">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S>",'Method_Citation'
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","'64','78','103','126'","<S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S><S sid=""78"" ssid=""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid=""103"" ssid=""32"">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid=""126"" ssid=""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S>",'Method_Citation'
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","'16','27','67','71'","<S sid=""16"" ssid=""2"">We call this approach parse hybridization.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""67"" ssid=""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S><S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S>",'Method_Citation'
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","'80','81','82','118'","<S sid=""80"" ssid=""9"">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S><S sid=""81"" ssid=""10"">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S><S sid=""82"" ssid=""11"">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S><S sid=""118"" ssid=""47"">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>",'Method_Citation'
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))",'6',"<S sid=""6"" ssid=""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996).</S>",'Method_Citation'
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"'78','103','119','126'","<S sid=""78"" ssid=""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid=""103"" ssid=""32"">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid=""119"" ssid=""48"">We do not show the numbers for the Bayes models in Table 2 because the parameters involved were established using this set.</S><S sid=""126"" ssid=""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S>",'Method_Citation'
