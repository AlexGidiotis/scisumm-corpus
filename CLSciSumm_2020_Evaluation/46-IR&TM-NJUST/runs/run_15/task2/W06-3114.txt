The average fluency judgement per judge ranged from 2.33 to 3.67, the average adequacy judgement ranged from 2.56 to 4.13.For statistics on this test set, refer to Figure 1.Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2.This is less than the 694 judgements 2004 DARPA/NIST evaluation, or the 532 judgements in the 2005 DARPA/NIST evaluation.The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.In fact, it is very difficult to maintain consistent standards, on what (say) an adequacy judgement of 3 means even for a specific language pair.We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.