Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence.Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.