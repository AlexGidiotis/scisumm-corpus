Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'26','38','89','190'","<S sid=""26"" ssid=""22"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""89"" ssid=""2"">In his study, Kintsch builds a model of how a verb’s meaning is modified in the context of its subject.</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'17','18','73','196'","<S sid=""17"" ssid=""13"">It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b.</S><S sid=""18"" ssid=""14"">That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.</S><S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S><S sid=""196"" ssid=""8"">Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully.</S>",'Method_Citation'
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'1','36','43','58'","<S sid=""1"" ssid=""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""58"" ssid=""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v, as is the case for tensor products.</S>",'Method_Citation'
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",'72',"<S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S>",'Method_Citation'
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'36','71','167','168'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""71"" ssid=""19"">In effect, this is what model (6) achieves.</S><S sid=""167"" ssid=""1"">Our experiments assessed the performance of seven composition models.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'2','3','36','161'","<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""3"" ssid=""3"">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S>",'Method_Citation'
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'36','38','40','75'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid=""75"" ssid=""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>",'Method_Citation'
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'51','58','161','167'","<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""58"" ssid=""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v, as is the case for tensor products.</S><S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S><S sid=""167"" ssid=""1"">Our experiments assessed the performance of seven composition models.</S>",'Method_Citation'
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'49','73','108','168'","<S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S><S sid=""108"" ssid=""21"">Specifically, they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs, each paired with 10 nouns, and 2 landmarks (400 pairs of sentences in total).</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'72','73','150','183'","<S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S><S sid=""73"" ssid=""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S><S sid=""150"" ssid=""63"">First, the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid=""183"" ssid=""17"">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S>",'Method_Citation'
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'61','163','176','194'","<S sid=""61"" ssid=""9"">One particularly useful constraint is to hold R fixed by focusing on a single well defined linguistic structure, for example the verb-subject relation.</S><S sid=""163"" ssid=""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid=""176"" ssid=""10"">The multiplicative and combined models yield means closer to the human ratings.</S><S sid=""194"" ssid=""6"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S>",'Method_Citation'
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'99','167','168','198'","<S sid=""99"" ssid=""12"">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S><S sid=""167"" ssid=""1"">Our experiments assessed the performance of seven composition models.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""198"" ssid=""10"">In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.</S>",'Method_Citation'
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'52','168','175','190'","<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""175"" ssid=""9"">We observe a similar pattern for the non compositional baseline model, the weighted additive model and Kintsch (2001).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'129','172','176','196'","<S sid=""129"" ssid=""42"">Figure 2 presents a box-and-whisker plot of the distribution of the ratings.</S><S sid=""172"" ssid=""6"">Here, we are interested in relative differences, since the two types of ratings correspond to different scales.</S><S sid=""176"" ssid=""10"">The multiplicative and combined models yield means closer to the human ratings.</S><S sid=""196"" ssid=""8"">Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully.</S>",'Method_Citation'
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'62','66','71','72'","<S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S sid=""66"" ssid=""14"">To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid=""71"" ssid=""19"">In effect, this is what model (6) achieves.</S><S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S>",'Method_Citation'
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'47','51','168','190'","<S sid=""47"" ssid=""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'2','12','168','190'","<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
