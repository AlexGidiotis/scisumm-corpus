Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","'9','11','28','70'","<S sid=""9"" ssid=""5"">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S><S sid=""28"" ssid=""24"">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S><S sid=""70"" ssid=""21"">Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>",'Method_Citation'
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","'5','9','23','70'","<S sid=""5"" ssid=""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid=""9"" ssid=""5"">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid=""23"" ssid=""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid=""70"" ssid=""21"">Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>",'Method_Citation'
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),'40',"<S sid=""40"" ssid=""12"">Templates in the column below are expanded from the upper ones.</S>",'Method_Citation'
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","'103','110'","<S sid=""103"" ssid=""14"">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.</S>","'Method_Citation','Result_Citation'"
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"'81','89','105','112'","<S sid=""81"" ssid=""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid=""89"" ssid=""14"">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid=""105"" ssid=""16"">At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid=""112"" ssid=""23"">Here the core perceptron was just the POS+ model in experiments above.</S>",'Method_Citation'
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","'33','51','70','132'","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S><S sid=""70"" ssid=""21"">Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S><S sid=""132"" ssid=""3"">This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large.</S>",'Method_Citation'
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)",'86',"<S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>",'Method_Citation'
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase",'70',"<S sid=""70"" ssid=""21"">Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>",'Method_Citation'
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","'11','105','106','118'","<S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S><S sid=""105"" ssid=""16"">At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid=""106"" ssid=""17"">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POSâ€™s are correctly labelled.</S><S sid=""118"" ssid=""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+.</S>",'Method_Citation'
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","'58','72','81','89'","<S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S><S sid=""72"" ssid=""23"">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S><S sid=""81"" ssid=""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid=""89"" ssid=""14"">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S>",'Method_Citation'
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","'30','33','46','113'","<S sid=""30"" ssid=""2"">It has comparable performance to CRFs, while with much faster training.</S><S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) âˆˆ X Ã— Y to a feature vector 4)(x, y) âˆˆ Rd, and a parameter vector Î±ï¿½ âˆˆ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""113"" ssid=""24"">Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.</S>",'Method_Citation'
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","'73','95','134','135'","<S sid=""73"" ssid=""24"">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid=""95"" ssid=""6"">For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus.</S><S sid=""134"" ssid=""5"">If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid=""135"" ssid=""6"">In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004).</S>",'Method_Citation'
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","'33','36','46','62'","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S><S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) âˆˆ X Ã— Y to a feature vector 4)(x, y) âˆˆ Rd, and a parameter vector Î±ï¿½ âˆˆ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""62"" ssid=""13"">Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.</S>",'Method_Citation'
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","'46','47','81','134'","<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) âˆˆ X Ã— Y to a feature vector 4)(x, y) âˆˆ Rd, and a parameter vector Î±ï¿½ âˆˆ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""47"" ssid=""19"">For an input character sequence x, we aim to find an output F(x) satisfying: vector 4)(x, y) and the parameter vector a.</S><S sid=""81"" ssid=""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid=""134"" ssid=""5"">If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>",'Method_Citation'
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)",'58',"<S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S>",'Method_Citation'
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","'11','26','77','106'","<S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S><S sid=""26"" ssid=""22"">In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid=""77"" ssid=""2"">In Chinese Joint S&T, the mission of the decoder is to find the boundary-POS labelled sequence with the highest score.</S><S sid=""106"" ssid=""17"">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POSâ€™s are correctly labelled.</S>",'Method_Citation'
