Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1061,P11-1144,0,2011,0,Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,"'3','37','38','140'","<S sid=""3"" ssid=""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S><S sid=""37"" ssid=""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid=""38"" ssid=""4"">More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid=""140"" ssid=""3"">The feature-HMM model works better for all languages, generalizing the results achieved for English by Berg-Kirkpatrick et al. (2010).</S>",'Method_Citation'
3,P11-1061,P14-1126,0,2011,0,"Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","'31','115','118','144'","<S sid=""31"" ssid=""8"">By aggregating the POS labels of the English tokens to types, we can generate label distributions for the English vertices.</S><S sid=""115"" ssid=""15"">The taggers were trained on datasets labeled with the universal tags.</S><S sid=""118"" ssid=""18"">Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset.</S><S sid=""144"" ssid=""7"">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>",'Method_Citation'
4,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","'30','37','57','127'","<S sid=""30"" ssid=""7"">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid=""37"" ssid=""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid=""127"" ssid=""27"">Furthermore we expect the label distributions on the foreign to be fairly noisy, because the graph constraints have not been taken into account yet.</S>",'Method_Citation'
5,P11-1061,N12-1086,0,2011,0,"Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","'47','51'","<S sid=""47"" ssid=""13"">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid=""51"" ssid=""17"">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>",'Method_Citation'
6,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","'75','83','105','163'","<S sid=""75"" ssid=""6"">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi − qjk2 = Ey(qi(y) − qj(y))2, and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid=""83"" ssid=""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid=""105"" ssid=""5"">The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006).</S><S sid=""163"" ssid=""2"">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S>",'Method_Citation'
7,P11-1061,N12-1052,0,2011,0,"Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","'83','124','148','159'","<S sid=""83"" ssid=""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid=""124"" ssid=""24"">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S><S sid=""148"" ssid=""11"">Our full model outperforms the “No LP” setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features.</S><S sid=""159"" ssid=""2"">Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>",'Method_Citation'
8,P11-1061,N12-1052,0,2011,0,"We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","'3','79','83','112'","<S sid=""3"" ssid=""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S><S sid=""79"" ssid=""10"">If an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags.</S><S sid=""83"" ssid=""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid=""112"" ssid=""12"">While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.</S>",'Method_Citation'
9,P11-1061,N12-1090,0,"Das and Petrov, (2011)",0,"MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus","MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))","'18','104'","<S sid=""18"" ssid=""14"">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid=""104"" ssid=""4"">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).</S>",'Method_Citation'
10,P11-1061,W11-2205,0,2011,0,"For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","'57','97','117','153'","<S sid=""57"" ssid=""23"">Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid=""97"" ssid=""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid=""117"" ssid=""17"">In other words, the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid=""153"" ssid=""16"">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>",'Method_Citation'
11,P11-1061,P13-1155,0,"Das and Petrov, 2011",0,"(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","'3','29','37','97'","<S sid=""3"" ssid=""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S><S sid=""29"" ssid=""6"">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S><S sid=""37"" ssid=""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid=""97"" ssid=""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S>",'Method_Citation'
12,P11-1061,D12-1127,0,2011,0,Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text,Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text,"'2','10','20','116'","<S sid=""2"" ssid=""2"">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S><S sid=""10"" ssid=""6"">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S><S sid=""20"" ssid=""16"">Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S><S sid=""116"" ssid=""16"">The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.</S>",'Method_Citation'
13,P11-1061,D12-1127,0,"Das and Petrov, 2011",0,"Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)","These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)","'2','10','13','20'","<S sid=""2"" ssid=""2"">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S><S sid=""10"" ssid=""6"">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S><S sid=""13"" ssid=""9"">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S><S sid=""20"" ssid=""16"">Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.</S>",'Method_Citation'
14,P11-1061,P12-3012,0,"Das and Petrov, 2011",0,"In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","'11','18','104','112'","<S sid=""11"" ssid=""7"">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009).</S><S sid=""18"" ssid=""14"">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S><S sid=""104"" ssid=""4"">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).</S><S sid=""112"" ssid=""12"">While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.</S>",'Method_Citation'
15,P11-1061,D11-1006,0,2011,0,"Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider","Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider","'105','115','120','153'","<S sid=""105"" ssid=""5"">The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006).</S><S sid=""115"" ssid=""15"">The taggers were trained on datasets labeled with the universal tags.</S><S sid=""120"" ssid=""20"">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid=""153"" ssid=""16"">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>",'Method_Citation'
16,P11-1061,D11-1006,0,2011,0,"In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","'112','116','118','153'","<S sid=""112"" ssid=""12"">While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.</S><S sid=""116"" ssid=""16"">The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.</S><S sid=""118"" ssid=""18"">Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset.</S><S sid=""153"" ssid=""16"">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>",'Method_Citation'
17,P11-1061,P13-2112,0,2011,0,"This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","'16','45','116','118'","<S sid=""16"" ssid=""12"">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid=""45"" ssid=""11"">Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.</S><S sid=""116"" ssid=""16"">The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.</S><S sid=""118"" ssid=""18"">Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset.</S>",'Method_Citation'
18,P11-1061,P13-2112,0,2011,0,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,"'73','83','153','159'","<S sid=""73"" ssid=""4"">Note that because we extracted only high-confidence alignments, many foreign vertices will not be connected to any English vertices.</S><S sid=""83"" ssid=""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid=""153"" ssid=""16"">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S><S sid=""159"" ssid=""2"">Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>",'Method_Citation'
19,P11-1061,P13-2112,0,"Das and Petrov, 2011",0,"We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","'36','74','124','144'","<S sid=""36"" ssid=""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S><S sid=""74"" ssid=""5"">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S sid=""124"" ssid=""24"">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S><S sid=""144"" ssid=""7"">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>",'Method_Citation'
