Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"'9','111','135','144'","<S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S><S sid=""111"" ssid=""40"">The first row represents the average accuracy of the three parsers we combine.</S><S sid=""135"" ssid=""64"">The average individual parser accuracy was reduced by more than 5% when we added this new parser, but the precision of the constituent voting technique was the only result that decreased significantly.</S><S sid=""144"" ssid=""6"">Combining multiple highly-accurate independent parsers yields promising results.</S>",'Method_Citation'
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","'38','39','104','125'","<S sid=""38"" ssid=""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid=""104"" ssid=""33"">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S><S sid=""125"" ssid=""54"">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>",'Method_Citation'
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","'24','49','97','102'","<S sid=""24"" ssid=""10"">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid=""49"" ssid=""35"">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid=""97"" ssid=""26"">If we were working with more than three parsers we could investigate minority constituents, those constituents that are suggested by at least one parser, but which the majority of the parsers do not suggest.</S><S sid=""102"" ssid=""31"">In Table 1 we see with very few exceptions that the isolated constituent precision is less than 0.5 when we use the constituent label as a feature.</S>",'Method_Citation'
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","'38','41','48'","<S sid=""38"" ssid=""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid=""41"" ssid=""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid=""48"" ssid=""34"">• Similarly, when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>",'Method_Citation'
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","'28','30','128','136'","<S sid=""28"" ssid=""14"">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S><S sid=""30"" ssid=""16"">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers, namely that the attribute values are conditionally independent when the target value is given.</S><S sid=""128"" ssid=""57"">Ties are rare in Bayes switching because the models are fine-grained — many estimated probabilities are involved in each decision.</S><S sid=""136"" ssid=""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>",'Method_Citation'
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,'38',"<S sid=""38"" ssid=""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>",'Method_Citation'
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","'32','119','124','128'","<S sid=""32"" ssid=""18"">In Equations 1 through 3 we develop the model for constructing our parse using naïve Bayes classification.</S><S sid=""119"" ssid=""48"">We do not show the numbers for the Bayes models in Table 2 because the parameters involved were established using this set.</S><S sid=""124"" ssid=""53"">This is the first set that gives us a fair evaluation of the Bayes models, and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid=""128"" ssid=""57"">Ties are rare in Bayes switching because the models are fine-grained — many estimated probabilities are involved in each decision.</S>",'Method_Citation'
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","'60','126','140','142'","<S sid=""60"" ssid=""46"">First we present the non-parametric version of parser switching, similarity switching: The intuition for this technique is that we can measure a similarity between parses by counting the constituents they have in common.</S><S sid=""126"" ssid=""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid=""140"" ssid=""2"">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S><S sid=""142"" ssid=""4"">Both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S>",'Method_Citation'
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","'37','67','99','126'","<S sid=""37"" ssid=""23"">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid=""67"" ssid=""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S><S sid=""99"" ssid=""28"">Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.</S><S sid=""126"" ssid=""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S>",'Method_Citation'
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","'72','111','112','127'","<S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid=""111"" ssid=""40"">The first row represents the average accuracy of the three parsers we combine.</S><S sid=""112"" ssid=""41"">The second row is the accuracy of the best of the three parsers.'</S><S sid=""127"" ssid=""56"">Parser 3, the most accurate parser, was chosen 71% of the time, and Parser 1, the least accurate parser was chosen 16% of the time.</S>",'Result_Citation'
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","'102','108','124'","<S sid=""102"" ssid=""31"">In Table 1 we see with very few exceptions that the isolated constituent precision is less than 0.5 when we use the constituent label as a feature.</S><S sid=""108"" ssid=""37"">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S><S sid=""124"" ssid=""53"">This is the first set that gives us a fair evaluation of the Bayes models, and the Bayes switching model performs significantly better than its non-parametric counterpart.</S>",'Method_Citation'
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","'34','37','67','99'","<S sid=""34"" ssid=""20"">Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.</S><S sid=""37"" ssid=""23"">Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.</S><S sid=""67"" ssid=""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S><S sid=""99"" ssid=""28"">Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.</S>",'Method_Citation'
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","'19','73','100','105'","<S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S><S sid=""73"" ssid=""2"">We used section 23 as the development set for our combining techniques, and section 22 only for final testing.</S><S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.</S><S sid=""105"" ssid=""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S>",'Method_Citation'
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","'38','41','87','99'","<S sid=""38"" ssid=""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid=""41"" ssid=""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid=""87"" ssid=""16"">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid=""99"" ssid=""28"">Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.</S>",'Method_Citation'
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","'54','100','105','136'","<S sid=""54"" ssid=""40"">If the parse contains productions from outside our grammar the machine has no direct method for handling them (e.g. the resulting database query may be syntactically malformed).</S><S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.</S><S sid=""105"" ssid=""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S><S sid=""136"" ssid=""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>",'Method_Citation'
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","'14','17','20'","<S sid=""14"" ssid=""10"">We used these three parsers to explore parser combination techniques.</S><S sid=""17"" ssid=""3"">The substructures that are unanimously hypothesized by the parsers should be preserved after combination, and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid=""20"" ssid=""6"">Since our goal is to perform well under these measures we will similarly treat constituents as the minimal substructures for combination.</S>",'Method_Citation'
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"'19','73','100','105'","<S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S><S sid=""73"" ssid=""2"">We used section 23 as the development set for our combining techniques, and section 22 only for final testing.</S><S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse.</S><S sid=""105"" ssid=""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S>",'Method_Citation'
