In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.Time for Moses itself to load, including loading the language model and phrase table, is included.The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.All language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.Generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension.For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17−20 bits).Sorted arrays store key-value pairs in an array sorted by key, incurring no space overhead.