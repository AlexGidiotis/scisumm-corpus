The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.Vector addition is by far the most common method for representing the meaning of linguistic sequences.We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.One particularly useful constraint is to hold R fixed by focusing on a single well defined linguistic structure, for example the verb-subject relation.We used Spearman’s ρ, a non parametric correlation coefficient, to avoid making any assumptions about the distribution of the similarity ratings.This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.