We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal tree-bank.In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.