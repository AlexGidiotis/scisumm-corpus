We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.Ideally, the “glue” documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.However, the growth of the web, and in particular Wikipedia, has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.The second topic, concerning children, demonstrates the variability of everyday terminology: although the four Romance languages are closely related, they use etymologically unrelated words for children.Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.