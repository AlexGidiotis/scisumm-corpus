Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.",'32',"<S sid=""32"" ssid=""4"">We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>",'Method_Citation'
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","'51','53','69','85'","<S sid=""51"" ssid=""2"">Additional features most widely used are related to word or POS ngrams.</S><S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid=""69"" ssid=""20"">Formally, an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid=""85"" ssid=""10"">Lines 3 — 11 generate a N-best list for each character position i.</S>",'Method_Citation'
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"'38','39','42','43'","<S sid=""38"" ssid=""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S><S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S><S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>",'Method_Citation'
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j",'110',"<S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.</S>","'Method_Citation','Result_Citation'"
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"'11','105','112','118'","<S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S><S sid=""105"" ssid=""16"">At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid=""112"" ssid=""23"">Here the core perceptron was just the POS+ model in experiments above.</S><S sid=""118"" ssid=""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+.</S>","'Method_Citation','Result_Citation'"
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","'6','34','53','54'","<S sid=""6"" ssid=""2"">Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001).</S><S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid=""54"" ssid=""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams.</S>",'Method_Citation'
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","'12','86','96'","<S sid=""12"" ssid=""8"">Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S><S sid=""96"" ssid=""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>",'Method_Citation'
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase",'6',"<S sid=""6"" ssid=""2"">Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001).</S>",'Method_Citation'
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","'4','11'","<S sid=""4"" ssid=""4"">On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S>",'Result_Citation'
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","'26','79','80'","<S sid=""26"" ssid=""22"">In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid=""79"" ssid=""4"">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid=""80"" ssid=""5"">At each position i, we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i, K) (K is assigned 20 in all our experiments) and ending at position i, then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l), and select for position i a N-best list of candidate results from all these candidates.</S>",'Method_Citation'
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","'33','34','38','43'","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""38"" ssid=""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>",'Method_Citation'
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","'42','43','100'","<S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S><S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid=""100"" ssid=""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S>",'Method_Citation'
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","'33','34','46','63'","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""63"" ssid=""14"">As all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.</S>",'Method_Citation'
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","'46','47','53','119'","<S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""47"" ssid=""19"">For an input character sequence x, we aim to find an output F(x) satisfying: vector 4)(x, y) and the parameter vector a.</S><S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid=""119"" ssid=""30"">We also find that the perceptron model functions as the kernel of the outside-layer linear model.</S>",'Method_Citation'
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","'50','53','121'","<S sid=""50"" ssid=""1"">In theory, any useful knowledge can be incorporated into the perceptron directly, besides the characterbased features already adopted.</S><S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid=""121"" ssid=""32"">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>",'Method_Citation'
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","'33','46','112','118'","<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S><S sid=""112"" ssid=""23"">Here the core perceptron was just the POS+ model in experiments above.</S><S sid=""118"" ssid=""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+.</S>",'Method_Citation'
