Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'21','140','141','268'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""140"" ssid=""12"">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S><S sid=""141"" ssid=""13"">Syntactic decoders, such as cdec (Dyer et al., 2010), build state from null context then store it in the hypergraph node for later extension.</S><S sid=""268"" ssid=""10"">For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>",'Method_Citation'
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'199','232','237','244'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""232"" ssid=""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'83','84','132','199'","<S sid=""83"" ssid=""61"">When SRILM estimates a model, it sometimes removes n-grams but not n + 1-grams that extend it to the left.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""132"" ssid=""4"">We call these N − 1 words state.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>",'Method_Citation'
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'21','103','199','268'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""268"" ssid=""10"">For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>",'Method_Citation'
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'21','146','194','210'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""146"" ssid=""18"">If the log backoff of wnf is also zero (it may not be in filtered models), then wf should be omitted from the state.</S><S sid=""194"" ssid=""13"">Further, the probing hash table does only one random lookup per query, explaining why it is faster on large data.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S>",'Method_Citation'
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'9','24','57','239'","<S sid=""9"" ssid=""4"">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""57"" ssid=""35"">Interpolation search formalizes the notion that one opens a dictionary near the end to find the word “zebra.” Initially, the algorithm knows the array begins at b +— 0 and ends at e +— |A|−1.</S><S sid=""239"" ssid=""58"">Overall, language modeling significantly impacts decoder performance.</S>",'Method_Citation'
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),'237',"<S sid=""237"" ssid=""56"">Moses keeps language models and many other resources in static variables, so these are still resident in memory.</S>",'Method_Citation'
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'80','83','84','199'","<S sid=""80"" ssid=""58"">The highestorder N-gram array omits backoff and the index, since these are not applicable.</S><S sid=""83"" ssid=""61"">When SRILM estimates a model, it sometimes removes n-grams but not n + 1-grams that extend it to the left.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>",'Method_Citation'
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'27','84','163','252'","<S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""163"" ssid=""35"">Further, it needs extra pointers in the trie, increasing model size by 40%.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'13','38','92','233'","<S sid=""13"" ssid=""8"">IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid=""38"" ssid=""16"">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S><S sid=""233"" ssid=""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times, such as in parameter tuning.</S>",'Method_Citation'
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","'85','109','165','247'","<S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S><S sid=""109"" ssid=""13"">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S sid=""165"" ssid=""37"">The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.</S><S sid=""247"" ssid=""66"">This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.</S>",'Method_Citation'
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","'118','143','210','249'","<S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""143"" ssid=""15"">IRSTLM and BerkeleyLM use this state function (and a limit of N −1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S><S sid=""249"" ssid=""68"">Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.</S>",'Method_Citation'
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","'115','216','264','277'","<S sid=""115"" ssid=""19"">Most similar is scrolling queries, wherein left-to-right queries that add one word at a time are optimized.</S><S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S><S sid=""264"" ssid=""6"">For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S><S sid=""277"" ssid=""4"">These performance gains transfer to improved system runtime performance; though we focused on Moses, our code is the best lossless option with cdec and Joshua.</S>",'Method_Citation'
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'21','165','239','240'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""165"" ssid=""37"">The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.</S><S sid=""239"" ssid=""58"">Overall, language modeling significantly impacts decoder performance.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","'80','85','162','269'","<S sid=""80"" ssid=""58"">The highestorder N-gram array omits backoff and the index, since these are not applicable.</S><S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S><S sid=""162"" ssid=""34"">BerkeleyLM uses states to optimistically search for longer n-gram matches first and must perform twice as many random accesses to retrieve backoff information.</S><S sid=""269"" ssid=""11"">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state.</S>",'Method_Citation'
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'79','80','85','146'","<S sid=""79"" ssid=""57"">Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.</S><S sid=""80"" ssid=""58"">The highestorder N-gram array omits backoff and the index, since these are not applicable.</S><S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S><S sid=""146"" ssid=""18"">If the log backoff of wnf is also zero (it may not be in filtered models), then wf should be omitted from the state.</S>",'Method_Citation'
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'9','43','47','85'","<S sid=""9"" ssid=""4"">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S>",'Method_Citation'
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'60','109','196','247'","<S sid=""60"" ssid=""38"">Otherwise, the scope of the search problem shrinks recursively: if A[pivot] < k then this becomes the new lower bound: l +— pivot; if A[pivot] > k then u +— pivot.</S><S sid=""109"" ssid=""13"">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S sid=""196"" ssid=""15"">This suggests a strategy: run interpolation search until the range narrows to 4096 or fewer entries, then switch to binary search.</S><S sid=""247"" ssid=""66"">This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.</S>",'Method_Citation'
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'80','84','85','162'","<S sid=""80"" ssid=""58"">The highestorder N-gram array omits backoff and the index, since these are not applicable.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S><S sid=""162"" ssid=""34"">BerkeleyLM uses states to optimistically search for longer n-gram matches first and must perform twice as many random accesses to retrieve backoff information.</S>",'Method_Citation'
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","'9','47','80','252'","<S sid=""9"" ssid=""4"">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""80"" ssid=""58"">The highestorder N-gram array omits backoff and the index, since these are not applicable.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
