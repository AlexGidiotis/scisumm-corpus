Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0,"Foster et al, 2010",0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)","Another popular task in SMT is domain adaptation (Foster et al, 2010)",'141',"<S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",'Method_Citation'
2,D10-1044,P12-1048,0,"Foster et al, 2010",0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","'78','93','95','96'","<S sid=""78"" ssid=""15"">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates, and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid=""93"" ssid=""30"">The 8 similarity-to-IN features are based on word frequencies and scores from various models trained on the IN corpus: To avoid numerical problems, each feature was normalized by subtracting its mean and dividing by its standard deviation.</S><S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid=""96"" ssid=""33"">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>",'Method_Citation'
3,D10-1044,D12-1129,0,"Foster et al., 2010",0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now","Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)","'53','105','141'","<S sid=""53"" ssid=""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007).</S><S sid=""105"" ssid=""9"">Compared to the EMEA/EP setting, the two domains in the NIST setting are less homogeneous and more similar to each other; there is also considerably more IN text available.</S><S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",'Method_Citation'
4,D10-1044,P14-2093,0,2010,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","'51','63','78','99'","<S sid=""51"" ssid=""15"">For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S><S sid=""63"" ssid=""27"">The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score.</S><S sid=""78"" ssid=""15"">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates, and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid=""99"" ssid=""3"">The dev and test sets were randomly chosen from the EMEA corpus.</S>",'Method_Citation'
5,D10-1044,E12-1055,0,2010,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)","Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation","'59','62','80'","<S sid=""59"" ssid=""23"">To set β, we used the same criterion as for α, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S><S sid=""62"" ssid=""26"">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid=""80"" ssid=""17"">We model po(s|t) using a MAP criterion over weighted phrase-pair counts: and from the similarity to (5), assuming y = 0, we see that wλ(s, t) can be interpreted as approximating pf(s, t)/po(s, t).</S>",'Method_Citation'
6,D10-1044,E12-1055,0,2010,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs","Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs","'23','26','28','59'","<S sid=""23"" ssid=""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S><S sid=""28"" ssid=""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid=""59"" ssid=""23"">To set β, we used the same criterion as for α, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S>",'Method_Citation'
7,D10-1044,E12-1055,0,2010,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model","Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model","'20','48','51','126'","<S sid=""20"" ssid=""17"">Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions.</S><S sid=""48"" ssid=""12"">This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S><S sid=""51"" ssid=""15"">For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S><S sid=""126"" ssid=""30"">The 4th block contains instance-weighting models trained on all features, used within a MAP TM combination, and with a linear LM mixture.</S>",'Method_Citation'
8,D10-1044,E12-1055,0,Foster et al 2010,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),"'25','51','69','145'","<S sid=""25"" ssid=""22"">For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.</S><S sid=""51"" ssid=""15"">For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S><S sid=""69"" ssid=""6"">Intuitively, as suggested by the example in the introduction, this is the right granularity to capture domain effects.</S><S sid=""145"" ssid=""2"">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>",'Method_Citation'
9,D10-1044,E12-1055,0,Foster et al 2010,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models,"'30','122','123','141'","<S sid=""30"" ssid=""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only.</S><S sid=""122"" ssid=""26"">The linear LM (lin lm), TM (lin tm) and MAP TM (map tm) used with non-adapted counterparts perform in all cases slightly worse than the log-linear combination, which adapts both LM and TM components.</S><S sid=""123"" ssid=""27"">However, when the linear LM is combined with a linear TM (lm+lin tm) or MAP TM (lm+map TM), the results are much better than a log-linear combination for the EMEA setting, and on a par for NIST.</S><S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",'Method_Citation'
10,D10-1044,P12-1099,0,"Foster et al, 2010",0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","'51','78','122','126'","<S sid=""51"" ssid=""15"">For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S><S sid=""78"" ssid=""15"">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates, and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid=""122"" ssid=""26"">The linear LM (lin lm), TM (lin tm) and MAP TM (map tm) used with non-adapted counterparts perform in all cases slightly worse than the log-linear combination, which adapts both LM and TM components.</S><S sid=""126"" ssid=""30"">The 4th block contains instance-weighting models trained on all features, used within a MAP TM combination, and with a linear LM mixture.</S>",'Method_Citation'
11,D10-1044,P12-1099,0,2010,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),Our technique for setting ? m is similar to that outlined in Foster et al (2010),'32',"<S sid=""32"" ssid=""29"">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results.</S>",'Method_Citation'
12,D10-1044,P12-1099,0,"Foster et al., 2010",0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)","For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)",'75',"<S sid=""75"" ssid=""12"">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>",'Method_Citation'
13,D10-1044,P12-1099,0,2010,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT","Foster et al (2010), however, uses a different approach to select related sentences from OUT","'26','67','87','141'","<S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S><S sid=""67"" ssid=""4"">We extend the Matsoukas et al approach in several ways.</S><S sid=""87"" ssid=""24"">A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: cI(s, t) + w,\(s, t)co(, s, t), suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful, but at the cost of having to learn to downweight OUT separately for each t, which we suspect would require more training data for reliable performance.</S><S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",'Method_Citation'
14,D10-1044,P12-1099,0,2010,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,"'19','89','131','153'","<S sid=""19"" ssid=""16"">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid=""89"" ssid=""26"">We used 22 features for the logistic weighting model, divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language, and one intended to capture similarity to the IN domain.</S><S sid=""131"" ssid=""35"">The general-language features have a slight advantage over the similarity features, and both are better than the SVM feature.</S><S sid=""153"" ssid=""10"">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>",'Method_Citation'
15,D10-1044,P13-1126,0,"Foster et al, 2010",0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs","As in (Foster et al, 2010), this approach works at the level of phrase pairs","'26','59','152'","<S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S><S sid=""59"" ssid=""23"">To set β, we used the same criterion as for α, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S><S sid=""152"" ssid=""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>",'Method_Citation'
16,D10-1044,D11-1033,0,2010,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","'0','48','51','100'","<S sid=""0"" ssid=""1"">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S><S sid=""48"" ssid=""12"">This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S><S sid=""51"" ssid=""15"">For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S><S sid=""100"" ssid=""4"">Figure 1 shows sample sentences from these domains, which are widely divergent.</S>",'Method_Citation'
17,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance","Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance","'60','69','119','136'","<S sid=""60"" ssid=""24"">The matching sentence pairs are then added to the IN corpus, and the system is re-trained.</S><S sid=""69"" ssid=""6"">Intuitively, as suggested by the example in the introduction, this is the right granularity to capture domain effects.</S><S sid=""119"" ssid=""23"">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S><S sid=""136"" ssid=""5"">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>",'Method_Citation'
18,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","'26','59','95','141'","<S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S><S sid=""59"" ssid=""23"">To set β, we used the same criterion as for α, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004).</S><S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid=""141"" ssid=""10"">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>",'Method_Citation'
19,D10-1044,P14-1012,0,"Foster et al, 2010",0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","'26','93','95','131'","<S sid=""26"" ssid=""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.</S><S sid=""93"" ssid=""30"">The 8 similarity-to-IN features are based on word frequencies and scores from various models trained on the IN corpus: To avoid numerical problems, each feature was normalized by subtracting its mean and dividing by its standard deviation.</S><S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid=""131"" ssid=""35"">The general-language features have a slight advantage over the similarity features, and both are better than the SVM feature.</S>",'Method_Citation'
