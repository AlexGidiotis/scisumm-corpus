In fact, we found that enabling IRSTLMâ€™s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.Time for Moses itself to load, including loading the language model and phrase table, is included.Moses keeps language models and many other resources in static variables, so these are still resident in memory.To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state.Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.The highestorder N-gram array omits backoff and the index, since these are not applicable.Most similar is scrolling queries, wherein left-to-right queries that add one word at a time are optimized.For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.