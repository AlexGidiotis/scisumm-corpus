The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).Figure 1 provides some statistics about this corpus.For statistics on this test set, refer to Figure 1.Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements, we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.Since different judges judged different systems (recall that judges were excluded to judge system output from their own institution), we normalized the scores.However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.Human judges also pointed out difficulties with the evaluation of long sentences.• We evaluated translation from English, in addition to into English.