These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).Additionally, PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1, ... , L. In other words, rather than using a single set of topics Φ = {φ1, ... , φT}, as in LDA, there are L sets of language-specific topics, Φ1, ... , ΦL, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.