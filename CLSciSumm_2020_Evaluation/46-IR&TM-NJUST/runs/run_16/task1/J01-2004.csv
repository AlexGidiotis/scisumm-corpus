Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",'99',"<S sid=""99"" ssid=""3"">These can be divided into two rough groups: those that use the grammar as a language model, and those that use a parser to uncover phrasal heads standing in an important relation (c-command) to the current word.</S>",'Method_Citation'
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,'99',"<S sid=""99"" ssid=""3"">These can be divided into two rough groups: those that use the grammar as a language model, and those that use a parser to uncover phrasal heads standing in an important relation (c-command) to the current word.</S>",'Method_Citation'
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank",'357',"<S sid=""357"" ssid=""113"">The corpus comes with a baseline trigram model, using a 20,000-word open vocabulary, and trained on approximately 40 million words.</S>",'Method_Citation'
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",'346',"<S sid=""346"" ssid=""102"">These results are particularly remarkable, given that we did not build our model as a language model per se, but rather as a parsing model.</S>",'Method_Citation'
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",'234',"<S sid=""234"" ssid=""138"">The beam threshold at word w, is a function of the probability of the top-ranked candidate analysis on priority queue kiwi, and the number of candidates on H1+1.</S>",'Method_Citation'
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",'115',"<S sid=""115"" ssid=""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S>",'Method_Citation'
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",'119',"<S sid=""119"" ssid=""23"">The second stage predicts the POS given the word and the left context.</S>",'Method_Citation'
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",'289',"<S sid=""289"" ssid=""45"">These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S>",'Method_Citation'
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)",'234',"<S sid=""234"" ssid=""138"">The beam threshold at word w, is a function of the probability of the top-ranked candidate analysis on priority queue kiwi, and the number of candidates on H1+1.</S>",'Method_Citation'
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",'291',"<S sid=""291"" ssid=""47"">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.</S>",'Method_Citation'
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)",'283',"<S sid=""283"" ssid=""39"">Unlike the Roark and Johnson parser, however, our coverage did not substantially drop as the amount of conditioning information increased, and in some cases, coverage improved slightly.</S>",'Method_Citation'
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)",'236',"<S sid=""236"" ssid=""140"">If p is the probability of the highest-ranked analysis on H1Â±1, then another analysis is discarded if its probability falls below pf(-y, IH,+11), where -y is an initial parameter, which we call the base beam factor.</S>",'Method_Citation'
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",'291',"<S sid=""291"" ssid=""47"">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.</S>",'Method_Citation'
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",'267',"<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S>",'Method_Citation'
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures",'238',"<S sid=""238"" ssid=""142"">Thus, if 100 analyses have already been pushed onto then a candidate analysis must have a probability above 10-5/3 to avoid being pruned.</S>",'Method_Citation'
