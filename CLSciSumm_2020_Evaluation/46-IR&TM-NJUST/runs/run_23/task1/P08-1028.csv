Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'36','43','53','63'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""63"" ssid=""11"">This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.</S>",'Method_Citation'
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'93','150'","<S sid=""93"" ssid=""6"">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid=""150"" ssid=""63"">First, the additive model in (7) weighs differentially the contribution of the two constituents.</S>",'Method_Citation'
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'43','54','76','141'","<S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""54"" ssid=""2"">The construction of the semantic space depends on the definition of linguistic context (e.g., neighbouring words can be documents or collocations), the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw co-occurrence frequencies or ratios of probabilities).</S><S sid=""76"" ssid=""24"">The models considered so far assume that components do not ‘interfere’ with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S><S sid=""141"" ssid=""54"">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>",'Method_Citation'
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting",'72',"<S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.</S>",'Method_Citation'
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'12','36','43','45'","<S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S>",'Method_Citation'
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'43','45','48','53'","<S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S>",'Method_Citation'
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'36','48','53','57'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""48"" ssid=""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>",'Method_Citation'
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'51','53','57','62'","<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S>",'Method_Citation'
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'36','45','47','168'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid=""47"" ssid=""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",'Method_Citation'
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'25','150','174','197'","<S sid=""25"" ssid=""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid=""150"" ssid=""63"">First, the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid=""197"" ssid=""9"">We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.</S>",'Method_Citation'
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'34','39','66','153'","<S sid=""34"" ssid=""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid=""39"" ssid=""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid=""66"" ssid=""14"">To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid=""153"" ssid=""66"">Specifically, we considered eleven models, varying in their weightings, in steps of 10%, from 100% noun through 50% of both verb and noun to 100% verb.</S>",'Method_Citation'
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'36','45','53','174'","<S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S>",'Method_Citation'
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'53','131','168','190'","<S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""131"" ssid=""44"">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",'Method_Citation'
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'74','132','137','171'","<S sid=""74"" ssid=""22"">As an example if we set α to 0.4 and β to 0.6, then horse= [0 2.4 0.8 4 1.6] and run = [0.6 4.8 2.4 2.4 0], and their sum horse + run = [0.6 5.6 3.2 6.4 1.6].</S><S sid=""132"" ssid=""45"">We also measured how well humans agree in their ratings.</S><S sid=""137"" ssid=""50"">More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items.</S><S sid=""171"" ssid=""5"">For comparison, we also show the human ratings for these items (UpperBound).</S>",'Method_Citation'
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'49','124','171','181'","<S sid=""49"" ssid=""22"">The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.</S><S sid=""124"" ssid=""37"">Participants were asked to rate how similar the two sentences were on a scale of one to seven.</S><S sid=""171"" ssid=""5"">For comparison, we also show the human ratings for these items (UpperBound).</S><S sid=""181"" ssid=""15"">In order to establish which ones fit our data better, we examined whether the correlation coefficients achieved differ significantly using a t-test (Cohen and Cohen, 1983).</S>",'Method_Citation'
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'47','53','63','65'","<S sid=""47"" ssid=""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid=""63"" ssid=""11"">This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.</S><S sid=""65"" ssid=""13"">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>",'Method_Citation'
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'12','36','43','45'","<S sid=""12"" ssid=""8"">In fact, the commonest method for combining the vectors is to average them.</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S>",'Method_Citation'
