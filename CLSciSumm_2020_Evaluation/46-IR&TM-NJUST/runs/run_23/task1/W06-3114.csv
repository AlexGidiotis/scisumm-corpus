Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","'49','73','166','171'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""73"" ssid=""12"">We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other.</S><S sid=""166"" ssid=""59"">Lack of correct reference translations was pointed out as a short-coming of our evaluation.</S><S sid=""171"" ssid=""2"">While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.</S>",'Method_Citation'
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","'5','33','132'","<S sid=""5"" ssid=""3"">• We evaluated translation from English, in addition to into English.</S><S sid=""33"" ssid=""26"">While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid=""132"" ssid=""25"">It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.</S>",'Method_Citation'
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","'34','107','129','173'","<S sid=""34"" ssid=""27"">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S><S sid=""107"" ssid=""23"">Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2.</S><S sid=""129"" ssid=""22"">All systems (except for Systran, which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid=""173"" ssid=""4"">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>",'Method_Citation'
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","'74','92','99','143'","<S sid=""74"" ssid=""13"">While we had up to 11 submissions for a translation direction, we did decide against presenting all 11 system outputs to the human judge.</S><S sid=""92"" ssid=""8"">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S><S sid=""99"" ssid=""15"">Systems that generally do worse than others will receive a negative one.</S><S sid=""143"" ssid=""36"">For instance, for out-ofdomain English-French, Systran has the best BLEU and manual scores.</S>",'Method_Citation'
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)",'22',"<S sid=""22"" ssid=""15"">The text type are editorials instead of speech transcripts.</S>",'Method_Citation'
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","'35','39','150','154'","<S sid=""35"" ssid=""1"">For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""150"" ssid=""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French), at least as measured by BLEU.</S><S sid=""154"" ssid=""47"">While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S>",'Method_Citation'
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","'93','113','135','145'","<S sid=""93"" ssid=""9"">If one system is perfect, another has slight flaws and the third more flaws, a judge is inclined to hand out judgements of 5, 4, and 3.</S><S sid=""113"" ssid=""6"">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S><S sid=""135"" ssid=""28"">The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.</S><S sid=""145"" ssid=""38"">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>",'Method_Citation'
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"'39','48','49','140'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""48"" ssid=""14"">Confidence Interval: Since BLEU scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply.</S><S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",'Method_Citation'
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),'39',"<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>",'Method_Citation'
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","'50','58','59','102'","<S sid=""50"" ssid=""16"">Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S><S sid=""59"" ssid=""25"">The sign test checks, how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid=""102"" ssid=""18"">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>",'Method_Citation'
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","'59','61','87','102'","<S sid=""59"" ssid=""25"">The sign test checks, how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid=""61"" ssid=""27"">We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: If p(0..k; n, p) < 0.05, or p(0..k; n, p) > 0.95 then we have a statistically significant difference between the systems.</S><S sid=""87"" ssid=""3"">Since different judges judged different systems (recall that judges were excluded to judge system output from their own institution), we normalized the scores.</S><S sid=""102"" ssid=""18"">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>",'Method_Citation'
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","'49','120','137','150'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""120"" ssid=""13"">In Figure 4, we displayed the number of system comparisons, for which we concluded statistical significance.</S><S sid=""137"" ssid=""30"">This is because different judges focused on different language pairs.</S><S sid=""150"" ssid=""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French), at least as measured by BLEU.</S>",'Method_Citation'
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","'39','71','145','165'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""71"" ssid=""10"">Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.</S><S sid=""145"" ssid=""38"">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S><S sid=""165"" ssid=""58"">However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).</S>",'Method_Citation'
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","'58','99','126','167'","<S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S><S sid=""99"" ssid=""15"">Systems that generally do worse than others will receive a negative one.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S><S sid=""167"" ssid=""60"">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>",'Method_Citation'
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","'13','18','21','126'","<S sid=""13"" ssid=""6"">We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.</S><S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S><S sid=""21"" ssid=""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",'Method_Citation'
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","'18','33','34'","<S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S><S sid=""33"" ssid=""26"">While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid=""34"" ssid=""27"">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>",'Method_Citation'
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","'130','154','155','175'","<S sid=""130"" ssid=""23"">This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.</S><S sid=""154"" ssid=""47"">While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid=""155"" ssid=""48"">For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.</S><S sid=""175"" ssid=""6"">Replacing this with an ranked evaluation seems to be more suitable.</S>",'Method_Citation'
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","'5','12','18','132'","<S sid=""5"" ssid=""3"">• We evaluated translation from English, in addition to into English.</S><S sid=""12"" ssid=""5"">To summarize, we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S><S sid=""132"" ssid=""25"">It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.</S>",'Method_Citation'
