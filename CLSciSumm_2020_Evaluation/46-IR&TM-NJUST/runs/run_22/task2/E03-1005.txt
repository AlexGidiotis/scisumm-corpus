In this paper, we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).For the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability.Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).A new nonterminal is created for each node in the training data.Tested on the OVIS corpus, Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al.