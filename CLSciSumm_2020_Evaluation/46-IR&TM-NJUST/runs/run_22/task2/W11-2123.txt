For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words), then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.Collisions between two keys in the table can be identified at model building time.Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity.Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.By contrast, BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.