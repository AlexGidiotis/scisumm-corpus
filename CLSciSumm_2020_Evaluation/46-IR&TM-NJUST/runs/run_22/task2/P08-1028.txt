We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.Vector addition is by far the most common method for representing the meaning of linguistic sequences.To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.First, the additive model in (7) weighs differentially the contribution of the two constituents.This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.For comparison, we also show the human ratings for these items (UpperBound).Vector addition does not increase the dimensionality of the resulting vector.The simple additive model fails to distinguish between High and Low Similarity items.