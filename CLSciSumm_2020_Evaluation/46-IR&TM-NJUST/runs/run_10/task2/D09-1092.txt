An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence.The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item.For each document in the query language we rank all documents in the target language and record the rank of the actual translation.Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.