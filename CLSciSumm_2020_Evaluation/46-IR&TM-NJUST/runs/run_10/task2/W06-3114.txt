For statistics on this test set, refer to Figure 1.This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.While we had up to 11 submissions for a translation direction, we did decide against presenting all 11 system outputs to the human judge.The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.If two systems’ scores are close, this may simply be a random effect in the test data.Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.• We evaluated translation from English, in addition to into English.There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them.