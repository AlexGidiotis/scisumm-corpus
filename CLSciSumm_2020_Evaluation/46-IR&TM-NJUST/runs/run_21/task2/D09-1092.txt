To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.The internet makes it possible for people all over the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages.In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.These versions of Wikipedia were selected to provide a diverse range of language families, geographic areas, and quantities of text.One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.