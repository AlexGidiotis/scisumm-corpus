Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","'32','143','184'","<S sid=""32"" ssid=""8"">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid=""143"" ssid=""92"">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften,” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid=""184"" ssid=""18"">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>",'Method_Citation'
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","'29','130','154'","<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S><S sid=""130"" ssid=""79"">In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","'21','131','138','148'","<S sid=""21"" ssid=""17"">The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.</S><S sid=""131"" ssid=""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S><S sid=""138"" ssid=""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid=""148"" ssid=""97"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>",'Method_Citation'
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","'35','97','127','187'","<S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""97"" ssid=""46"">Rather, these results are intended as a quantitative analysis of the difference between the two models.</S><S sid=""127"" ssid=""76"">One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.</S><S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>",'Method_Citation'
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","'52','157','175','177'","<S sid=""52"" ssid=""1"">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid=""157"" ssid=""106"">For each document in the query language we rank all documents in the target language and record the rank of the actual translation.</S><S sid=""175"" ssid=""9"">We preprocessed the data by removing tables, references, images and info-boxes.</S><S sid=""177"" ssid=""11"">In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.</S>",'Method_Citation'
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","'35','129'","<S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""129"" ssid=""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).</S>",'Method_Citation'
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","'23','53','113','154'","<S sid=""23"" ssid=""19"">The internet makes it possible for people all over the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages.</S><S sid=""53"" ssid=""2"">In this case, we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid=""113"" ssid=""62"">By doing this, every document in S has its own distribution over topics, independent of any other documents.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","'8','29','35','154'","<S sid=""8"" ssid=""4"">In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S><S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","'29','35','130','154'","<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S><S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""130"" ssid=""79"">In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","'54','55','88','130'","<S sid=""54"" ssid=""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.</S><S sid=""55"" ssid=""4"">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S><S sid=""88"" ssid=""37"">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S><S sid=""130"" ssid=""79"">In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).</S>",'Method_Citation'
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","'8','88','154','174'","<S sid=""8"" ssid=""4"">In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid=""88"" ssid=""37"">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S><S sid=""174"" ssid=""8"">These versions of Wikipedia were selected to provide a diverse range of language families, geographic areas, and quantities of text.</S>",'Method_Citation'
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","'35','88','130','154'","<S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""88"" ssid=""37"">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S><S sid=""130"" ssid=""79"">In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","'6','35','131','156'","<S sid=""6"" ssid=""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S><S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""131"" ssid=""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S><S sid=""156"" ssid=""105"">We use both Jensen-Shannon divergence and cosine distance.</S>",'Method_Citation'
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","'55','130','132','152'","<S sid=""55"" ssid=""4"">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S><S sid=""130"" ssid=""79"">In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).</S><S sid=""132"" ssid=""81"">Unlike previous work (Koehn and Knight, 2002), we evaluate all words, not just nouns.</S><S sid=""152"" ssid=""101"">We begin by dividing the data into a training set of 69,550 document tuples and a test set of 17,435 document tuples.</S>",'Method_Citation'
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","'35','60','127','187'","<S sid=""35"" ssid=""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S><S sid=""60"" ssid=""9"">Details by language are shown in Table 1.</S><S sid=""127"" ssid=""76"">One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.</S><S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>",'Method_Citation'
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","'42','50','89','187'","<S sid=""42"" ssid=""8"">Given a corpus of training and test document tuples—W and W', respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.</S><S sid=""50"" ssid=""16"">, ΦL.The probability of held-out document tuples W' given training tuples W is then approximated by Topic assignments for a test document tuple sampling.</S><S sid=""89"" ssid=""38"">Analytically calculating the probability of a set of held-out document tuples given Φ1, ... , ΦL and αm is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.</S><S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>",'Method_Citation'
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,'142',"<S sid=""142"" ssid=""91"">EN “comitology” and IT lang Topics at P = 0.01 “comitalogia”) that simply were not in the lexica.</S>",'Method_Citation'
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","'65','72','79','187'","<S sid=""65"" ssid=""14"">This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.</S><S sid=""72"" ssid=""21"">To answer this question, we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid=""79"" ssid=""28"">For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.</S><S sid=""187"" ssid=""21"">To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.</S>",'Method_Citation'
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","'29','60','68','154'","<S sid=""29"" ssid=""5"">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S><S sid=""60"" ssid=""9"">Details by language are shown in Table 1.</S><S sid=""68"" ssid=""17"">The third topic demonstrates differences in inflectional variation.</S><S sid=""154"" ssid=""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).</S>",'Method_Citation'
