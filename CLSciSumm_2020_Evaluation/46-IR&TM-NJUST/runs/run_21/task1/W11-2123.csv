Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'12','200','218','281'","<S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""281"" ssid=""2"">Hieu Hoang named the code “KenLM” and assisted with Moses along with Barry Haddow.</S>",'Method_Citation'
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'14','26','84','252'","<S sid=""14"" ssid=""9"">MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity.</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'9','14','146','239'","<S sid=""9"" ssid=""4"">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid=""14"" ssid=""9"">MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity.</S><S sid=""146"" ssid=""18"">If the log backoff of wnf is also zero (it may not be in filtered models), then wf should be omitted from the state.</S><S sid=""239"" ssid=""58"">Overall, language modeling significantly impacts decoder performance.</S>",'Method_Citation'
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'13','26','43','201'","<S sid=""13"" ssid=""8"">IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""201"" ssid=""20"">Unlike Germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.</S>",'Method_Citation'
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'27','84','98','185'","<S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S><S sid=""185"" ssid=""4"">We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.</S>",'Method_Citation'
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'24','84','135','252'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""135"" ssid=""7"">Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),'11',"<S sid=""11"" ssid=""6"">Many packages perform language model queries.</S>",'Method_Citation'
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'43','79','84','200'","<S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""79"" ssid=""57"">Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>",'Method_Citation'
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'27','43','84','200'","<S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>",'Method_Citation'
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'34','38','92','149'","<S sid=""34"" ssid=""12"">Therefore, a populated probing hash table consists of an array of buckets that contain either one entry or are empty.</S><S sid=""38"" ssid=""16"">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S><S sid=""149"" ssid=""21"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S>",'Method_Citation'
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","'13','26','43','193'","<S sid=""13"" ssid=""8"">IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""193"" ssid=""12"">It also uses less memory, with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>",'Method_Citation'
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)",'249',"<S sid=""249"" ssid=""68"">Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.</S>",'Method_Citation'
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","'25','197','214','244'","<S sid=""25"" ssid=""3"">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid=""197"" ssid=""16"">However, reads in the TRIE data structure are more expensive due to bit-level packing, so we found that it is faster to use interpolation search the entire time.</S><S sid=""214"" ssid=""33"">For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.</S><S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>",'Method_Citation'
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'200','218','240','266'","<S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S><S sid=""266"" ssid=""8"">Finally, other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>",'Method_Citation'
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","'6','43','79','92'","<S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""79"" ssid=""57"">Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.</S><S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S>",'Method_Citation'
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'88','235','269','273'","<S sid=""88"" ssid=""66"">By contrast, BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid=""235"" ssid=""54"">Part of the gap between resident and virtual memory is due to the time at which data was collected.</S><S sid=""269"" ssid=""11"">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state.</S><S sid=""273"" ssid=""15"">This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.</S>",'Method_Citation'
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'24','26','43','240'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'43','60','62','248'","<S sid=""43"" ssid=""21"">Collisions between two keys in the table can be identified at model building time.</S><S sid=""60"" ssid=""38"">Otherwise, the scope of the search problem shrinks recursively: if A[pivot] < k then this becomes the new lower bound: l +— pivot; if A[pivot] > k then u +— pivot.</S><S sid=""62"" ssid=""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words), then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid=""248"" ssid=""67"">As noted for the perplexity task, we do not expect cache to grow substantially with model size, so RandLM remains a low-memory option.</S>",'Method_Citation'
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'6','79','84','85'","<S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S><S sid=""79"" ssid=""57"">Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""85"" ssid=""63"">This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.</S>",'Method_Citation'
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","'83','84','200','252'","<S sid=""83"" ssid=""61"">When SRILM estimates a model, it sometimes removes n-grams but not n + 1-grams that extend it to the left.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""252"" ssid=""71"">It also does not prune, so comparing to our pruned model would be unfair.</S>",'Method_Citation'
