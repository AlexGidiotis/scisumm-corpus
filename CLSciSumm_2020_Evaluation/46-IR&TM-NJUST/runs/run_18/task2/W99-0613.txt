The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).The label for a test example with features x is then defined as In this paper we define h(x, y) as the following function of counts seen in training data: Count(x,y) is the number of times feature x is seen with label y in training data, Count(x) = EyEy Count(x, y). a is a smoothing parameter, and k is the number of possible labels.In our experiments we set the parameter values randomly, and then ran EM to convergence.The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).