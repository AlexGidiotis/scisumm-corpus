These can be divided into two rough groups: those that use the grammar as a language model, and those that use a parser to uncover phrasal heads standing in an important relation (c-command) to the current word.These results are particularly remarkable, given that we did not build our model as a language model per se, but rather as a parsing model.These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'Unlike the Roark and Johnson parser, however, our coverage did not substantially drop as the amount of conditioning information increased, and in some cases, coverage improved slightly.In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.