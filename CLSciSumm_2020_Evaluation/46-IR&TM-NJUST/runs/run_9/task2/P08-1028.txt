Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully.To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).