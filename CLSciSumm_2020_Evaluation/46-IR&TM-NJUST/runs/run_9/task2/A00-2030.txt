Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events.