That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal tree-bank.When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled "MaxEnt-Inspired'.Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.