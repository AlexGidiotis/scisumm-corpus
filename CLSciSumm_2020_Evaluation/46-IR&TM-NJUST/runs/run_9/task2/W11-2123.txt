We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.The BerkeleyLM direct-mapped cache is in principle faster than caches implemented by RandLM and by IRSTLM, so we may write a C++ equivalent implementation as future work.Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.IRSTLM and BerkeleyLM use this state function (and a limit of N âˆ’1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.