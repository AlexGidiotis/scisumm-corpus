Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'12','103','140'","<S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""140"" ssid=""12"">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>",'Method_Citation'
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'42','68','98','185'","<S sid=""42"" ssid=""20"">With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.</S><S sid=""68"" ssid=""46"">The trie data structure is commonly used for language modeling.</S><S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S><S sid=""185"" ssid=""4"">We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.</S>",'Method_Citation'
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'8','132','158','159'","<S sid=""8"" ssid=""3"">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid=""132"" ssid=""4"">We call these N − 1 words state.</S><S sid=""158"" ssid=""30"">These are written to the state s(wn1) and returned so that they can be used for the following query.</S><S sid=""159"" ssid=""31"">Saving state allows our code to walk the data structure exactly once per query.</S>",'Method_Citation'
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'42','118','185','240'","<S sid=""42"" ssid=""20"">With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.</S><S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""185"" ssid=""4"">We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'84','118','185','210'","<S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""185"" ssid=""4"">We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S>",'Method_Citation'
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'24','52','200','240'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""52"" ssid=""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"'1','6','135','274'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S><S sid=""135"" ssid=""7"">Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors.</S><S sid=""274"" ssid=""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>",'Method_Citation'
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'24','84','103','240'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'84','118','149','200'","<S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""149"" ssid=""21"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>",'Method_Citation'
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'16','92','149'","<S sid=""16"" ssid=""11"">BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S><S sid=""149"" ssid=""21"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S>",'Method_Citation'
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","'42','200','247','273'","<S sid=""42"" ssid=""20"">With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""247"" ssid=""66"">This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.</S><S sid=""273"" ssid=""15"">This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.</S>",'Method_Citation'
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","'27','118','143','250'","<S sid=""27"" ssid=""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""143"" ssid=""15"">IRSTLM and BerkeleyLM use this state function (and a limit of N −1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.</S><S sid=""250"" ssid=""69"">The BerkeleyLM direct-mapped cache is in principle faster than caches implemented by RandLM and by IRSTLM, so we may write a C++ equivalent implementation as future work.</S>",'Method_Citation'
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","'0','188','218','249'","<S sid=""0"" ssid=""1"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""188"" ssid=""7"">For queries, we uniformly sampled 10 million hits and 10 million misses.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""249"" ssid=""68"">Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.</S>",'Method_Citation'
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'84','210','233','247'","<S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S><S sid=""233"" ssid=""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times, such as in parameter tuning.</S><S sid=""247"" ssid=""66"">This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.</S>",'Method_Citation'
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","'84','103','118','240'","<S sid=""84"" ssid=""62"">In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""118"" ssid=""22"">The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'47','50','199','266'","<S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""50"" ssid=""28"">Given counts cn1 where e.g. c1 is the vocabulary size, total memory consumption, in bits, is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""266"" ssid=""8"">Finally, other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>",'Method_Citation'
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'0','200','210','240'","<S sid=""0"" ssid=""1"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S><S sid=""210"" ssid=""29"">In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.</S><S sid=""240"" ssid=""59"">In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.</S>",'Method_Citation'
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'61','103','186','229'","<S sid=""61"" ssid=""39"">Interpolation search is therefore a form of binary search with better estimates informed by the uniform key distribution.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""186"" ssid=""5"">For sorted lookup, we compare interpolation search, standard C++ binary search, and standard C++ set based on red-black trees.</S><S sid=""229"" ssid=""48"">Then we ran binary search to determine the least amount of memory with which it would run.</S>",'Method_Citation'
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'45','47','162','199'","<S sid=""45"" ssid=""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid=""47"" ssid=""25"">For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid=""162"" ssid=""34"">BerkeleyLM uses states to optimistically search for longer n-gram matches first and must perform twice as many random accesses to retrieve backoff information.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>",'Method_Citation'
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","'24','83','162','200'","<S sid=""24"" ssid=""2"">The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid=""83"" ssid=""61"">When SRILM estimates a model, it sometimes removes n-grams but not n + 1-grams that extend it to the left.</S><S sid=""162"" ssid=""34"">BerkeleyLM uses states to optimistically search for longer n-gram matches first and must perform twice as many random accesses to retrieve backoff information.</S><S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>",'Method_Citation'
