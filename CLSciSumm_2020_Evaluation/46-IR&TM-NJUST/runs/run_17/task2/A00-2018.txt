We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal treebank.We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.The method that gives the best results, however, uses a Markov grammar â€” a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].