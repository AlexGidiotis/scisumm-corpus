An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.Ideally, the “glue” documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.The second topic, concerning children, demonstrates the variability of everyday terminology: although the four Romance languages are closely related, they use etymologically unrelated words for children.We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary.English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item.More than 350 topics in the Finnish LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics.