Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'The SLM is like a trigram, except that the conditioning words are taken from the tops of the stacks of candidate parses in the beam, rather than from the linear order of the string.When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.