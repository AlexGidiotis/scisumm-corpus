We present a general framework for vector-based composition which allows us to consider different classes of models.To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.The models considered so far assume that components do not ‘interfere’ with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.