through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.
the bayes models were able to achieve significantly higher precision than their non-parametric counterparts.
it is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.
furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in section 2.1.
in equations 1 through 3 we develop the model for constructing our parse using na√Øve bayes classification.
these three parsers have given the best reported parsing results on the penn treebank wall street journal corpus (marcus et al., 1993).
none of the models we have presented utilize features associated with a particular constituent (i.e. the label, span, parent label, etc.) to influence parser preference.
it is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.
the combining algorithm is presented with the candidate parses and asked to choose which one is best.
the corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by collins (1997), charniak (1997) and ratnaparkhi (1997).
for example, one parser could be more accurate at predicting noun phrases than the other parsers.
for our experiments we also report the mean of precision and recall, which we denote by (p + r)i2 and f-measure.
