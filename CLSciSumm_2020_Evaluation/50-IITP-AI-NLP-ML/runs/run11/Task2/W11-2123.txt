for the perplexity and translation tasks, we used srilm to build a 5-gram english language model on 834 million tokens from europarl v6 (koehn, 2005) and the 2011 workshop on machine translation news crawl corpus with duplicate lines removed.
the binary language model from section 5.2 and text phrase table were forced into disk cache before each run.
performance improvements transfer to the moses (koehn et al., 2007), cdec (dyer et al., 2010), and joshua (li et al., 2009) translation systems where our code has been integrated.
it is generally considered to be fast (pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and klein, 2011), with a default implementation based on hash tables within each trie node.
for even larger models, storing counts (talbot and osborne, 2007; pauls and klein, 2011; guthrie and hepple, 2010) is a possibility.
all language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.
time for moses itself to load, including loading the language model and phrase table, is included.
later, berkeleylm (pauls and klein, 2011) described ideas similar to ours.
with a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.
lossy compressed models randlm (talbot and osborne, 2007) and sheffield (guthrie and hepple, 2010) offer better memory consumption at the expense of cpu and accuracy.
the model was built with open vocabulary, modified kneser-ney smoothing, and default pruning settings that remove singletons of order 3 and higher.
