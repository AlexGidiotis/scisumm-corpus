in this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (lpcfg-hr) to information extraction.
for example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure.
given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.
chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.
although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.
there is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging.
given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a while our focus throughout the project was on te and tr, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.
an integrated model can limit the propagation of errors by making all decisions jointly.
instead, our parsing algorithm, trained on the upenn treebank, was run on the new york times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.
our system for muc-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.
in this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on muc-7 template elements and template relations.
