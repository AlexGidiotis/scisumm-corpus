second, we treat the projected labels as features in an unsupervised model (§5), rather than using them directly for supervised training.
we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.
for each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (pmi) between the two.5 the similarity between two trigram types is given by summing over the pmi values over feature instantiations that they have in common.
altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.
as discussed in more detail in §3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types.
the supervised pos tagging accuracies (on this tagset) are shown in the last row of table 2.
we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (berg- kirkpatrick et al., 2010).
more recently, subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.
as indicated by bolding, for seven out of eight languages the improvements of the “with lp” setting are statistically significant with respect to the other models, including the “no lp” setting.11 overall, it performs 10.4% better than the hitherto state-of-the-art feature-hmm baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.
