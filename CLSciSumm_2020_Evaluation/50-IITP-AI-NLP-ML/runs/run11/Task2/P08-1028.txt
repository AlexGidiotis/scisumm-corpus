nlp tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (coccaro and jurafsky, 1998).
in this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
the neighbors, kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 unfortunately, comparisons across vector composition models have been few and far between in the literature.
the merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see frank et al. (2007) for a criticism of kintsch’s (2001) evaluation standards).
that day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.
we employed leave-one-out resampling (weiss and kulikowski, 1991), by correlating the data obtained from each participant with the ratings obtained from all other participants.
our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).
relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: this allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.
analysis of similarity ratings the reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.
