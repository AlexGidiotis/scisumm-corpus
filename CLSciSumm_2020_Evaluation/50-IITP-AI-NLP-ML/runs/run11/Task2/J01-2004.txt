also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.
in the event that no complete parse is found, the highest initially ranked parse on the last nonempty priority queue is returned.
their modifications included: (i) removing orthographic cues to structure (e.g., punctuation); (ii) replacing all numbers with the single token n; and (iii) closing the vocabulary at 10,000, replacing all other words with the unk token.
he was able to reduce both sentence and word error rates on the atis corpus using this method.
this contrasts with our perplexity results reported above, as well as with the recognition experiments in chelba (2000), where the best results resulted from interpolated models.
the standard language model used in many speech recognition systems is the trigram model, i.e., a markov model of order 2, which can be characterized by the following equation: to smooth the trigram models that are used in this paper, we interpolate the probability estimates of higher-order markov models with lower-order markov models (jelinek and mercer 1980).
if p is the probability of the highest-ranked analysis on h1Â±1, then another analysis is discarded if its probability falls below pf(-y, ih,+11), where -y is an initial parameter, which we call the base beam factor.
a simple pcfg conditions rule probabilities on the left-hand side of the rule.
